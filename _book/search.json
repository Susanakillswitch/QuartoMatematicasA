[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "clase 22 de agosto 2023",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "3  Tarea 1",
    "section": "",
    "text": "Exploring functions to generate random variables with a Bernoulli distribution.py\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\nfig_01, ax_01 = plt.subplots(1, 1)\nfig_02, ax_02 = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(mean, var, skew,kurt)\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax_01.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax_01.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\nr = bernoulli.rvs(p, size=1000)\nax_02.hist(r, bins=200)\nplt.show()\n\n \n\n\nExploring functions to generate random variables with a Gaussian distribution.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1, 1)\nmean, var, skew, kurt = norm.stats(moments='mvsk')\n\nx = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\nax.plot(\n    x,\n    norm.pdf(x),\n    'r-',\n    lw=5,\n    alpha=0.6,\n    label='norm pdf'\n)\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nvals = norm.ppf([0.001, 0.5, 0.999])\n\nnp.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n\nr = norm.rvs(size=50000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\nax.set_xlim([x[0], x[-1]])\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\nFigura 3\n\n\n\n\nRevising multivariate Gaussian.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nfrom scipy.stats import multivariate_normal\n\nx = np.linspace(0, 5, 100, endpoint=False)\ny = multivariate_normal.pdf(x, mean=2.5, cov=0.5);\n\nfig1 = plt.figure()\nax = fig1.add_subplot(111)\nax.plot(x, y)\n# plt.show()\n\nx, y = np.mgrid[-5:5:.1, -5:5:.1]\npos = np.dstack((x, y))\nrv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nax2.contourf(x, y, rv.pdf(pos))\n# plt.show()\n\nax = plt.figure().add_subplot(projection='3d')\nax.plot_surface(\n    x,\n    y,\n    rv.pdf(pos),\n    edgecolor='royalblue',\n    lw=0.5,\n    rstride=8,\n    cstride=8,\n    alpha=0.4\n)\nax.contour(x, y, rv.pdf(pos), zdir='z', offset=-.2, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='x', offset=-5, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='y', offset=5, cmap='coolwarm')\n\nax.set(\n    xlim=(-5, 5),\n    ylim=(-5, 5),\n    zlim=(-0.2, 0.2),\n    xlabel='X',\n    ylabel='Y',\n    zlabel='Z'\n)\nplt.show()"
  },
  {
    "objectID": "Tarea2.html#demostración",
    "href": "Tarea2.html#demostración",
    "title": "4  Tarea 2",
    "section": "Demostración:",
    "text": "Demostración:\nConsidere una caminata aleatoria que comienza en 0 con saltos \\(h\\) y \\(-h\\) igualmente probables en los momentos \\(\\delta\\), 2 \\(\\delta\\),\\(\\dots\\), donde \\(h\\) y \\(\\delta\\) son números positivos. Más precisamente, sea \\(\\{X_{n}\\}_{n=1}^{\\infty}\\) una sucesión de elementos aleatorios independientes e idénticamente distribuidos. variables con \\[\nP\\left[X_{i}=h\\right]=P\\left[X_{i}=-h\\right]=\\dfrac{1}{2},\\forall i,\n\\]\nSea \\(Y_{\\delta,h}(0)=0\\) y pongamos \\[\nY_{\\delta,h}(n\\delta)=X_{1}+X_{2}+\\cdots+X_{n}.\n\\]\nPara \\(t&gt;0\\), defina \\(Y_{\\delta,h}(t)\\) mediante linealización, es decir, para \\(n\\delta&lt;t&lt;(n + 1)\\delta\\), defina \\[\nY_{\\delta,h}(t)=\\frac{(n+1)\\delta-t}{\\delta}Y_{\\delta,h}(n\\delta)+\\frac{t-n\\delta}{\\delta}Y_{\\delta,h}((n+1)\\delta).\n\\]\nCalculemos la función característica de \\(Y_{\\delta,h}(t)\\), donde \\(\\lambda\\in\\mathbb{R}\\) fijo y sea \\(t=n\\delta\\) así, \\(n=t/\\delta\\). Entonces se tiene que\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right] & = & \\prod_{j=1}^{n}Ee^{i\\lambda X_{j}},\\text{ por ser variables independientes,}\\nonumber\\\\\n    & = & (Ee^{i\\lambda X_{j}})^{n},\\text{ por ser idénticamente distribuidas,}\\nonumber\\\\\n    & = & \\frac{1}{2}(e^{i\\lambda h}+e^{-i\\lambda h})^{n},\\nonumber\\\\\n    & = & (\\cos(\\lambda h))^{n},\\nonumber\\\\\n    & = & (\\cos(\\lambda h))^{t/\\delta},\n\\end{eqnarray}\\] \\[\n.\n\\tag{4.1}\\] Por otro lado, sea \\(u=\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\Rightarrow\\ln\\left(u\\right)=\\dfrac{1}{\\delta}\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]\\).\nUsando la expansión de Taylor de \\(\\cos\\left(x\\right)\\) se tiene que \\[\n\\cos\\left(\\lambda h\\right)\\approx1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!},\n\\]\nentonces \\[\\begin{eqnarray}\n    \\ln\\left(\\cos\\left(\\lambda h\\right)\\right) & \\approx & \\ln\\left[1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right]\\nonumber\\\\\n& \\approx & -\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}-\\frac{1}{2}\\left(-\\frac{\\lambda^{2}h^{2}}{2!}+\\frac{\\lambda^{4}h^{4}}{4!}\\right)^{2}\\nonumber\\\\\n  & = & -\\dfrac{\\lambda^{2} h^{2}}{2!}+\\dfrac{\\lambda ^{4}h^{4}}{4!}-\\frac{1}{2}\\left(\\frac{\\lambda^{4}h^{4}}{4}-\\frac{\\lambda^{6}h^{6}}{24^{2}}+\\frac{\\lambda^{8}h^{8}}{24}\\right)\\nonumber\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}+\\dfrac{\\lambda^{4} h^{4}}{24}-\\frac{\\lambda^{4}h^{4}}{8}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\nonumber\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\n\\end{eqnarray}\\] para una \\(h\\) pequeña, se satisface que, \\[\n-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\approx 0\n\\]\nPor lo tanto, \\(\\ln\\left(\\cos\\left(\\lambda h\\right)\\right)\\approx -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\).\\ Así, para \\(\\delta\\) y \\(h\\) pequeña, se tiene que \\(\\ln u\\approx \\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\).\\ Entonces\n\\[\\begin{equation}\n    u\\approx\\exp\\left[\\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\right]\n\\end{equation}\\]\nEntonces por la ecuación (Equation 4.1) \\[\\begin{equation}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\approx\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\n\\end{equation}\\]\nCalculando el limite \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left[-t\\left(\\left[\\dfrac{h^{2}}{\\delta}\\right]\\left(\\dfrac{\\lambda^{2}}{2}-\\dfrac{\\lambda^{4}h^{2}}{24}\\right)\\right)\\right],\n\\]\nAsumamos que \\(\\delta\\to0\\), \\(h\\to0\\) pero \\(h^{2}/\\delta\\to\\infty\\). Entonces \\(\\lim_{\\delta\\to0} Y_{\\delta, h}(t)\\) no existe. Por otro lado, consideremos la siguiente renormalización,\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right] & = & E\\left[\\exp (i\\lambda Y_{n,\\delta}\\left(t\\right))\\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\right]\\nonumber\\\\\n    & = & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)E\\exp\\left[ i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\nonumber\\\\\n    & \\approx & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\\nonumber\\\\\n    & = & \\exp\\left(-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right)\n\\end{eqnarray}\\]\nAsí, si \\(\\delta,h\\to0\\) de tal manera que \\(h^{2}/\\delta\\to\\infty\\) y \\(h^{4}/\\delta\\to0\\), entonces \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2}\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left(\\dfrac{\\left(\\lambda h\\right)^{4}}{24\\delta}\\right)=1\n\\]"
  },
  {
    "objectID": "Tarea3.html",
    "href": "Tarea3.html",
    "title": "5  Tarea 3",
    "section": "",
    "text": "Exercise 5.1 (Ejercicio 1:) Si \\(X\\thicksim N(\\mu,\\sigma)\\) entonces \\(\\left(\\dfrac{X-\\mu}{\\sigma}\\right)\\thicksim N(0,1)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\dfrac{X-\\mu}{\\sigma}\\),\n$$\n \\begin{aligned}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & E\\left[e^{it\\left(\\frac{X-\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & E\\left[e^{\\left(\\frac{itX}{\\sigma}-\\frac{it\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}E\\left[e^{\\left(\\frac{itX}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}}}dx\n \\end{aligned}\n$${#eq-1.1}\n\\[\\begin{eqnarray}\\label{1.1}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & E\\left[e^{it\\left(\\frac{X-\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & E\\left[e^{\\left(\\frac{itX}{\\sigma}-\\frac{it\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}E\\left[e^{\\left(\\frac{itX}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}}}dx\n\\end{eqnarray}\\] Observemos que, \\[\\begin{eqnarray}\\label{1.2}\n\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}} & = & \\frac{x^{2}-2x\\mu+\\mu^{2}-2itx\\sigma}{\\sigma^{2}}\\nonumber\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x\\mu}{\\sigma^{2}}+\\frac{\\mu^{2}}{\\sigma^{2}}-\\frac{2itx\\sigma}{\\sigma^{2}}\\nonumber\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x}{\\sigma}\\left(\\frac{\\mu+it\\sigma}{\\sigma^{2}}\\right)+\\frac{\\mu^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)^{2}+\\frac{\\mu^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\sigma\\mu}{\\sigma^{2}}-\\frac{(it\\sigma)^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}.\n\\end{eqnarray}\\]\nSustituyendo (\\(\\ref{1.2}\\)) en ((eq?)-{1.1}), resulta\n\\[\\begin{eqnarray}\\label{1.3}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left[\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}\\right]}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}e^{\\frac{it\\mu}{\\sigma}-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\\nonumber\\\\\n& = & e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\n\\end{eqnarray}\\]\nSea \\(u=\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\Longrightarrow du=\\frac{1}{\\sigma}dx\\), sustituyendo esto en (\\(\\ref{1.3}\\)), resulta \\[\\begin{equation}\\label{1.4}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) = e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}du\n\\end{equation}\\]\nde aquí se sigue que \\(u\\thicksim N(0,1)\\), entonces \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}dx=1.\n\\] sustituyendo esto ultimo en (\\(\\ref{1.4}\\)), se tiene,\n\\[\\begin{equation}\\label{1.5}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) =e^{-\\frac{t^{2}}{2}},\n\\end{equation}\\] Por otro lado, consideremos \\(Z\\thicksim N(0,1)\\), entonces \\[\\begin{equation*}\n\\varphi_{Z}(t) =e^{-\\frac{t^{2}}{2}}.\n\\end{equation*}\\]\nEntonces \\(\\varphi_{Z}(t)=\\varphi_{\\frac{X-\\mu}{\\sigma}}(t)\\), como las funciones características coinciden se concluye que \\(\\frac{X-\\mu}{\\sigma}\\thicksim N(0,1)\\).\n\n\nExercise 5.2 (Ejercicio 2:) Si \\(Y\\thicksim N(0,1)\\) entonces \\(\\sigma Y+\\mu \\thicksim N(\\mu,\\sigma)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\sigma Y+\\mu\\), \\[\\begin{eqnarray}\\label{2.1}\n\\varphi_{\\sigma Y+\\mu}(t) & = & E\\left[e^{it(\\sigma Y+\\mu)}\\right]\\nonumber\\\\\n& = & E\\left[e^{it\\sigma Y+it\\mu}\\right]\\nonumber\\\\\n& = & e^{it\\mu}E\\left[e^{it\\sigma Y}\\right]\\nonumber\\\\\n& = & e^{it\\mu}\\int_{-\\infty}^{\\infty}e^{it\\sigma y}\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-y^{2}}{2}}dy\\nonumber\\\\\n& = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y^{2}-2yit\\sigma) }dy.\n\\end{eqnarray}\\]\nObservemos que, \\[\\begin{eqnarray}\\label{2.2}\ny^{2}-2yit\\sigma & = & (y-it\\sigma)^{2}-(it\\sigma)^{2}\\nonumber\\\\\n& = & (y-it\\sigma)^{2}+t^{2}\\sigma^{2}.\n\\end{eqnarray}\\]\nSustituyendo, (\\(\\ref{2.2}\\)) en (\\(\\ref{2.1}\\)) resulta \\[\\begin{eqnarray}\\label{2.3}\n\\varphi_{\\sigma Y+\\mu}(t) & = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}((y-it\\sigma)^{2}+t^{2}\\sigma^{2}) }dy\\nonumber\\\\\n& = & e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy\n\\end{eqnarray}\\]\nTomando \\(u=y-it\\sigma\\Longrightarrow du=dy\\), se tiene que \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2} }{2}}du,\n\\] entonces \\(U\\thicksim N(0,1)\\), por lo tanto, \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=1\n\\]\nsustituyendo esto ultimo en (\\(\\ref{2.3}\\)), resulta, \\[\n\\varphi_{\\sigma Y+\\mu}(t)=e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] Sea \\(Z\\) una variable aleatoria tal que \\(Z\\thicksim N(\\mu,\\sigma)\\) sabemos que, \\[\n\\varphi_{Z}(t)=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] De estas dos ultimas igualdades se sigue que, \\[\n\\varphi_{Z}(t)=\\varphi_{\\sigma Y+\\mu}(t).\n\\] Dado que tienen iguales funciones características se concluye que, \\[\n\\sigma Y+\\mu\\thicksim N(\\mu,\\sigma)\n\\]\n\n\nExercise 5.3 (Ejercicio 3:) Si \\(X\\thicksim N(\\mu_{1},\\sigma_{1}^{2})\\), \\(Y\\thicksim N(\\mu_{2},\\sigma_{2}^{2})\\) además \\(X\\) y \\(Y\\) son independientes entonces \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nProof. Por definición, se tiene que, \\[\\begin{eqnarray}\\label{3.1}\n\\varphi_{X+Y}(t) & = & E[e^{it(X+Y)}]\\nonumber\\\\\n& = & E[e^{itX}e^{itY}]\\text{ por ser independientes, del ejercicio 4}\\nonumber\\\\\n& = & E[e^{itX}]E[e^{itY}]\\nonumber\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t).\n\\end{eqnarray}\\]\nPor otro lado, sea \\(Z\\) una variables aleatoria tal que, \\(Z\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\), sabemos que la función característica de \\(Z\\), esta dada por,\n\\[\\begin{eqnarray*}\n\\varphi_{Z}(t) & = & e^{it(\\mu_{1}+\\mu_{2})-\\frac{t^{2}}{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\nonumber\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}+it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\nonumber\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}}e^{it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t),\n\\end{eqnarray*}\\] entonces, de esta ultima igualdad y de (\\(\\ref{3.1}\\)) se sigue que, \\[\n\\varphi_{Z}(t)= \\varphi_{X+Y}(t).\n\\]\nComo las funciones características coinciden se sigue que, \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nExercise 5.4 (Ejercicio 4:) Si \\(X\\), \\(Y\\) son variables aleatorias normales entonces \\(X\\), \\(Y\\) son independientes si y solo si \\(E(XY)=E(X)E(Y)\\).\n\n\nProof. Primero recordemos que \\[\nE\\left(XY\\right)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\n\\]\nComo \\(X,Y\\) son independientes, sabemos que \\[\nf_{XY}\\left(x,y\\right)=f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\n\\]\nEntonces\n\\[\\begin{align*}\nE\\left(XY\\right) & =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{X}\\left(x\\right)f_{y}\\left(y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& =\\left(\\int_{-\\infty}^{\\infty}xf_{X}\\left(x\\right)\\mathrm{d}x\\right)\\left(\\int_{-\\infty}^{\\infty}yf_{y}\\left(y\\right)\\mathrm{d}y\\right)\\\\\n& =E\\left(X\\right)E\\left(Y\\right)\n\\end{align*}\\]\n\n\nTheorem 5.1 (Desigualdad de Chebyshev) Sea \\(X\\) una variable aleatoria con esperanza \\(\\mu=E(X)\\) y sea \\(\\varepsilon&gt;0\\). Entonces \\[\nP(|X-\\mu|\\geq\\varepsilon)\\leq\\frac{Var(X)}{\\varepsilon^{2}}\n\\]\n\n\nProof. Sea \\(Y=\\left|X-\\mu\\right|\\), observemos que \\(Y\\) es positiva, así por la desigualdad de Markov y dado que \\(\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] =\\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\), se cumple que\n\\[\\begin{align*}\n\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] & =\\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\\\\n& \\leq\\dfrac{E\\left[\\left(X-\\mu\\right)^{2}\\right]}{\\epsilon^{2}}=\\dfrac{\\text{Var}\\left[X\\right]}{\\epsilon^{2}}\n\\end{align*}\\]\n\n\nTheorem 5.2 (Ley de los grandes números) Sean \\(X_{1},X_{2},\\dots, X_{n}\\) procesos de ensayos independientes, con esperanza finita \\(\\mu=E(X_{j})\\) y varianza finita \\(\\sigma^{2}=Var(X_{j})\\). Sean \\(S_{n}=X_{1}+X_{2}+\\ldots+X_{n}\\). Entonces para cada \\(\\epsilon&gt;0\\).\n\\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nProof. Observemos que \\[\\begin{align*}\n\\text{Var}\\left[\\dfrac{S_{n}}{n}-\\mu\\right] & =\\dfrac{1}{n^{2}}\\text{Var}\\left(S_{n}\\right)\\\\\n& =\\dfrac{1}{n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(X_{i}\\right),\\text{ por ser iid}\\\\\n& =\\dfrac{\\sigma^{2}}{n}\n\\end{align*}\\]\nEntonces, por el Teorema 5.1, \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\leq\\dfrac{\\sigma^{2}}{n\\epsilon},\n\\] así, tomando el limite cuando \\(n\\to\\infty\\) \\[\n\\dfrac{\\sigma^{2}}{n\\epsilon}\\to0.\n\\]\nEntonces \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nTheorem 5.3 (Teorema del Limite Central) Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una secuencia de v.a.i.id con media \\(a\\) y varianza \\(b^{2}\\). Entonces para doo \\(\\alpha,\\beta\\in\\mathbb{R}\\), con \\(\\alpha&lt;\\beta\\), entonces \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right)=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}e^{\\left(-\\dfrac{1}{2}x^{2}\\right)}\\mathrm{d}x\n\\]\n\n\nProof. Definamos a \\[\nS_{M}={\\displaystyle \\sum_{i=1}^{M}}\\left[X_{i}-a\\right],\n\\] y \\[\nY_{M}=\\dfrac{S_{M}}{\\sqrt{M}b}.\n\\] Sea \\(\\varphi_{Y_{M}}\\) la función generadora de momentos de \\(Y_{M}\\) y \\(\\varphi\\) la función generadora de momentos de la distribución normal estándar, demostraremos que \\(\\varphi_{Y_{M}}\\to\\varphi\\).\nPor definición, \\[\\begin{align*}\n\\varphi_{Y_{M}}\\left(t\\right) & =E\\left[\\exp\\left(t\\dfrac{S_{M}}{\\sqrt{Mb}}\\right)\\right]\\\\\n& =\\varphi_{S_M}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\\\\n& =\\left[\\varphi_{\\left(X_{1}-a\\right)}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\right]^{M} \\text{ ya que, las }X_{i}\\text{ son i.i.d}\\\\\n& =\\left[E\\left[\\exp\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)\\right]\\right]^{M}\n\\end{align*}\\]\nRecordando la serie de Taylor \\[\\begin{align*}\n\\varphi_{Y_M}\\left(t\\right) & =\\left[\\sum_{i=0}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!}\\right]^{M}\\\\\n& =\\left[1+\\dfrac{1}{2}\\left(\\dfrac{t}{b\\sqrt{M}}\\right)^{2}E\\left[\\left(X_{1}-a\\right)^{2}\\right]+\\epsilon\\left(3\\right)\\right]^{M}\\\\\n& =\\left[1+\\dfrac{1}{M}\\dfrac{t^{2}}{2}+\\epsilon\\left(3\\right)\\right]^{M},\n\\end{align*}\\]\ndonde \\[\\begin{align*}\n\\epsilon\\left(3\\right) & =\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!},\n\\end{align*}\\]\nAhora sea \\(s=\\dfrac{t}{b\\sqrt{M}},\\) así,\n\\[\n\\epsilon\\left(3\\right)=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i}}{i!}\n\\] Además observemos que, cuando \\(t\\to0\\), \\(s\\to0\\).\nAsí, de lo anterior, si \\(\\varphi_{1}\\) existe, se cumple que, \\[\n\\dfrac{\\epsilon\\left(3\\right)}{s^{2}}=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i-2}}{i!}\\to0,\\text{ cuando, }s\\to0.\n\\]\nPor otro lado,\n\\[\n\\varphi_{Y_M}\\left(t\\right)=\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M},\n\\] y \\(s\\to0\\) cuando \\(M\\to\\infty\\).\nEntonces \\(\\epsilon\\left(3\\right)s^{-2}=M\\epsilon\\left(3\\right)b^{2}t^{-2}\\to0\\). Dado que \\(b,t\\) estan fijas, se cumple que \\[\nM\\epsilon\\left(3\\right)\\to0,\\text{ cuando, }M\\to\\infty,\n\\]\npor lo tanto \\[\n\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right) \\to\\dfrac{t^{2}}{2},\\text{ cuando, }M\\to\\infty\n\\] esto implica que,\n\\[\n\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M} \\to\\exp\\left(t^{2}/2\\right),M\\to\\infty\n\\]\nDe aqui se concluye que, \\[\n\\lim_{M\\to\\infty}\\varphi_{M}\\left(t\\right)  =\\exp\\left(t^{2}/2\\right)=\\varphi\\left(t\\right)\n\\]\nla cual es la función generadora de momentos de la distribución normal estándar. Por lo tanto \\[\nF_{M}\\left(x\\right)\\to F_{N\\left(0,1\\right)}\\left(x\\right)\n\\] que es equivalente a,\n\\[\nF_{M}\\left(b\\right)-F_{M}\\left(a\\right)  \\to F_{N}\\left(b\\right)-F_{N}\\left(a\\right)\n\\] \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right) =\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}\\exp\\left(-\\dfrac{1}{2}x^{2}\\right)\\mathrm{d}x\n\\]\n\n\nTheorem 5.4 Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una sucesión de v.a.i.i.d con media \\(a\\). Entonces \\[\n\\mathcal{P}\\left[\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}=a\\right]=1.\n\\]\n\n\nProof. Esto es similar a decir que \\[\n\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\stackrel{\\text{c.s}}{=}a\n\\]\nSin perdida de generalidad, diremos que \\(X_{i}\\geq0,\\forall i\\). Definamos \\[\nY_{n}=X_{n}I_{\\left[\\left|X_{n}\\right|\\leq n\\right]},Q_{n}=\\sum_{i=1}^{n}Y_{i}\n\\]\nPor la desigualdad de \\[\\begin{align*}\n\\sum_{n=1}^{\\infty}\\mathcal{P}\\left[\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right] & \\leq\\sum_{n=1}^{\\infty}\\dfrac{\\text{Var}\\left(Q_{n}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(Y_{i}\\right)\\\\\n& \\leq\\sum_{n=1}^{\\infty}\\dfrac{E\\left(Y_{n}^{2}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\int_{0}^{n}x^{2}\\mathrm{d}F\\\\\n& \\leq\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}}\\int_{0}^{n}x\\mathrm{d}F&lt;\\infty,\n\\end{align*}\\]\ndonde \\(F\\) es la función de distribución de \\(X_{i}\\). Luego \\[\nE\\left[X_{1}\\right]=\\lim_{n\\to\\infty}\\int_{0}^{n}x\\mathrm{d}F=\\lim_{n\\to\\infty}E\\left[Y_{n}\\right]=\\lim_{n\\to\\infty}\\dfrac{E\\left[Q_{n}\\right]}{n}.\n\\]\nEntonces, por el Lema de Borel Canteli. \\(\\mathcal{\\mathcal{P}}\\left[\\limsup\\left(\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right)\\right]=0\\)\n\\[\n\\lim_{n\\to\\infty}\\dfrac{Q_{n}}{n}=E\\left[X_{1}\\right],\\text{c.s}\n\\]\nAhora, calcularemos la siguiente probabilidad \\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}\\neq Y_{i}\\right]=\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\n\\]\ncomo \\(E\\left[X_{i}\\right]&lt;\\infty\\) y \\(X_{i}\\) son v.a.i.i.d.\n\\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\\leq E\\left[X_{1}\\right]&lt;\\infty\n\\]\nDe nuevo, por el Lema de Borel Cantelli. \\[\n\\mathcal{P}\\left[\\limsup\\left[X_{i}\\neq Y_{i}\\right]\\right]=0,\\forall i\n\\]\nEntonces \\[\\begin{align*}\nX_{i} & =Y_{i},\\text{c.s}\\\\\n\\Rightarrow & \\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\to E\\left[X_{1}\\right]=\\mu.\\text{ c.s}\n\\end{align*}\\]"
  },
  {
    "objectID": "Tarea4.html#ejercicio-1",
    "href": "Tarea4.html#ejercicio-1",
    "title": "6  Tarea 4",
    "section": "6.1 Ejercicio 1",
    "text": "6.1 Ejercicio 1\nSea \\(W(t)\\) un movimiento Browniano estándar en \\([0,T]\\). Pruebe que para cualquier \\(c&gt;0\\) fijo, \\[\nV(t) = \\dfrac{1}{c} W(c^2 t)\n\\]\nes un movimiento Browniano sobre \\([0,T]\\).\n\n6.1.1 Demostración\nVeamos que \\(V\\) cumple las propiedades del movimiento Browniano.\n\n6.1.1.1 Propiedad 1 (Que comience en 0)\nSe tiene que, \\(V(0) = \\dfrac{1}{c} W (c^2\\cdot0)=0\\).\n\n\n6.1.1.2 Propiedad 2 (Incrementos Independientes)\nSean \\(s&lt;t&lt;u&lt;v\\), por definición de \\(V\\), se tiene que, \\[\nE[\\left(V(t)-V(s)\\right)\\left(V(v)-V(u)\\right)]=\\dfrac{1}{c^2}E[\\left(W(c^2 t)-W(c^2 s)\\right)\\left(W(c^2 v)-W(c^2 u)\\right)]\n\\]\nDado que \\(W\\) tiene incrementos independientes, se cumple que. \\[\\begin{align*}\n\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right] & =\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\right]E\\left[\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right]\n\\end{align*}\\]\nEntonces \\(V\\) tiene incrementos independientes.\n\n\n6.1.1.3 Propiedad 3 (Incrementos estacionarios)\nSea \\(s&lt;t\\). \\[\nV(t)-V(s)=\\dfrac{1}{c}\\left[W(c^2 t) - W(c^2 s)\\right]\n\\]\nPor las propiedades de la definicion del movimiento Browniano.\n\\[\\begin{align*}\nE\\left[V(t)-V(s)\\right] & =\\dfrac{1}{c}E\\left[W(c^{2}t)-W(c^{2}s)\\right]=0\\\\\n\\text{Var}\\left[V(t)-V(s)\\right] & =\\dfrac{1}{c^{2}}\\text{Var}\\left[W(c^{2}t)-W(c^{2}s)\\right]=\\dfrac{1}{c^{2}}\\left(c^{2}\\left(t-s\\right)\\right)=t-s\n\\end{align*}\\]\nEntonces \\(V\\) tiene incrementos estacionarios.\nCon todo lo anterior se concluye que, \\(V\\) es un movimiento browniano."
  },
  {
    "objectID": "Tarea4.html#ejercicio-2",
    "href": "Tarea4.html#ejercicio-2",
    "title": "6  Tarea 4",
    "section": "6.2 Ejercicio 2",
    "text": "6.2 Ejercicio 2\nHacer un script para ilustrar la propiedad de escalado del movimiento Browniano para el caso de \\(c = \\dfrac{1}{5}\\). Estar seguro que usa el mismo camino browniano discretizado en cada subplot.\n\n\nBrowniano escalado, con c=1/5.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\nT = 1  \nn= 100  \ndt = 1 / (n - 1)\ndw = np.sqrt(dt) * prng.standard_normal(n - 1) \nw = np.concatenate(([0],dw.cumsum()))\n\ntime = np.linspace(0,T, n)\nc = 0.2  # 1/5\nc_time = c**2 * time  \nc_w = c**(-1) * w  \n\nfig, browniano_escalado = plt.subplots(2)\nbrowniano_escalado[0].plot(time, w)\nbrowniano_escalado[1].plot(c_time, c_w)\nbrowniano_escalado[0].set_title('Movimiento browniano')\nbrowniano_escalado[1].set_title('Moviemiento browniano escalado')\nplt.show()\n\n\n\n\nFigura 1"
  },
  {
    "objectID": "Tarea4.html#ejercicio-3",
    "href": "Tarea4.html#ejercicio-3",
    "title": "6  Tarea 4",
    "section": "6.3 Ejercicio 3",
    "text": "6.3 Ejercicio 3\nModifique el script half_brownian_refinement.py encapsulando el código en una función. Esta función deberá recibir el extremo derecho del intervalo \\([0, T]\\) y el número de incrementos \\(N\\) de un camino browniano base. El propósito es calcular los incrementos de relleno de una refinamiento con \\(2N\\) incrementos.\n\n\nBrowniano refinado, con refinamiento 2N.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\n\ndef refined_brownian_2n(T,L):\n    dt = T / L\n    W = np.zeros(L + 1)\n    W_refined = np.zeros(2 * L + 1)\n    xi = np.sqrt(dt) * prng.normal(size=L)\n    xi_half = np.sqrt(0.5 * dt) * prng.normal(size=L)\n    W[1:] = xi.cumsum()\n    W_ = np.roll(W, -1)\n    W_half = 0.5 * (W + W_)\n    W_half = np.delete(W_half, -1) + xi_half\n    W_refined[1::2] = W_half\n    W_refined[2::2] = W[1:]\n    t = np.arange(0, T + dt, dt)\n    t_half = np.arange(0, T + 0.5 * dt, 0.5 * dt)\n    return t,t_half,W, W_refined"
  },
  {
    "objectID": "Tarea4.html#ejercicio-4",
    "href": "Tarea4.html#ejercicio-4",
    "title": "6  Tarea 4",
    "section": "6.4 Ejercicio 4",
    "text": "6.4 Ejercicio 4\nEn un script separado, incluya la función de arriba y grafique una figura con la trayectoria del browniano con 100 incrementos y muestre su refinamiento correspondiente.\n\n\nBrowniano refinado, con refinamiento 2N y 100 incrementos.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h_b_r as hbr\n\na, b, c, d = hbr.refined_brownian_2n(1, 100)\n\nplt.plot(a, c, 'r-+')\nplt.plot(\n    b,\n    d,\n    'g*--',\n    # alpha = transparecia\n    )\nplt.show()\n\n\n\n\nFigura 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]