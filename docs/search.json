[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matematicas aplicadas",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "3  Tarea 1",
    "section": "",
    "text": "Exercise 3.1 Se generan variables aleatorias Bernoulli y el histograma de los valores que toma con paremetro \\(p=0.3\\).\n\n\n\nExploring functions to generate random variables with a Bernoulli distribution.py\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\nfig_01, ax_01 = plt.subplots(1, 1)\nfig_02, ax_02 = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(mean, var, skew,kurt)\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax_01.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax_01.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\nr = bernoulli.rvs(p, size=1000)\nax_02.hist(r, bins=200)\nplt.show()\n\n \n\nExercise 3.2 Se generan variables aleatorias normales y el histograma de los valores que toma.\n\n\n\nExploring functions to generate random variables with a Gaussian distribution.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1, 1)\nmean, var, skew, kurt = norm.stats(moments='mvsk')\n\nx = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\nax.plot(\n    x,\n    norm.pdf(x),\n    'r-',\n    lw=5,\n    alpha=0.6,\n    label='norm pdf'\n)\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nvals = norm.ppf([0.001, 0.5, 0.999])\n\nnp.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n\nr = norm.rvs(size=50000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\nax.set_xlim([x[0], x[-1]])\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\nFigura 3\n\n\n\nExercise 3.3 Modificando reproducir el gráfico de una distribución gaussiana bivariada con media vectorial \\(\\mu[0.1, 0.5]\\) y matriz de covarianza \\[\\Sigma=\n\\begin{bmatrix}\n3.0 & 0.3\\\\\n0.75 & 1.5\n\\end{bmatrix}\n\\]\n\n\n\nRevising multivariate Gaussian.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nfrom scipy.stats import multivariate_normal\n\nx = np.linspace(0, 5, 100, endpoint=False)\ny = multivariate_normal.pdf(x, mean=2.5, cov=0.5);\n\nfig1 = plt.figure()\nax = fig1.add_subplot(111)\nax.plot(x, y)\n# plt.show()\n\nx, y = np.mgrid[-5:5:.1, -5:5:.1]\npos = np.dstack((x, y))\nrv = multivariate_normal([0.1, 0.5], [[3.0, 0.3], [0.75, 1.5]])\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nax2.contourf(x, y, rv.pdf(pos))\n# plt.show()\n\nax = plt.figure().add_subplot(projection='3d')\nax.plot_surface(\n    x,\n    y,\n    rv.pdf(pos),\n    edgecolor='royalblue',\n    lw=0.5,\n    rstride=8,\n    cstride=8,\n    alpha=0.4\n)\nax.contour(x, y, rv.pdf(pos), zdir='z', offset=-.2, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='x', offset=-5, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='y', offset=5, cmap='coolwarm')\n\nax.set(\n    xlim=(-5, 5),\n    ylim=(-5, 5),\n    zlim=(-0.2, 0.2),\n    xlabel='X',\n    ylabel='Y',\n    zlabel='Z'\n)\nplt.show()"
  },
  {
    "objectID": "Tarea2.html#demostración",
    "href": "Tarea2.html#demostración",
    "title": "4  Tarea 2",
    "section": "Demostración:",
    "text": "Demostración:\nConsidere una caminata aleatoria que comienza en 0 con saltos \\(h\\) y \\(-h\\) igualmente probables en los momentos \\(\\delta\\), 2 \\(\\delta\\),\\(\\dots\\), donde \\(h\\) y \\(\\delta\\) son números positivos. Más precisamente, sea \\(\\{X_{n}\\}_{n=1}^{\\infty}\\) una sucesión de elementos aleatorios independientes e idénticamente distribuidos. variables con \\[\nP\\left[X_{i}=h\\right]=P\\left[X_{i}=-h\\right]=\\dfrac{1}{2},\\forall i,\n\\]\nSea \\(Y_{\\delta,h}(0)=0\\) y pongamos \\[\nY_{\\delta,h}(n\\delta)=X_{1}+X_{2}+\\cdots+X_{n}.\n\\]\nPara \\(t&gt;0\\), defina \\(Y_{\\delta,h}(t)\\) mediante linealización, es decir, para \\(n\\delta&lt;t&lt;(n + 1)\\delta\\), defina \\[\nY_{\\delta,h}(t)=\\frac{(n+1)\\delta-t}{\\delta}Y_{\\delta,h}(n\\delta)+\\frac{t-n\\delta}{\\delta}Y_{\\delta,h}((n+1)\\delta).\n\\]\nCalculemos la función característica de \\(Y_{\\delta,h}(t)\\), donde \\(\\lambda\\in\\mathbb{R}\\) fijo y sea \\(t=n\\delta\\) así, \\(n=t/\\delta\\). Entonces se tiene que\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right] & = & \\prod_{j=1}^{n}Ee^{i\\lambda X_{j}},\\text{ por ser variables independientes,}\\\\\n    & = & (Ee^{i\\lambda X_{j}})^{n},\\text{ por ser idénticamente distribuidas,}\\\\\n    & = & \\frac{1}{2}(e^{i\\lambda h}+e^{-i\\lambda h})^{n},\\\\\n    & = & (\\cos(\\lambda h))^{n},\\\\\n    & = & (\\cos(\\lambda h))^{t/\\delta},\n\\end{eqnarray}\\] \\[\n\\\n\\tag{4.1}\\]\nPor otro lado, sea \\(u=\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\Rightarrow\\ln\\left(u\\right)=\\dfrac{1}{\\delta}\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]\\).\nUsando la expansión de Taylor de \\(\\cos\\left(x\\right)\\) se tiene que \\[\n\\cos\\left(\\lambda h\\right)\\approx1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!},\n\\]\nentonces\n\\[\\begin{eqnarray}\n    \\ln\\left(\\cos\\left(\\lambda h\\right)\\right) & \\approx & \\ln\\left[1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right]\\\\\n& \\approx & -\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}-\\frac{1}{2}\\left(-\\frac{\\lambda^{2}h^{2}}{2!}+\\frac{\\lambda^{4}h^{4}}{4!}\\right)^{2}\\\\\n  & = & -\\dfrac{\\lambda^{2} h^{2}}{2!}+\\dfrac{\\lambda ^{4}h^{4}}{4!}-\\frac{1}{2}\\left(\\frac{\\lambda^{4}h^{4}}{4}-\\frac{\\lambda^{6}h^{6}}{24^{2}}+\\frac{\\lambda^{8}h^{8}}{24}\\right)\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}+\\dfrac{\\lambda^{4} h^{4}}{24}-\\frac{\\lambda^{4}h^{4}}{8}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\n\\end{eqnarray}\\]\npara una \\(h\\) pequeña, se satisface que, \\[\n-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\approx 0\n\\]\nPor lo tanto, \\(\\ln\\left(\\cos\\left(\\lambda h\\right)\\right)\\approx -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\).\\ Así, para \\(\\delta\\) y \\(h\\) pequeña, se tiene que \\(\\ln u\\approx \\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\).\\ Entonces\n\\[\\begin{equation}\n    u\\approx\\exp\\left[\\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\right]\n\\end{equation}\\]\nEntonces por la ecuación (Equation 4.1),\n\\[\\begin{equation}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\approx\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\n\\end{equation}\\]\nCalculando el limite \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left[-t\\left(\\left[\\dfrac{h^{2}}{\\delta}\\right]\\left(\\dfrac{\\lambda^{2}}{2}-\\dfrac{\\lambda^{4}h^{2}}{24}\\right)\\right)\\right],\n\\]\nAsumamos que \\(\\delta\\to0\\), \\(h\\to0\\) pero \\(h^{2}/\\delta\\to\\infty\\). Entonces \\(\\lim_{\\delta\\to0} Y_{\\delta, h}(t)\\) no existe. Por otro lado, consideremos la siguiente renormalización,\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right] & = & E\\left[\\exp (i\\lambda Y_{n,\\delta}\\left(t\\right))\\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\right]\\\\\n    & = & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)E\\exp\\left[ i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\\\\n    & \\approx & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\\\\\n    & = & \\exp\\left(-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right)\n\\end{eqnarray}\\]\nAsí, si \\(\\delta,h\\to0\\) de tal manera que \\(h^{2}/\\delta\\to\\infty\\) y \\(h^{4}/\\delta\\to0\\), entonces \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2}\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left(\\dfrac{\\left(\\lambda h\\right)^{4}}{24\\delta}\\right)=1\n\\]"
  },
  {
    "objectID": "Tarea3.html",
    "href": "Tarea3.html",
    "title": "5  Tarea 3",
    "section": "",
    "text": "Exercise 5.1 Si \\(X\\thicksim N(\\mu,\\sigma^{2})\\) entonces \\(\\left(\\dfrac{X-\\mu}{\\sigma}\\right)\\thicksim N(0,1)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\dfrac{X-\\mu}{\\sigma}\\),\n\\[\\begin{eqnarray}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & E\\left[e^{it\\left(\\frac{X-\\mu}{\\sigma}\\right)}\\right]\\\\\n& = & E\\left[e^{\\left(\\frac{itX}{\\sigma}-\\frac{it\\mu}{\\sigma}\\right)}\\right]\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}E\\left[e^{\\left(\\frac{itX}{\\sigma}\\right)}\\right]\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}}}dx\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.1}\\]\nObservemos que,\n\\[\\begin{eqnarray}\\label{1.2}\n\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}} & = & \\frac{x^{2}-2x\\mu+\\mu^{2}-2itx\\sigma}{\\sigma^{2}}\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x\\mu}{\\sigma^{2}}+\\frac{\\mu^{2}}{\\sigma^{2}}-\\frac{2itx\\sigma}{\\sigma^{2}}\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x}{\\sigma}\\left(\\frac{\\mu+it\\sigma}{\\sigma^{2}}\\right)+\\frac{\\mu^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)^{2}+\\frac{\\mu^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\sigma\\mu}{\\sigma^{2}}-\\frac{(it\\sigma)^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.2}\\]\nSustituyendo (Equation 5.2) en (Equation 5.1), resulta\n\\[\\begin{eqnarray}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left[\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}\\right]}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}e^{\\frac{it\\mu}{\\sigma}-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\\\\\n& = & e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.3}\\]\nSea \\(u=\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\Longrightarrow du=\\frac{1}{\\sigma}dx\\), sustituyendo esto en (Equation 5.3), resulta\n\\[\\begin{equation}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) = e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}du\n\\end{equation}\\] \\[\n\\\n\\tag{5.4}\\]\nde aquí se sigue que \\(u\\thicksim N(0,1)\\), entonces \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}dx=1.\n\\] sustituyendo esto ultimo en (Equation 5.4), se tiene,\n\\[\\begin{equation}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) =e^{-\\frac{t^{2}}{2}},\n\\end{equation}\\] \\[\n\\\n\\tag{5.5}\\] Por otro lado, consideremos \\(Z\\thicksim N(0,1)\\), entonces\n\\[\\begin{equation*}\n\\varphi_{Z}(t) =e^{-\\frac{t^{2}}{2}}.\n\\end{equation*}\\]\nEntonces \\(\\varphi_{Z}(t)=\\varphi_{\\frac{X-\\mu}{\\sigma}}(t)\\), como las funciones características coinciden se concluye que \\(\\frac{X-\\mu}{\\sigma}\\thicksim N(0,1)\\).\n\n\nExercise 5.2 Si \\(Y\\thicksim N(0,1)\\) entonces \\(\\sigma Y+\\mu \\thicksim N(\\mu,\\sigma)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\sigma Y+\\mu\\),\n\\[\\begin{eqnarray}\n\\varphi_{\\sigma Y+\\mu}(t) & = & E\\left[e^{it(\\sigma Y+\\mu)}\\right]\\\\\n& = & E\\left[e^{it\\sigma Y+it\\mu}\\right]\\\\\n& = & e^{it\\mu}E\\left[e^{it\\sigma Y}\\right]\\\\\n& = & e^{it\\mu}\\int_{-\\infty}^{\\infty}e^{it\\sigma y}\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-y^{2}}{2}}dy\\\\\n& = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y^{2}-2yit\\sigma) }dy.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.6}\\]\nObservemos que,\n\\[\\begin{eqnarray}\ny^{2}-2yit\\sigma & = & (y-it\\sigma)^{2}-(it\\sigma)^{2}\\\\\n& = & (y-it\\sigma)^{2}+t^{2}\\sigma^{2}.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.7}\\]\nSustituyendo, (Equation 5.7) en (Equation 5.6) resulta\n\\[\\begin{eqnarray}\n\\varphi_{\\sigma Y+\\mu}(t) & = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}((y-it\\sigma)^{2}+t^{2}\\sigma^{2}) }dy\\\\\n& = & e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.8}\\]\nTomando \\(u=y-it\\sigma\\Longrightarrow du=dy\\), se tiene que \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2} }{2}}du,\n\\] entonces \\(U\\thicksim N(0,1)\\), por lo tanto, \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=1\n\\]\nsustituyendo esto ultimo en (Equation 5.8), resulta, \\[\n\\varphi_{\\sigma Y+\\mu}(t)=e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] Sea \\(Z\\) una variable aleatoria tal que \\(Z\\thicksim N(\\mu,\\sigma)\\) sabemos que, \\[\n\\varphi_{Z}(t)=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] De estas dos ultimas igualdades se sigue que, \\[\n\\varphi_{Z}(t)=\\varphi_{\\sigma Y+\\mu}(t).\n\\] Dado que tienen iguales funciones características se concluye que, \\[\n\\sigma Y+\\mu\\thicksim N(\\mu,\\sigma)\n\\]\n\n\nExercise 5.3 Si \\(X\\thicksim N(\\mu_{1},\\sigma_{1}^{2})\\), \\(Y\\thicksim N(\\mu_{2},\\sigma_{2}^{2})\\) además \\(X\\) y \\(Y\\) son independientes entonces \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nProof. Por definición, se tiene que,\n\\[\\begin{eqnarray}\n\\varphi_{X+Y}(t) & = & E[e^{it(X+Y)}]\\\\\n& = & E[e^{itX}e^{itY}]\\text{ por ser independientes, del ejercicio 4}\\\\\n& = & E[e^{itX}]E[e^{itY}]\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t).\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.9}\\]\nPor otro lado, sea \\(Z\\) una variables aleatoria tal que, \\(Z\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\), sabemos que la función característica de \\(Z\\), esta dada por,\n\\[\\begin{eqnarray*}\n\\varphi_{Z}(t) & = & e^{it(\\mu_{1}+\\mu_{2})-\\frac{t^{2}}{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}+it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}}e^{it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t),\n\\end{eqnarray*}\\]\nentonces, de esta ultima igualdad y de (Equation 5.9) se sigue que, \\[\n\\varphi_{Z}(t)= \\varphi_{X+Y}(t).\n\\]\nComo las funciones características coinciden se sigue que, \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nExercise 5.4 (Ejercicio 4:) Si \\(X\\), \\(Y\\) son variables aleatorias normales entonces \\(X\\), \\(Y\\) son independientes si y solo si \\(E(XY)=E(X)E(Y)\\).\n\n\nProof. Primero recordemos que \\[\nE\\left(XY\\right)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\n\\]\nComo \\(X,Y\\) son independientes, sabemos que \\[\nf_{XY}\\left(x,y\\right)=f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\n\\]\nEntonces\n\\[\\begin{eqnarray}\nE\\left(XY\\right) & = & \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& = & \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{X}\\left(x\\right)f_{y}\\left(y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& = & \\left(\\int_{-\\infty}^{\\infty}xf_{X}\\left(x\\right)\\mathrm{d}x\\right)\\left(\\int_{-\\infty}^{\\infty}yf_{y}\\left(y\\right)\\mathrm{d}y\\right)\\\\\n& = & E\\left(X\\right)E\\left(Y\\right)\n\\end{eqnarray}\\]\n\n\nTheorem 5.1 (Desigualdad de Chebyshev) Sea \\(X\\) una variable aleatoria con esperanza \\(\\mu=E(X)\\) y sea \\(\\varepsilon&gt;0\\). Entonces \\[\nP(|X-\\mu|\\geq\\varepsilon)\\leq\\frac{Var(X)}{\\varepsilon^{2}}\n\\]\n\n\nProof. Sea \\(Y=\\left|X-\\mu\\right|\\), observemos que \\(Y\\) es positiva, así por la desigualdad de Markov y dado que \\(\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] =\\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\), se cumple que\n\\[\\begin{eqnarray}\n\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] & = & \\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\\\\n& \\leq & \\dfrac{E\\left[\\left(X-\\mu\\right)^{2}\\right]}{\\epsilon^{2}}=\\dfrac{\\text{Var}\\left[X\\right]}{\\epsilon^{2}}\n\\end{eqnarray}\\]\n\n\nTheorem 5.2 (Ley de los grandes números) Sean \\(X_{1},X_{2},\\dots, X_{n}\\) procesos de ensayos independientes, con esperanza finita \\(\\mu=E(X_{j})\\) y varianza finita \\(\\sigma^{2}=Var(X_{j})\\). Sean \\(S_{n}=X_{1}+X_{2}+\\ldots+X_{n}\\). Entonces para cada \\(\\epsilon&gt;0\\).\n\\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nProof. Observemos que\n\\[\\begin{eqnarray}\n\\text{Var}\\left[\\dfrac{S_{n}}{n}-\\mu\\right] & = & \\dfrac{1}{n^{2}}\\text{Var}\\left(S_{n}\\right)\\\\\n& = & \\dfrac{1}{n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(X_{i}\\right),\\text{ por ser iid}\\\\\n& = & \\dfrac{\\sigma^{2}}{n}\n\\end{eqnarray}\\]\nEntonces, por el Teorema 5.1, \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\leq\\dfrac{\\sigma^{2}}{n\\epsilon},\n\\] así, tomando el limite cuando \\(n\\to\\infty\\) \\[\n\\dfrac{\\sigma^{2}}{n\\epsilon}\\to0.\n\\]\nEntonces \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nTheorem 5.3 (Teorema del Limite Central) Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una secuencia de v.a.i.id con media \\(a\\) y varianza \\(b^{2}\\). Entonces para doo \\(\\alpha,\\beta\\in\\mathbb{R}\\), con \\(\\alpha&lt;\\beta\\), entonces \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right)=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}e^{\\left(-\\dfrac{1}{2}x^{2}\\right)}\\mathrm{d}x\n\\]\n\n\nProof. Definamos a \\[\nS_{M}={\\displaystyle \\sum_{i=1}^{M}}\\left[X_{i}-a\\right],\n\\] y \\[\nY_{M}=\\dfrac{S_{M}}{\\sqrt{M}b}.\n\\] Sea \\(\\varphi_{Y_{M}}\\) la función generadora de momentos de \\(Y_{M}\\) y \\(\\varphi\\) la función generadora de momentos de la distribución normal estándar, demostraremos que \\(\\varphi_{Y_{M}}\\to\\varphi\\).\nPor definición,\n\\[\\begin{eqnarray}\n\\varphi_{Y_{M}}\\left(t\\right) & = & E\\left[\\exp\\left(t\\dfrac{S_{M}}{\\sqrt{Mb}}\\right)\\right]\\\\\n& = & \\varphi_{S_M}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\\\\n& = & \\left[\\varphi_{\\left(X_{1}-a\\right)}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\right]^{M} \\text{ ya que, las }X_{i}\\text{ son i.i.d}\\\\\n& = & \\left[E\\left[\\exp\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)\\right]\\right]^{M}\n\\end{eqnarray}\\]\nRecordando la serie de Taylor\n\\[\\begin{eqnarray}\n\\varphi_{Y_M}\\left(t\\right) & = & \\left[\\sum_{i=0}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!}\\right]^{M}\\\\\n& = & \\left[1+\\dfrac{1}{2}\\left(\\dfrac{t}{b\\sqrt{M}}\\right)^{2}E\\left[\\left(X_{1}-a\\right)^{2}\\right]+\\epsilon\\left(3\\right)\\right]^{M}\\\\\n& = & \\left[1+\\dfrac{1}{M}\\dfrac{t^{2}}{2}+\\epsilon\\left(3\\right)\\right]^{M},\n\\end{eqnarray}\\]\ndonde\n\\[\\begin{eqnarray}\n\\epsilon\\left(3\\right) & = &\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!},\n\\end{eqnarray}\\]\nAhora sea \\(s=\\dfrac{t}{b\\sqrt{M}},\\) así,\n\\[\n\\epsilon\\left(3\\right)=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i}}{i!}\n\\] Además observemos que, cuando \\(t\\to0\\), \\(s\\to0\\).\nAsí, de lo anterior, si \\(\\varphi_{1}\\) existe, se cumple que, \\[\n\\dfrac{\\epsilon\\left(3\\right)}{s^{2}}=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i-2}}{i!}\\to0,\\text{ cuando, }s\\to0.\n\\]\nPor otro lado,\n\\[\n\\varphi_{Y_M}\\left(t\\right)=\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M},\n\\] y \\(s\\to0\\) cuando \\(M\\to\\infty\\).\nEntonces \\(\\epsilon\\left(3\\right)s^{-2}=M\\epsilon\\left(3\\right)b^{2}t^{-2}\\to0\\). Dado que \\(b,t\\) estan fijas, se cumple que \\[\nM\\epsilon\\left(3\\right)\\to0,\\text{ cuando, }M\\to\\infty,\n\\]\npor lo tanto \\[\n\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right) \\to\\dfrac{t^{2}}{2},\\text{ cuando, }M\\to\\infty\n\\] esto implica que,\n\\[\n\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M} \\to\\exp\\left(t^{2}/2\\right),M\\to\\infty\n\\]\nDe aqui se concluye que, \\[\n\\lim_{M\\to\\infty}\\varphi_{M}\\left(t\\right)  =\\exp\\left(t^{2}/2\\right)=\\varphi\\left(t\\right)\n\\]\nla cual es la función generadora de momentos de la distribución normal estándar. Por lo tanto \\[\nF_{M}\\left(x\\right)\\to F_{N\\left(0,1\\right)}\\left(x\\right)\n\\] que es equivalente a,\n\\[\nF_{M}\\left(b\\right)-F_{M}\\left(a\\right)  \\to F_{N}\\left(b\\right)-F_{N}\\left(a\\right)\n\\] \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right) =\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}\\exp\\left(-\\dfrac{1}{2}x^{2}\\right)\\mathrm{d}x\n\\]\n\n\nTheorem 5.4 Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una sucesión de v.a.i.i.d con media \\(a\\). Entonces \\[\n\\mathcal{P}\\left[\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}=a\\right]=1.\n\\]\n\n\nProof. Esto es similar a decir que \\[\n\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\stackrel{\\text{c.s}}{=}a\n\\]\nSin perdida de generalidad, diremos que \\(X_{i}\\geq0,\\forall i\\). Definamos \\[\nY_{n}=X_{n}I_{\\left[\\left|X_{n}\\right|\\leq n\\right]},Q_{n}=\\sum_{i=1}^{n}Y_{i}\n\\]\nPor la desigualdad de Chebyshev\n\\[\\begin{eqnarray}\n\\sum_{n=1}^{\\infty}\\mathcal{P}\\left[\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right] & \\leq & \\sum_{n=1}^{\\infty}\\dfrac{\\text{Var}\\left(Q_{n}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(Y_{i}\\right)\\\\\n& \\leq & \\sum_{n=1}^{\\infty}\\dfrac{E\\left(Y_{n}^{2}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\int_{0}^{n}x^{2}\\mathrm{d}F\\\\\n& \\leq & \\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}}\\int_{0}^{n}x\\mathrm{d}F&lt;\\infty,\n\\end{eqnarray}\\]\ndonde \\(F\\) es la función de distribución de \\(X_{i}\\). Luego \\[\nE\\left[X_{1}\\right]=\\lim_{n\\to\\infty}\\int_{0}^{n}x\\mathrm{d}F=\\lim_{n\\to\\infty}E\\left[Y_{n}\\right]=\\lim_{n\\to\\infty}\\dfrac{E\\left[Q_{n}\\right]}{n}.\n\\]\nEntonces, por el Lema de Borel Canteli. \\(\\mathcal{\\mathcal{P}}\\left[\\limsup\\left(\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right)\\right]=0\\)\n\\[\n\\lim_{n\\to\\infty}\\dfrac{Q_{n}}{n}=E\\left[X_{1}\\right],\\text{c.s}\n\\]\nAhora, calcularemos la siguiente probabilidad \\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}\\neq Y_{i}\\right]=\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\n\\]\ncomo \\(E\\left[X_{i}\\right]&lt;\\infty\\) y \\(X_{i}\\) son v.a.i.i.d.\n\\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\\leq E\\left[X_{1}\\right]&lt;\\infty\n\\]\nDe nuevo, por el Lema de Borel Cantelli. \\[\n\\mathcal{P}\\left[\\limsup\\left[X_{i}\\neq Y_{i}\\right]\\right]=0,\\forall i\n\\]\nEntonces\n\\[\\begin{eqnarray}\nX_{i} & = & Y_{i},\\text{c.s}\\\\\n& \\Rightarrow & \\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\to E\\left[X_{1}\\right]=\\mu.\\text{ c.s}\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "Tarea4.html",
    "href": "Tarea4.html",
    "title": "6  Tarea 4",
    "section": "",
    "text": "Exercise 6.1 Sea \\(W(t)\\) un movimiento Browniano estándar en \\([0,T]\\). Pruebe que para cualquier \\(c&gt;0\\) fijo, \\[\nV(t) = \\dfrac{1}{c} W(c^2 t)\n\\]\nes un movimiento Browniano sobre \\([0,T]\\).\n\n\nProof. \nVeamos que \\(V\\) cumple las propiedades del movimiento Browniano. \nPropiedad C1 (Que comience en 0).\nSe tiene que, \\(V(0) = \\dfrac{1}{c} W (c^2\\cdot0)=0\\).\n\nPropiedad C2 (Incrementos Independientes).\nSean \\(s&lt;t&lt;u&lt;v\\), por definición de \\(V\\), se tiene que, \\[\nE[\\left(V(t)-V(s)\\right)\\left(V(v)-V(u)\\right)]=\\dfrac{1}{c^2}E[\\left(W(c^2 t)-W(c^2 s)\\right)\\left(W(c^2 v)-W(c^2 u)\\right)]\n\\]\nDado que \\(W\\) tiene incrementos independientes, se cumple que,\n\\[\\begin{eqnarray}\n\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right] & = &\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\right]E\\left[\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right]\n\\end{eqnarray}\\]\nEntonces \\(V\\) tiene incrementos independientes.\n\nPropiedad C3 (Incrementos estacionarios).\nSea \\(s&lt;t\\). \\[\nV(t)-V(s)=\\dfrac{1}{c}\\left[W(c^2 t) - W(c^2 s)\\right]\n\\]\nPor las propiedades de la definicion del movimiento Browniano.\n\\[\\begin{eqnarray}\nE\\left[V(t)-V(s)\\right] & = & \\dfrac{1}{c}E\\left[W(c^{2}t)-W(c^{2}s)\\right]=0\\\\\n\\text{Var}\\left[V(t)-V(s)\\right] & = & \\dfrac{1}{c^{2}}\\text{Var}\\left[W(c^{2}t)-W(c^{2}s)\\right]=\\dfrac{1}{c^{2}}\\left(c^{2}\\left(t-s\\right)\\right)=t-s\n\\end{eqnarray}\\]\nEntonces \\(V\\) tiene incrementos estacionarios.\nCon todo lo anterior se concluye que, \\(V\\) es un movimiento browniano.\n\n\nExercise 6.2 Hacer un script para ilustrar la propiedad de escalado del movimiento Browniano para el caso de \\(c = \\dfrac{1}{5}\\). Estar seguro que usa el mismo camino browniano discretizado en cada subplot.\n\n\n\nBrowniano escalado, con c=1/5.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\nT = 1  \nn= 100  \ndt = 1 / (n - 1)\ndw = np.sqrt(dt) * prng.standard_normal(n - 1) \nw = np.concatenate(([0],dw.cumsum()))\n\ntime = np.linspace(0,T, n)\nc = 0.2  # 1/5\nc_time = c**2 * time  \nc_w = c**(-1) * w  \n\nfig, browniano_escalado = plt.subplots(2)\nbrowniano_escalado[0].plot(time, w)\nbrowniano_escalado[1].plot(c_time, c_w)\nbrowniano_escalado[0].set_title('Movimiento browniano')\nbrowniano_escalado[1].set_title('Moviemiento browniano escalado')\nplt.show()\n\n\n\n\nFigura 1\n\n\n\nExercise 6.3 Modifique el script half_brownian_refinement.py encapsulando el código en una función. Esta función deberá recibir el extremo derecho del intervalo \\([0, T]\\) y el número de incrementos \\(N\\) de un camino browniano base. El propósito es calcular los incrementos de relleno de una refinamiento con \\(2N\\) incrementos.\n\n\n\nBrowniano refinado, con refinamiento 2N.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\n\ndef refined_brownian_2n(T,L):\n    dt = T / L\n    W = np.zeros(L + 1)\n    W_refined = np.zeros(2 * L + 1)\n    xi = np.sqrt(dt) * prng.normal(size=L)\n    xi_half = np.sqrt(0.5 * dt) * prng.normal(size=L)\n    W[1:] = xi.cumsum()\n    W_ = np.roll(W, -1)\n    W_half = 0.5 * (W + W_)\n    W_half = np.delete(W_half, -1) + xi_half\n    W_refined[1::2] = W_half\n    W_refined[2::2] = W[1:]\n    t = np.arange(0, T + dt, dt)\n    t_half = np.arange(0, T + 0.5 * dt, 0.5 * dt)\n    return t,t_half,W, W_refined\n\n\nExercise 6.4 En un script separado, incluya la función de arriba y grafique una figura con la trayectoria del browniano con 100 incrementos y muestre su refinamiento correspondiente.\n\n\n\nBrowniano refinado, con refinamiento 2N y 100 incrementos.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h_b_r as hbr\n\na, b, c, d = hbr.refined_brownian_2n(1, 100)\n\nplt.plot(a, c, 'r-+')\nplt.plot(\n    b,\n    d,\n    'g*--',\n    # alpha = transparecia\n    )\nplt.show()\n\n\n\n\nFigura 2"
  },
  {
    "objectID": "Tarea5.html",
    "href": "Tarea5.html",
    "title": "7  Tarea 5",
    "section": "",
    "text": "Exercise 7.1 Demuestre que el movimiento browniano satisface \\[\nE[|W(t)-W(s)|^{2}]=|t-s|.\n\\]\n\n\nProof. Consideremos dos casos:\nSi \\(t&gt;s\\).\n\\[\\begin{align*}\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right] & =E\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right]\\\\\n& =t-s,\n\\end{align*}\\]\nya que, \\(W(t)-W(s)\\thicksim N(0,t-s)\\).\nMientras que si \\(t\\leq s\\).\n\\[\\begin{align*}\nE\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right] & =E\\left[\\left(W\\left(s\\right)-W\\left(t\\right)\\right)^{2}\\right]\\\\\n& =s-t,\n\\end{align*}\\]\npor lo tanto \\[\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right]=\\left|t-s\\right|\n\\]\n\n\nExercise 7.2 Dados \\(W(t_{i})\\) y \\(W(t_{i+1})\\), demuestre que la variable aleatoria \\[\nW(t_{i+\\frac{1}{2}}):=\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+\\frac{1}{2}\\sqrt{\\delta t\\xi},\\quad\\xi\\thicksim N(0,1)\n\\] satisface las tres condiciones C1, C2, C3 de la definicion de movimiento Browniano.\n\n\nProof. (C1) Veamos que \\(W\\left(0\\right)=0\\), cuando \\(t=0\\).\nSe tiene por definicion del proceso que, \\[\nW\\left(0\\right)=\\frac{1}{2}(W(0)+W(0))+\\frac{1}{2}\\sqrt{\\delta(0)\\xi}=0.\n\\] Por la propiedad C1 se satisface.\n\n(C2) Que tenga incrementos estacionarios.\nNotemos que\n\\[\\begin{align*}\nW(t_{i+\\frac{1}{2}})-W(t_{i}) & = \\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)+W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi-\\frac{1}{2}(W(t_{i})+W(t_{i}))\\\\\n& = \\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi,\n\\end{align*}\\]\nEntonces\n\\[\\begin{eqnarray*}\nE\\left[W(t_{i+\\frac{1}{2}})-W(t_{i})\\right] & = & E\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]\\right]+E\\left[\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & \\dfrac{1}{2}E\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & 0\\quad\\text{ya que, }E\\left[\\xi\\right]=0\\text{ y }E\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]=0.\n\\end{eqnarray*}\\]\ny\n\\[\\begin{eqnarray}\nVar\\left[W(t_{i+\\frac{1}{2}})-W(t_{i})\\right]& = & Var\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & Var\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]\\right]+Var\\left[\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & \\dfrac{1}{4}Var\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{4}\\delta t Var\\left[\\xi\\right]\\\\\n& = & \\dfrac{1}{4}\\delta t+\\dfrac{1}{4}\\delta t\\quad\\text{ya que, }Var\\left[\\xi\\right]=1\\text{ y }Var\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]=\\delta t\\\\\n& = & \\dfrac{1}{2}\\delta t.\n\\end{eqnarray}\\]\nAdemás, sabemos que la combinación lineal de normales es una nornal.\nPor lo tanto \\(W(t_{i+\\frac{i}{2}})-W(t_{i})\\sim N\\left(0,\\dfrac{\\delta t}{2}\\right)\\), con esto C2 se cumple.\n\n(C3) Que tenga incrementos independientes.\nPara esta parte usaremos que dos variables aleatorias \\(X\\) y \\(Y\\) son independientes si y solo si \\[\nE(XY)=E(X)E(Y)\n\\]\ncalculemos \\(E\\left[\\left(W\\left(t_{i+1}\\right)-W\\left(t_{i+\\frac{1}{2}}\\right)\\right)\\left(W(t_{j+1})-W\\left(t_{j+\\frac{1}{2}}\\right)\\right)\\right]\\) y definamos a \\(\\Delta W(t_{i}):=W(t_{i+1})-W(t_{i})\\).\nPor lo anterior se tiene que:\n\\[\\begin{eqnarray*}\n    E\\left[\\left(\\Delta W\\left(t_{i+\\frac{1}{2}}\\right)\\right)\\left(\\Delta W\\left(t_{j+\\frac{1}{2}}\\right)\\right)\\right]& = & E\\left[\\left(\\dfrac{1}{2}\\Delta W\\left(t_{i}\\right)+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)\\left(\\dfrac{1}{2}\\Delta W\\left(t_{j}\\right)+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)\\right],\n\\end{eqnarray*}\\]\ndonde \\(\\Delta W\\left(t_{i+\\frac{1}{2}}\\right)=W(t_{i+1})-W\\left(t_{i+\\frac{1}{2}}\\right)\\) y \\(\\Delta W\\left(t_{j+\\frac{1}{2}}\\right)=W(t_{j+1})-W\\left(t_{j+\\frac{1}{2}}\\right)\\).\\ Desarrollando la parte derecha de la igualdad anterior, resulta\n\\[\\begin{eqnarray*}\nE\\left[\\left(\\Delta W(t_{i+\\frac{1}{2}})\\right)\\left(\\Delta W(t_{j+\\frac{1}{2}})\\right)\\right]& = & E\\left[\\dfrac{1}{4}\\Delta W(t_{i})\\Delta W(t_{j})+\\dfrac{1}{4}\\Delta W(t_{i})\\sqrt{\\delta t}\\xi\\right.\\\\\n& & \\left.+\\dfrac{1}{4}\\Delta W(t_{j})\\sqrt{\\delta t}\\xi+\\left(\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)^{2}\\right]\\\\\n\\text{ya que, }\\Delta W(t_{i}),\\Delta W(t_{j})\\text{ son independientes} & = & \\dfrac{1}{4}E\\left[\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j})\\right]+\\dfrac{1}{4}E\\left[\\Delta W(t_{i})\\right]\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{1}{4}E\\left[\\Delta W(t_{j})\\right]\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n  & = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\dfrac{1}{2}\\Delta W(t_{j})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]+E\\left[\\dfrac{1}{2}\\Delta W(t_{j})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n  & = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]+E\\left[\\dfrac{1}{2}\\Delta W(t_{j})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right] +E\\left[\\dfrac{1}{2}\\Delta W(t_{j})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]+E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\\\\n& = & E\\left[\\Delta W(t_{i+\\frac{1}{2}})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right].\n\\end{eqnarray*}\\]\nPor lo tanto \\(E\\left[\\left(\\Delta W(t_{i+\\frac{1}{2}})\\right)\\left(\\Delta W(t_{j+\\frac{1}{2}})\\right)\\right]= E\\left[\\Delta W(t_{i+\\frac{1}{2}})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\), con lo que se concluye que se satisface la propiedad C3. Con todo lo anterior se concluye que \\(W(t_{i+\\frac{1}{2}})\\) define un Movimiento Browniano.\n\n\nExercise 7.3 Generalice la formula en el {Exercise 10.2} para el caso, dado \\(W(t_{i}), W(t_{i+1})\\), y \\(\\alpha\\in(0,1)\\) el valor \\[\nW(t_{i}+\\alpha dt)\n\\] satisface las tres condiciones que define un movimiento Browniano.\n\n\nProof. Observemos que \\[\nt_{i+\\alpha}=\\alpha t_{i+1}+(1-\\alpha)t_{i},\n\\] y \\[\nW(t_{i+\\alpha})-W(t_{i}) \\sim \\alpha\\sqrt{ dt}N(0,1)\n\\] Definamos a \\[\nW(t_{i+\\alpha})=W\\left(t_{i}+\\alpha\\Delta t\\right) :=\\left(1-\\alpha\\right)W(t_{i})+\\alpha W(t_{i+1})+Y.\n\\] donde \\(Y\\) será una v.a independiente de \\(W\\left(t\\right)\\).\nDado que,\n\\[\\begin{align*}\nW(t_{i+\\alpha})-W(t_{i}) & =\\left(1-\\alpha\\right)W(t_{i})+\\alpha W(t_{i+1})+Y-W_{i}\\\\\n& =\\alpha\\left(W_{i+1}-W(t_{i})\\right)+Y.\n\\end{align*}\\]\nEntonces, \\[\nE\\left[W(t_{i+\\alpha})-W(t_{i})\\right]-E[\\alpha\\left(W(t_{i+1})-W(t_{i})\\right)]=E\\left[Y\\right]\\Longrightarrow E[Y]=0,\n\\] y \\[\nVar\\left[W(t_{i+\\alpha})-W(t_{i})\\right]=\\alpha^{2}d t+Var\\left[Y\\right],\n\\] Así, \\[\nVar\\left[Y\\right]=d t\\left(\\alpha-\\alpha^{2}\\right),\n\\] entonces \\(Y=\\sqrt{\\alpha\\left(1-\\alpha\\right)dt}\\xi,\\xi\\sim N\\left(0,1\\right)\\).\nCon esto se cumple \\(C1\\). \\[\nW\\left(0\\right)=0.\n\\] y por construcción análogamente que el ejercicio anterior se satisfacen las propiedades C2 y C3.\n\n\nExercise 7.4 Suponga que \\(X\\thicksim N(0,1)\\), sabemos que \\(E[X]=0\\) y \\(E(X^{2})=1\\).\nAdemás de la definción, el pésimo-momento satisface \\[\nE[X^{p}]=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}x^{p}\\exp(-x^{2}/2)dx.\n\\] Usando esta relación, demuestre que \\(E[X^{3}]=0\\) y \\(E[X^{4}]=3\\). Entonces deduce que un incremento Browniano \\(\\delta W_{i}:=W(t_{i+1})-W(t_{i})\\) satisface que \\(E[\\delta Wt_{i}^{3}]=0\\) y \\(E[\\delta Wt_{i}^{4}]=3\\delta t^{2}\\). Entonces encuentre una expresion para \\(E[X^{p}]\\) para un entero positivo \\(p\\)\n\n\nProof. De la definición del \\(p\\)-esimo momento se tiene para \\(p=4\\), que \\[\nE\\left[X^{4}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{4}\\exp\\left(-\\frac{x^{2}}{2}\\right)d x.\n\\] resolviendo esta integral por el método integración por partes, se tiene que \\(E\\left[X^{4}\\right]=uv-\\int vdu\\) , donde \\(u=x^{3}\\Longrightarrow du=3x^{2}dx\\) y \\(dv=x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\), calculemos primero \\(v\\),\n\\[\\begin{eqnarray*}\nv & = & \\int x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n& = & \\dfrac{1}{\\sqrt{2\\pi}}\\int x\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\quad\\text{ sea } y=-\\dfrac{x^{2}}{2}\\Longrightarrow dy=-xdx\\\\\n& = & -\\dfrac{1}{\\sqrt{2\\pi}}\\int \\exp{\\left(y\\right)}dy\\\\\n& = & -\\dfrac{1}{\\sqrt{2\\pi}} \\exp{\\left(y\\right)}\\\\\n& = &  -\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}.\n\\end{eqnarray*}\\]\nSustituyendo todo lo anterior se tiene que,\n\\[\\begin{eqnarray*}\n    E\\left[X^{4}\\right] & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}3x^{2}dx\\\\\n     & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+3\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} x^{2}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+3E[X^{2}],\n\\end{eqnarray*}\\]\npor otro lado, \\[\n\\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}=\\lim_{x\\to\\infty}-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}+\\lim_{x\\to-\\infty}x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}=0.\n\\] Por lo tanto, dado que \\(E[X^{2}]=1\\), se concluye que \\[\nE\\left[X^{4}\\right]=3.\n\\] Procediendo de igual manera que el caso anterior, se tiene que: \\[\nE\\left[X^{3}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{3}\\exp\\left(-\\frac{x^{2}}{2}\\right)d x.\n\\] tomando \\(u=x^{2}\\Longrightarrow du=2xdx\\) y \\(dv\\), \\(v\\) igual al caso anterior, se tiene que\n\\[\\begin{eqnarray*}\n    E\\left[X^{3}\\right] & = & \\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -2x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & \\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+2E[X]\\\\\n     & = & 0,\n\\end{eqnarray*}\\]\nusando el hecho que \\(E[X]=0\\) y \\[\n\\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}=\\lim_{x\\to\\infty}-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}+\\lim_{x\\to-\\infty}x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}=0.\n\\] Por lo tanto, \\[\nE\\left[X^{3}\\right]=0.\n\\] De manera general se tiene que,\n\\[\\begin{eqnarray*}\n    E\\left[X^{p}\\right] & = & \\left.-x^{p-1}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -(p-1)x^{p-2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & 0+(p-1)E[X^{p-2}]\\\\\n     & = & (p-1)E[X^{p-2}].\n\\end{eqnarray*}\\]\nPor otro lado, observemos que \\(\\delta W_{i}\\sim N\\left(0,\\delta t\\right)\\), donde \\(\\delta t=t_{i+1}-t_{i}\\), entonces \\[\nZ=\\dfrac{\\delta W_{i}}{\\sqrt{\\delta t}}\\sim N\\left(0,1\\right),\n\\]\nque por lo visto anteriormente, para \\(p=4\\). \\[\nE\\left[Z^{4}\\right]=3\\Longrightarrow E[(\\delta W_{i})^{4}]=E[Z^{4}](\\delta t )^{2}=3(\\delta t )^{2}\n\\]\ny para \\(p=3\\), resulta \\[\nE[Z^{3}]=0\\Longrightarrow E[(\\delta W_{i})^{3}]=E[Z^{3}](\\delta t )^{3/2}=0.\n\\]\n\n\nExercise 7.5 Suponga que \\(X\\thicksim N(0,1)\\). Demuestre que para \\(a, b\\in\\mathbb{R}\\), \\[\nE[\\exp(a+bX)]=\\exp\\left(a+\\frac{1}{2}b^{2}\\right).\n\\] Por lo tanto deduzca que \\[\nE[\\exp(t+\\frac{1}{4}W_{t})]=\\exp\\left(\\frac{32}{33}t\\right).\n\\]\n\n\nProof. Se tiene que \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=\\exp{(a)}E\\left[\\exp\\left(bX\\right)\\right],\n\\]\nobservemos que \\(bX\\sim N\\left(0,b^{2}\\right)\\) además, \\(E\\left[\\exp\\left(bX\\right)\\right]\\) es la función generadora de momentos cuando \\(t=1\\) \\[\nM_{bX}\\left(1\\right)=E\\left[\\exp\\left(bX\\right)\\right]=\\exp\\left(\\dfrac{b^{2}}{2}\\right),\n\\] sustituyendo, resulta \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=\\exp{(a)}\\exp\\left(\\dfrac{b^{2}}{2}\\right)=\\exp\\left(a+\\dfrac{1}{2}b^{2}\\right).\n\\] Ahora calculemos \\(E\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]\\), se tiene que, \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=E\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right],\n\\] entonces consideremos a \\(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\), observemos que, \\(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\sim N\\left(0,1\\right),\\) por lo tanto, podemos usar la fórmula anterior con \\(a=t\\) y \\(b=\\dfrac{1}{4}\\sqrt{t}\\),\n\\[\\begin{align*}\nE\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right] & = E\\left[\\exp\\left(t+\\dfrac{1}{4}\\sqrt{t}\\left(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\right)\\right)\\right]\\\\\n& =\\exp\\left(t+\\dfrac{1}{2}\\left(\\dfrac{1}{16}t\\right)\\right)\\\\\n& =\\exp\\left(t+\\dfrac{1}{32}t\\right)\\\\\n& =\\exp\\left(\\dfrac{33}{32}t\\right).\n\\end{align*}\\]\nPor lo tanto se concluye que, \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=\\exp\\left(\\dfrac{33}{32}t\\right).\n\\]"
  },
  {
    "objectID": "Tarea6.html",
    "href": "Tarea6.html",
    "title": "Matematicas aplicadas",
    "section": "",
    "text": "Exercise 1 Cree un scrip para muestrear 10000 rutas del proceso \\(u(t,W_{t})\\) definido en el ejercicio Exercise 7.5. Graficar 10 rutas de muestra y la media de 10000 rutas de muestra de este proceso \\(u(t,W_{t})\\).\n\n\nSolution. \n\n\nsimulación ejercicio 7-5.py\n\nimport numpy as np\nimport aux_functions as aux\nimport matplotlib.pyplot as plt\n\ndef b_function(t, a, w): \n    y = np.exp(t+ a * w)\n    return y\n\n\nn_samples = 10000\nn = 100\nt_initial = 0\nt_final = 1\n\nmean = np.zeros(n) \nfor i in range(n_samples):\n    time, b_w = aux.strong_brownian(t_final,n) \n    y = b_function(time,0.25, b_w) \n    if i &lt; 10:\n        plt.plot(time,b_w,'g-',alpha = 0.5) \n    mean += y \n\nmean = (n_samples)**(-1) * mean \ntime = np.linspace(0,t_final, n_)\n\ny = [np.exp(33 / 32 * t) for t in time] \nplt.plot(time, mean, 'r-*', alpha = 0.5)\nplt.plot(time, y,'b-',alpha = 0.8)\nplt.show()\n\n\n\n\nFigura 1\n\n\n\n\nExercise 2 Siguiendo las ideas para llenar un camino browniano en puntos \\(t_{i+\\frac{1}{2}}:=t_{i}+\\frac{1}{2}\\delta t\\). Haga una función de Python para llenar un camino browniano dada una fracción \\(\\alpha\\in(0,1)\\) para llenar en los puntos \\(t_{i+\\alpha}:=t_{i}+\\alpha\\delta t\\)\n\n\nSolution. \n\n\nsimulación para llenar un camino browniano con alpha=0,3.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport aux_functions as aux\n\n\nt_final = 1\nn = 100\ndelta_t = 1/(n - 1)\nalpha = 0.3\n\nprng = np.random.RandomState(123456789)\n\ntime, w = aux.strong_brownian(1,n) \n\ny = np.sqrt(delta_t *(alpha - alpha ** 2)) * prng.standard_normal(n - 1)  \n\nw_ = np.roll(w, -1)  \n\nw_alpha = alpha * w_ + (1 - alpha) * w \nw_alpha = np.delete(w_alpha, -1) \nw_alpha += y \nw_ref = np.zeros(2* n -1) \n\nw_ref[0::2] = w \nw_ref[1::2] = w_alpha\n\ntime_ref = np.zeros(2 * n - 1) \n\nfor i in range(2 * n - 1):\n    if i % 2 == 0:\n        time_ref[i] = time[int(i / 2)] \n    else:\n        time_ref[i] = time[int(i / 2)] + alpha * delta_t \n\nplt.plot(time_ref,w_ref,'g-')\nplt.plot(time, w,'ro')\nplt.show()\n\n\n\n\nFigura 2"
  },
  {
    "objectID": "Tarea7.html",
    "href": "Tarea7.html",
    "title": "9  Tarea 7",
    "section": "",
    "text": "Exercise 9.1 Sea \\(W(t)\\) un Movimiento Browniano y \\(Z_{i}\\) una colección de variables aleatorias i.i.d, con distribución \\(N\\left(0,\\frac{\\delta t}{4}\\right)\\).\nPruebe que la suma \\[\n\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right),\n\\] tiene valor esperado igual a cero y una varianza de \\(O(\\delta t)\\).\n\n\nProof. Sin perdida de generalidad dado como estan definidas \\(Z_{i}\\) y \\(W(t_{i+1})-W(t_{i})\\) podemos suponer que son variables aleatorias independientes para cada \\(i=1,\\dots ,L\\). Entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\mathbb{E}\\left[Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\\\\n& = &  \\sum_{i=0}^{L}\\mathbb{E}(Z_{i})\\mathbb{E}\\left(W(t_{i+1})-W(t_{i})\\right)\\\\\n& = & 0 \\quad\\text{ ya que, por hipótesis, }\\mathbb{E}(Z_{i})=0\\text{ y }\\mathbb{E}\\left(W(t_{i+1})-W(t_{i})\\right)=0\n\\end{eqnarray*}\\] \\[\n\\therefore \\mathbb{E}\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]=0.\n\\] Ahora calculemos la varianza; sabemos que \\(Var(X)=\\mathbb{E}(X^{2})-(\\mathbb{E}(X))^{2}\\), sustituyendo resulta\n\\[\\begin{eqnarray*}\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &   \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]-\\left(\\mathbb{E}\\left[\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right]\\right)^{2}\\\\\n& = &  \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]\\quad\\text{ usando el hecho que tiene valor esperado igual a 0}.\n\\end{eqnarray*}\\] Por el Teorema multinomial, se tiene que\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] & = &  \\mathbb{E}\\left[\\sum_{i=0}^{L} \\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \\sum_{i=0}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right].\n\\end{eqnarray*}\\] Dado que \\(i\\neq j\\), sin perdida de generalidad podemos suponer que \\(i&lt;j\\), entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right] & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\n\\}\\\\\n& = & \\mathbb{E}\\{[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j} ]\\mathbb{E}\\left[(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\\}\\\\\n& = & 0\n\\end{eqnarray*}\\] Por otro lado,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2} & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{i}\\right]\n\\}\\\\\n& = & \\mathbb{E}\\{Z_{i}^{2}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{i}\\right]\\}\\\\\n& = & \\mathbb{E}[Z_{i}^{2}](t_{i+1}-t_{i})\\\\\n& = & \\dfrac{\\delta t}{4}(t_{i+1}-t_{i}),\n\\end{eqnarray*}\\] sustituyendo todo lo anterior, resulta \\[\\begin{eqnarray*}\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\dfrac{\\delta t}{4}(t_{i+1}-t_{i})\\\\\n& = &  \\dfrac{\\delta t}{4}(t_{L+1}-t_{0}).\n\\end{eqnarray*}\\] Para un \\(L\\) suficientemente grande podemos considerar que, dado un \\(\\varepsilon&gt;0\\), tal que, \\((t_{L+1}-t_{0})\\leq\\dfrac{\\varepsilon}{4}\\), así \\[\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\leq \\varepsilon\\delta t.\n\\] Por lo tanto, con esto se concluye que, la varianza es de orden \\(\\delta t\\).\n\n\nExercise 9.2 La regla del punto medio de la integral de Riemann de una función \\(h\\in C^{2}([a,b])\\) sobre una partición de \\(L\\) puntos del intervalo \\([a,b]\\) está dada por, \\[\n\\int_{a}^{b}h(t)dt=\\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}h\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)\\delta t.\n\\] Use la relación \\[\nW\\left(\\frac{t_{i}+t_{i+1}}{2}\\right)=\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ \\underbrace{Z_{i}}_{i.i.d.\\sim N(0,\\delta t/4)},\n\\] y el ejercicio anterior para demostrar que la regla del punto medio de la integral de Riemann implica que \\[\n\\int_{0}^{T}W(t)dW(t)=\\frac{1}{2}W(T)^{2}.\n\\]\n\n\nProof. Sea \\(\\Delta_{L}=\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la regla del punto medio, aplicada para \\(h(t)=W(t)\\), se tiene que\n\\[\\begin{eqnarray*}\n\\int_{0}^{T}W(t)dW(t) & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}W\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\left[\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ Z_{i}\\right](W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\frac{1}{2}\\left(W(t_{i+1})^{2}-W(t_{i})^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\frac{1}{2}\\left(W(T)^{2}-W(0)^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\frac{1}{2}W(T)^{2}+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i})).\n\\end{eqnarray*}\\] De la igualdad anterior solo nos faltaria demostrar que, \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2}\n\\] es decir, \\[\n\\lim_{\\|\\Delta_{L}\\|\\to0}E\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]=0\n\\] Del ejercicio anterior, sabemos que \\[\nE\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] = O(\\delta t)\\leq \\varepsilon\\|\\Delta_{L}\\|,\n\\] así, tomando el limite cuando \\(\\|\\Delta_{L}\\|\\to\\) se tiene que, \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2},\n\\] por lo tanto, sustituyendo este último resultado, se concluye que \\[\n\\int_{0}^{T}W(t)dW(t)=\\frac{1}{2}W(T)^{2}\n\\]\n\n\nExercise 9.3 Usando la aproximación de la suma de Riemann \\[\n\\int_{0}^{T}h(t)dW(t)\\sim\\sum_{i=0}^{L}h(t_{i})(W(t_{i+1})-W(t_{i})),\n\\tag{9.1}\\] argumente que, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T^{3}}{3}.\n\\] Por tanto, enuncie la isometría de Itô y deduzca que esta isometría es válida para el caso \\(h(t)=t\\).\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la aproximación de la suma de Riemann, tenemos que \\[\\begin{eqnarray*}\n\\int_{0}^{T}tdW(t) & \\sim & \\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\\\\n\\Longrightarrow \\left(\\int_{0}^{T}tdW(t)\\right)^{2} & \\sim & \\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2},\n\\end{eqnarray*}\\] del Teorema Multinomial, se sigue que \\[\n\\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2} = \\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\n\\] de las relaciones anteriores se tiene que, \\[\\begin{eqnarray*}\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & \\sim &  \\mathbb{E}\n\\left[\\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}\\mathbb{E}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i}),\n\\end{eqnarray*}\\] además, observemos que, \\[\n\\lim_{L\\to 0}\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i})=\\int_{0}^{T}t^{2}dt=\\frac{T}{3}\n\\] entonces, de esto último se concluye que, \\[\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T}{3}.\n\\] Por otro lado, de la isometría de Itô, se cumple que \\[\\begin{eqnarray*}\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & = & \\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)\\left(\\int_{0}^{T}tdW(t)\\right)\\right]\\\\\n& = &\n\\int_{0}^{T}\\mathbb{E}(t^{2})dt\\\\\n& = &\n\\int_{0}^{T}t^{2}dt\\\\\n& = & \\frac{T}{3}.\n\\end{eqnarray*}\\] Por lo tanto, la isometría de Itô se cumple para \\(h(t)=t\\)\n\n\nExercise 9.4 Escriba una función de Python para calcular la integral de Itô del movimiento Browniano \\(W(t)\\) sobre \\([0,T]\\).\n\n\nSolution. \n\n\ncalculando la integral de Itô del movimiento Browniano W(t).py\n\nimport numpy as np\n\ndef strong_brownian(t, n): \n    dt = t / n\n    dw = np.zeros(n)\n    w = np.zeros(n)\n    for i in np.arange(1, n):\n        dw[i] = np.sqrt(dt)*np.random.standard_normal()\n        w[i] = w[i - 1] + dw[i]\n    time = np.linspace(0, t, n)\n    return time, w\n\n\ndef f(x, t):\n    y = x\n    return y\n     \n\ndef fB(partition, x, t):\n    y = 0\n    for i in range(len(partition) - 1):\n        if partition[i] &lt;= t &lt; partition[i + 1]:\n            y = f(x, t)\n    return y\n\ndef ito_n(n, t):\n    time, w = strong_brownian(t, n)\n    integral = np.zeros(n)\n    for i in range(n - 1):\n        integral[i] = fB(time, w[i], time[i]) * (w[i + 1] - w[i])\n    ito = integral.sum()\n    return w, ito"
  },
  {
    "objectID": "Tarea8.html",
    "href": "Tarea8.html",
    "title": "10  Tarea 8",
    "section": "",
    "text": "Exercise 10.1 Use la aproximación de la suma de Riemann la ecuación (Equation 9.1). Muestra la propiedad de linealidad de la integral estocástica. Es decir, \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}=\\alpha\\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\), de la aproximación de la suma de Riemann ecuación (Equation 9.1), se satisface que, \\[\\begin{eqnarray*}\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t} & \\sim & \\sum_{i=0}^{L}(\\alpha f(t_{i})+\\beta g(t_{i}))(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\sum_{i=0}^{L}\\alpha f(t_{i})(W(t_{i+1})-W(t_{i}))+ \\sum_{i=0}^{L}\\beta g(t_{i})(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\alpha\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))+\\beta\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))\n\\end{eqnarray*}\\] observemos que, \\[\n\\alpha\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))\\text{ es la aproximacion de la suma de Riemann de; } \\alpha\\int_{0}^{T} f(t) dW_{t},\n\\] análogamente se tiene para \\(\\beta\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))\\), por lo tanto de aquí se sigue que, tomando el limite cuando \\(L\\to\\infty\\), resulta \\[\n\\alpha\\lim_{L\\to\\infty}\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))=\\alpha \\int_{0}^{T}f(t)dW_{t}\n\\] y \\[\n\\beta\\lim_{L\\to\\infty}\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))=\\beta\\int_{0}^{T}g(t)dW_{t}\n\\] entonces de estas dos últimas relaciones, se concluye que: \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}= \\alpha \\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\n\nExercise 10.2 Escriba con detalle la demostración del siguiente Teorema, también incluya la demostración del Lema 5.18 del Mao.\n\n\nTheorem 10.1 (6.1 del Mao) Sea \\(f\\in\\mathcal{M}^{2}([0,T];\\mathbb{R})\\), sea \\(\\rho,\\tau\\) dos tiempos de paro tales que \\(0\\leq\\rho\\leq\\tau\\leq T\\). Entonces \\[\\begin{eqnarray}\n\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dW_{s}\\mid\\mathcal{F}_{\\rho}\\right) & = & 0,\\\\\n\\mathbb{E}\\left(\\left|\\int_{\\rho}^{\\tau}f(s)dW_{s}\\right|^{2}\\mid\\mathcal{F}_{\\rho}\\right) & = & \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}\\left|f(s)\\right|^{2}ds\\mid\\mathcal{F}_{\\rho}\\right).\n\\end{eqnarray}\\]\n\nAntes de la demostración del teorema Theorem 10.1, veamos el siguiente Lema.\n\n\nLemma 10.1 (5.18 del Mao) Sea \\(f\\in\\mathcal{M}^{2}([0,T],\\mathbb{R})\\) y sea \\(\\tau\\) un tiempo de paro tal que \\(0\\leq \\tau\\leq T\\). Entonces \\[\n\\int_{0}^{\\tau}f(s)dW(s)=I(\\tau),\n\\] donde \\(\\{I(t)\\}_{0\\leq t \\leq t}\\) es la integral indefinida de \\(f\\) dada por la Definición 5.11.\n\n\nProof. La definición 5.11 del Mao, nos dice que, \\[\nI(t)=\\int_{0}^{t}f(s)dW(s),\\quad 0\\leq t\\leq T\n\\] Por otro lado, de la definición 5.15 del Mao, también se tiene que \\[\n\\int_{0}^{\\tau}f(s)dW(s)=\\int_{0}^{T}\\mathbb{I}_{[0,\\tau]}f(s)dW(s)\n\\] así, por las dos definiciones anteriores se cumple que, \\[\n\\int_{0}^{\\tau}f(s)dW(s)=I(\\tau)\n\\]\n\n\nProof (Del Teorema 6.1). El Teorema de paro de la martingala de Doob, nos dice que: \\[\\begin{equation}\nE(I(\\tau)|\\mathcal{F}_{\\rho})=I(\\rho)\n\\end{equation}\\] Además la definición 5.15, nos dice que para \\(\\rho\\) otro tiempo de paro, tal que \\(0\\leq \\rho\\leq \\tau\\), se cumple que \\[\n\\int_{\\rho}^{\\tau}f(s)dW(s)=\\int_{0}^{\\tau}f(s)dW(s)-\\int_{0}^{\\rho}f(s)dW(s).\n\\] Entonces, aplicando la igualdad anterior, el Lema Lemma 10.1 y el Teorema de paro de la martingala de Doob, resulta \\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dW_{s}\\Big|\\mathcal{F}_{\\rho}\\right) & = & \\mathbb{E}\\left(\\left(\\int_{0}^{\\tau}f(s)dW(s)-\\int_{0}^{\\rho}f(s)dW(s)\\right)\\Big|\\mathcal{F}_{\\rho}\\right)\\\\\n& = & \\mathbb{E}(I(\\tau)-I(\\rho)|\\mathcal{F}_\\rho)\\\\\n& = & \\mathbb{E}(I(\\tau)|\\mathcal{F}_\\rho)-\\mathbb{E}(I(\\rho)|\\mathcal{F}_\\rho)\\\\\n& = & I(\\rho)-I(\\rho)\\\\\n& = & 0\n\\end{eqnarray*}\\] De aquí, se concluye que la primera relación del teorema Theorem 10.1 se satisface.\nPor otro lado, nuevamente por el Teorema de paro de Doob, se tiene que \\[\\begin{equation}\nE(I^{2}(\\tau)-\\langle I,I\\rangle_{\\tau}|\\mathcal{F}_{\\rho})=I^{2}(\\rho)-\\langle I,I\\rangle_{\\rho},\n\\end{equation}\\] y además, del Teorema 5.14 del Mao, se tiene que \\[\n\\langle I,I\\rangle_{t}=\\int_{0}^{t}|f(s)|^{2}ds.\n\\] De estos dos hechos anteriores, resulta\n\\[\\begin{eqnarray*}\n\\mathbb{E}(|I(\\tau)-I(\\rho)|^{2}|\\mathcal{F}_{\\rho}) & = & \\mathbb{E}(I^{2}(\\tau)-2I(\\rho)I(\\tau)+I^{2}(\\rho)|\\mathcal{F}_{\\rho}) \\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-2I(\\rho)\\mathbb{E}(I(\\tau)|\\mathcal{F}_{\\rho})+I^{2}(\\rho) \\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-2I(\\rho)^{2}+I^{2}(\\rho)\\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-I^{2}(\\rho)\\\\\n& = & \\mathbb{E}(\\langle I,I\\rangle_{\\tau}-\\langle I, I\\rangle_{\\rho}|\\mathcal{F}_{\\rho})\\\\\n& = & \\mathbb{E}\\left(\\int_{0}^{\\tau}|f(s)|^{2}ds-\\int_{0}^{\\rho}|f(s)|^{2}ds|\\mathcal{F}_{\\rho}\\right)\\\\\n& = & \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}|f(s)|^{2}ds|\\mathcal{F}_{\\rho}\\right).\n\\end{eqnarray*}\\] Ya que, \\[\n\\mathbb{E}\\left(\\left|\\int_{\\rho}^{\\tau}f(s)dW_{s}\\right|^{2}\\mid\\mathcal{F}_{\\rho}\\right)=\\mathbb{E}(|I(\\tau)-I(\\rho)|^{2}|\\mathcal{F}_{\\rho}),\n\\] se sigue que la segunda relación del teorema Theorem 10.1 se satisface.\n\n\nExercise 10.3 Usando la aproximación de la suma de Riemann ecuación (Equation 9.1), la isometría de Itô y la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\) pruebe que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right]=\\int_{0}^{T}\\mathbb{E}[f(t)g(t)]dt.\n\\]\n\n\nProof. Sea \\(a=\\int_{0}^{T}g(t)dW_{t}\\) y \\(b=\\int_{0}^{T}f(t)dW_{t}\\), entonces de la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\), se tiene que \\[\\begin{eqnarray*}\n4\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right) & = & \\left(\\int_{0}^{T}g(t)dW_{t}+\\int_{0}^{T}f(t)dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}g(t)dW_{t}-\\int_{0}^{T}f(t)dW_{t}\\right)^{2}\\\\\n& = & \\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2},\n\\end{eqnarray*}\\] Entonces\n\\[\\begin{eqnarray*}\n4\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right] & = & \\mathbb{E}\\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\mathbb{E}\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2}\\\\\n& = &\\left(\\int_{0}^{T} \\mathbb{E}(g(t)+f(t))^{2}dt\\right)-\\left(\\int_{0}^{T}\\mathbb{E}(g(t)-f(t))^{2}dt\\right)\\quad\\text{esto se sigue por la isometria de Itô}\\\\\n& = & \\left(\\int_{0}^{T}(\\mathbb{E}[(g(t)+f(t))^{2}]-\\mathbb{E}[(g(t)-f(t))^{2}])dt\\right)\\\\\n& = & \\left(\\int_{0}^{T}\\mathbb{E}[(g(t)+f(t))^{2}-(g(t)-f(t))^{2}]dt\\right)\\quad\\text{usando nuevamente que, } 4ab=(a+b)^{2}-(a-b)^{2}\\\\\n& = & 4\\left(\\int_{0}^{T}\\mathbb{E}[g(t)f(t)]dt\\right)\n\\end{eqnarray*}\\] De aquí, se concluye que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right]=\\left(\\int_{0}^{T}\\mathbb{E}[g(t)f(t)]dt\\right)\n\\]\n\n\nExercise 10.4 Usando la suma de Riemann ecuación (Equation 9.1), deduzca que, \\[\n\\int_{0}^{T}W(t)^{2}dW(t)=\\dfrac{1}{3}W(T)^{3}-\\int_{0}^{T}W(t)dt.\n\\]\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). Primero, observemos que, \\[\n3W(t_{i})^{2}(W(t_{i+1})-W(t_{i}))=W(t_{i+1})^{3}-\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-3\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})-W(t_{i-1})^{3},\n\\] entonces de la ecuación (Equation 9.1) y la relación anterior, tenemos que,\n\\[\\begin{align*}\n\\int_{0}^{T}W(t)^{2}dW(t) & \\sim \\sum_{i=0}^{L}W(t_{i})^{2}(W(t_{i+1})-W(t_{i}))\\\\\n& = \\frac{1}{3} \\sum_{i=0}^{L}\\left[W(t_{i+1})^{3}-W(t_{i-1})^{3}\\right]-\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& =\\frac{1}{3}( W(T)^{3}-W(t_{0})^{3})- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& =\\frac{1}{3}W(T)^{3}- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i}).\n\\end{align*}\\] Afirmamos que \\(\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\to 0\\) en \\(L^{2}\\).\nEn efecto, calculemos la media de la variación cuadrática. Del Teorema Multinomial, resulta \\[\n\\frac{1}{9}\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right)^{2}\\right]=\\frac{1}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right]+\\frac{2}{9}\\sum_{i\\neq j}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right],\n\\] notemos que, como \\(i\\neq j\\), sin perdida de generalidad supongamos que \\(i&lt;j\\), entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right] & = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]\\right|\\mathcal{F}_{j}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\mathbb{E}\\left.\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right|\\mathcal{F}_{j}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right]\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]\n\\end{eqnarray*}\\]\ndado que de la tarea 5 se demostro que, \\(\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right]=0\\), de la última igualdad ses concluye que, \\[\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]=0.\n\\] Por lo tanto, \\[\n\\frac{2}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]=0.\n\\] Por otro lado, también de la tarea 5, sabemos que \\[\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] = 15\\left(t_{i+1}-t_{i}\\right)^{3},\n\\] entonces\n\\[\\begin{align*}\n\\frac{1}{9}\\sum_{i=0}^{L}E\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] & =\\frac{5}{3}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)^{3}\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}L\\to0,\\quad\\text{cuando,  }\\|\\Delta_{L}\\|\\to0.\n\\end{align*}\\] Con todo lo anterior se concluye que, \\[\n\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\to 0.\n\\] Ahora veamos que, \\[\n\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\to \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right)\\text{ en }L^{2}\n\\] Observemos que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})- \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right) \\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}W(t_{i})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right] \\right)^{2}\\right]\\\\\n& = & \\mathbb{E}\\left[\\sum_{i=0}^{L}W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2} +\\sum_{i\\neq j}W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] \\right]\\\\\n& = & \\sum_{i=0}^{L}\\mathbb{E}\\left[W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2}\\right] +\\sum_{i\\neq j}\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] \\right],\n\\end{eqnarray*}\\] para \\(i&lt;j\\), se tiene que\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\left(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right)\\right] & = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\left(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right)\\right]\\right|\\mathcal{F}_{j}\\right\\}\\\\\n& = &  \\mathbb{E}\\left\\{W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\mathbb{E}\\left[\\left.\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right|\\mathcal{F}_{j}\\right]\\right\\}\\\\\n& = &  \\mathbb{E}\\left\\{W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right]\\right\\}\\\\\n& = & 0.\n\\end{eqnarray*}\\] Por lo tanto, \\[\n\\sum_{i=0}^{L}W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] =0.\n\\] Además, \\[\\begin{eqnarray*}\n\\mathbb{E}\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)^{2}\\right] & = & \\mathbb{E}\\left\\{\\mathbb{E}\\left.\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)^{2}\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{4}-2\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)+\\left(t_{i+1}-t_{i}\\right)^{2}\\right)\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\mathbb{E}\\left.\\left[W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{4}-2W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)+W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{4}\\right|\\mathcal{F}_{i}\\right]-2\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)\\right|\\mathcal{F}_{i}\\right]+\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right|\\mathcal{F}_{i}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{W(t_{i})^{2}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{4}|\\mathcal{F}_{i}\\right]-2W(t_{i})^{2}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)|\\mathcal{F}_{i}\\right]+W(t_{i})^{2}\\mathbb{E}\\left[\\left(t_{i+1}-t_{i}\\right)^{2}|\\mathcal{F}_{i}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{W(t_{i})^{2}3(t_{i+1}-t_{i})^{2}-2W(t_{i})^{2}(t_{i+1}-t_{i})^{2}+W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right\\}\\\\\n& = & 3t_{i}(t_{i+1}-t_{i})^{2}-2t_{i}(t_{i+1}-t_{i})^{2}+t_{i}(t_{i+1}-t_{i})^{2}\\\\\n& = & 2t_{i}(t_{i+1}-t_{i})^{2},\n\\end{eqnarray*}\\] de esta última igualdad se sigue que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\sum_{i=0}^{L}W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2}\\right] & = & \\sum_{i=0}^{L}2t_{i}(t_{i+1}-t_{i})^{2}\\\\\n& \\leq & 2L\\|\\Delta_{L}\\|\\sum_{i=0}^{L}t_{i+1}-t_{i}\\\\\n& = & 2\\|\\Delta_{L}\\|L^{2}\\to0,\\quad\\text{ cuando }\\|\\Delta_{L}\\|\\to0.\n\\end{eqnarray*}\\] Por lo tanto, \\[\n\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\to \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right).\n\\] Así, sustituyendo todo lo anterior, resulta\n\\[\\begin{eqnarray*}\n\\int_{0}^{T}W(t)^{2}dW(t) & = & \\lim_{L\\to\\infty}\\frac{1}{3}W(T)^{3}-\\lim_{L\\to\\infty}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& = & \\frac{1}{3}W(T)^{3}-\\int_{0}^{T}W(t)dt\n\\end{eqnarray*}\\]\n\n\nExercise 10.5 Verifique que la isometría de Itô, dada por \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right],\n\\tag{10.1}\\] se tiene cuando \\(h(t):= 1\\).\n\n\nProof. Del ejercicio Exercise 11.1 con \\(h(t)=1\\), se tiene que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW(t)\\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW_{t}\\right)\\left(\\int_{0}^{T}1dW_{t}\\right)\\right]\\\\\n& = & \\int_{0}^{T}\\mathbb{E}[1]dt\\\\\n& = & \\int_{0}^{T}dt\\\\\n& = & T.\n\\end{eqnarray*}\\] Por otro lado, \\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right] & = & \\mathbb{E}\\left[\\int_{0}^{T}1^{2}dt\\right]\\\\\n& = & \\mathbb{E}\\left[\\int_{0}^{T}dt\\right]\\\\\n& = & \\mathbb{E}\\left[T\\right]\\\\\n& = & T\n\\end{eqnarray*}\\] Así, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}dW(t)\\right)^{2}\\right]= \\mathbb{E}\\left[\\int_{0}^{T}dt\\right],\n\\] y con esto se concluye que, para \\(h(t)=1\\) se satisface que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]= \\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right].\n\\]"
  },
  {
    "objectID": "Tarea9.html",
    "href": "Tarea9.html",
    "title": "11  Tarea 9",
    "section": "",
    "text": "El siguiente código calcula la aproximación de la Integral de Ito. Con \\(T= 1\\), \\(L=2^13\\) correspondiente al error de \\(\\mathcal{O}(10^{-3})\\)\nimport numpy as np\nT = 1.0\nL = 2**13\ndt = T / L\ndW = np.sqrt(dt) * np.random.normal(size=L)\nW = np.zeros(L + 1)\nW[1 :] = np.cumsum(dW)\nito_integral = np.sum(np.multiply(W[0: -1], dW))\nerr = np.abs(ito_integral - 0.5 * (W[-1] ** 2 - T))\nAdapta este código para la Integral de Stratonovich correspondente y evalue el error.\n\n\nSolution. \n\n\nAproximando la integral de Stratonovich.py\n\nimport numpy as np\n\ndef bw(t, n):\n    dt = t / (n - 1)\n    dw = np.sqrt(dt) * np.random.standard_normal(n - 1)\n    w = np.zeros(n)\n    w[1:] = dw.cumsum()\n    time = np.linspace(0, t, n)\n    return time, w\n\nt_f = 1\nn_p = 2** 13\nt, wt = bw(t_f, n_p)\ny = 0.5 * np.sqrt(t[1] - t[0]) * np.random.standard_normal(n_p)\nstratonovich = [(0.5 * (wt[i + 1] + wt[i]) + y[i])* (wt[i + 1] - wt[i]) for i in range(n_p - 1)]\nstratonovich = np.array(stratonovich).sum()\nprint(np.abs(stratonovich - 0.5 * wt[-1] ** 2))\n\n\n\nEscoja un integrando y computacionalmente verifique la Isometría de Ito de la Ecuación (Equation 10.1).\n\n\nSolution. \n\n\nsimulacion de la isometria de Ito con h(t)=t.py\n\nimport numpy as np\nimport aux_functions as aux\n\nn = 500\nn2 = 500\ntime=1\nintegral1 = time**3/3\n\ndef strong_brownian(t, n): \n    dt = t / n\n    dw = np.zeros(n)\n    w = np.zeros(n)\n    for i in np.arange(1, n):\n        dw[i] = np.sqrt(dt)*np.random.standard_normal()\n        w[i] = w[i - 1] + dw[i]\n    time = np.linspace(0, t, n)\n    return time, w\n\nE_If = 0\nfor j in range(n2):\n   I_fn = 0\n   t,w = strong_brownian(time, n)\n   for i in range(n-1):\n      I_fi= t[i]*(w[i+1]-w[i])\n      I_fn += I_fi\n   I_f = I_fn ** 2\n   E_If += I_f\n\nprint(n2 ** (-1) * E_If)\nprint(integral1)\n\n\n\nExercise 11.1 Sea \\(\\tau\\) un tiempo de paro. Prueba que \\(W\\left(t+\\tau\\right)-W\\left(\\tau\\right)\\) es un movimiento browniano.\n\n\nProof. Definamos a, \\[\nW_{\\tau}\\left(t\\right)=W\\left(t+\\tau\\right)-W\\left(\\tau\\right),\n\\]\nClaramente \\(W_{\\tau}\\left(0\\right)=0\\), ya que: \\[\nW_{\\tau}\\left(0\\right)=W\\left(\\tau\\right)-W\\left(\\tau\\right)=0.\n\\]\nSea \\(s\\leq t\\), observemos que\n\\[\\begin{align*}\nW_{\\tau}\\left(t\\right)-W_{\\tau}\\left(s\\right) & =W\\left(t+\\tau\\right)-W\\left(\\tau\\right)-\\left[W\\left(s+\\tau\\right)-W\\left(\\tau\\right)\\right]\\\\\n& =W\\left(t+\\tau\\right)-W\\left(s+\\tau\\right),\n\\end{align*}\\]\ndado que \\(W\\left(t+\\tau\\right)-W\\left(s+\\tau\\right)\\sim N\\left(0,t-s\\right)\\) entonces de la última igualdad se sigue que \\[\nW_{\\tau}\\left(t\\right)-W_{\\tau}\\left(s\\right)\\sim N\\left(0,t-s\\right).\n\\] De aquí se concluye que \\(W_{\\tau}(t)\\) tiene incrementos independientes y estacionarios.\nPor lo tanto, con todo lo anterior se concluye que \\(W_{\\tau}(t)\\) es un Movimiento Browniano.\n\n\nExercise 11.2 Sea \\(W_{1}\\left(t\\right),W_{2}\\left(t\\right)\\) movimientos brownianos independientes con punto inicial \\(\\left(W_{1}\\left(0\\right),W_{2}\\left(0\\right)\\right)\\neq\\left(0,0\\right)\\).\nDefina \\(X_{t}:=\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right).\\)\n\n\n\nDemuestre que \\(X_{t}\\) es una martingala local.\nDemuestre que \\(E\\left|X_{t}\\right|&lt;\\infty\\) para cada \\(t&gt;0\\).\nDemuestre que \\(X_{t}\\) no es una martingala.\n\n\n\nProof. \\((a)\\) Consideremos a, \\[\n\\tau_{n}=\\inf_{t}\\left\\{ X_{t}=n\\right\\}\n\\]\nDado que \\(X_{t}\\) no es acotada, se tiene que, \\[\n\\tau_{n}\\to\\infty,\\text{ cuando }n\\to\\infty,\\forall n.\n\\]\nAhora probaremos que \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es una martingala.\nPrimero veamos que es adaptado a la filtración:\nSi \\(\\tau_{n}\\geq t\\) lo tenemos por construcción, ya que, en este caso \\[\nX_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=X_{t},\n\\] y \\(X_{t}\\) si es adaptado con respecto a la filtración. Ahora si \\(\\tau_{n}&lt;t\\), tenemos que, \\[\nX_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=X_{\\tau_{n}}=n.\n\\] Además, observemos que, \\[\n\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=n\\right]\\subset\\left[\\tau_{n}&lt;t\\right],\n\\] y dado que \\(\\tau_{n}\\) es tiempo de paro, se cumple que \\(\\left[\\tau_{n}&lt;t\\right]\\in\\mathcal{F}_{t}\\). Por lo tanto, de la última relación se sigue que, \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es adaptado a la filtración.\nAhora solo nos queda probar que es una martingala. Sea \\(s&lt;t\\), se tiene que,\n\\[\\begin{align*}\nE\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\mid\\mathcal{F}_{s}\\right] & =E\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }1_{[t&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }1_{\\left[\\tau_{n}\\leq t\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{t}1_{[t&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq t\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{s}1_{[s&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq s\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =X_{s}1_{[s&lt;\\tau_{n}]}+X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq s\\right]}\\\\\n& =X_{\\min\\left\\{ s,\\tau_{n}\\right\\} },\n\\end{align*}\\]\ny con esto se concluye que \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es una martingala.\n\n\\((b)\\) Observemos que, como \\(X_{t} =\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\), entonces \\[\n\\exp\\left(X_{t}\\right) =W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right).\n\\] Asi,\n\\[\\begin{align*}\nE\\left[\\exp\\left(X_{t}\\right)\\right] & =E\\left[W_{1}^{2}\\left(t\\right)\\right]+E\\left[W_{2}^{2}\\left(t\\right)\\right]\\\\\n& =2t,\n\\end{align*}\\]\ndado que, \\(X_{t}\\geq0,\\forall t\\) y \\(X_{t} \\le\\exp\\left(X_{t}\\right)\\), entonces \\[\nE\\left[X_{t}\\right]\\leq2t&lt;\\infty,\\forall t\n\\] \\((c)\\) Primero recordemos lo siguiente:\nSi \\(X_{t}\\) es martingala entonces \\(E\\left[X_{t}\\right]\\) es constante, entonces este resultado nos diría que si \\(E\\left[X_{t}\\right]\\) no es constante, \\(X_{t}\\) no es martingala.\nDado lo anterior, supongamos que existe \\(c\\in\\mathbb{R}\\) tal que \\[\nE\\left[X_{t}\\right]=c,\\forall t\\Longrightarrow E\\left[\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\right] =c.\n\\] Así, \\[\n\\int_{0}^{\\infty}\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\mathrm{d}\\mathcal{P} =c,\n\\] de aquí se tendría que la integral es finita. Entonces \\[\nX_{t}\\to0,t\\to\\infty,\\text{ c.s}.\n\\] De esto último y de la continuidad de la exponencial, se concluye que \\[\nW_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\to1,t\\to\\infty,\\text{c.s}\n\\] Pero de lo demostrado del inciso anterior, sabemos que, \\[\nE\\left[W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right]=2t,\n\\] la cual converge a infinito, cuando \\(t\\to\\infty\\). Así, llegamos a una contradicción. Por lo tanto, \\(E\\left[X_{t}\\right]\\) no es constante, y se siguiría que \\(X_{t}\\) no puede ser martingala."
  },
  {
    "objectID": "Tarea10.html",
    "href": "Tarea10.html",
    "title": "12  Tarea 10",
    "section": "",
    "text": "Considere la ecuación diferencial estocástica lineal con ruido multiplicativo. \\[\n\\mathrm{d}Y\\left(t\\right)=\\left(\\mu+\\dfrac{1}{2}\\sigma^{2}\\right)Y\\left(t\\right)\\mathrm{d}t+\\sigma\\mathrm{d}W\\left(t\\right)\n\\tag{12.1}\\] Usando la función \\[\nu\\left(t,x\\right)=y_{0}\\exp\\left(\\mu t+\\sigma x\\right),\n\\] y la diferencial \\[\ndS(t)=dW(t)\n\\] Aplique la Fórmula de Ito a la función \\[\ndu(t,S_{t}).\n\\] Use esta relación para demostrar que \\[\nY\\left(t\\right)=Y\\left(0\\right)\\exp\\left(\\mu t+\\sigma W\\left(t\\right)\\right),\n\\tag{12.2}\\]\nresuelve (Equation 12.1). Es decir, \\[\ndY(t)=du(t,S_{t})=\\left(\\mu +\\frac{1}{2}\\sigma^{2}\\right)u(t,S_{t})dt+\\sigma u(t, S_{t})dW_{t}\n\\]\n\n\nSolution. Consideremos \\[\nu\\left(t,x\\right)=y_{0}\\exp\\left(\\mu t+\\sigma x\\right)\n\\]\ncalculando las derivadas parciales, resulta \\[\\begin{align*}\n\\partial_{t}u & =\\mu u\\\\\n\\partial_{x}u & =\\sigma u\\\\\n\\partial_{xx} & =\\sigma^{2}u.\n\\end{align*}\\]\nAdemás sabemos que,\n\\[\n\\mathrm{d}S_{t}=\\mathrm{d}W_{t}\n\\]\nEntonces \\[\\begin{align*}\n\\mathrm{d}u\\left(t,Y_{t}\\right) & =\\mu u\\mathrm{d}t+\\sigma u\\mathrm{d}Y_{t}+\\dfrac{1}{2}\\sigma^{2}u\\left(\\mathrm{d}Y_{t}\\right)^{2}\\\\\n& =\\mu u\\mathrm{d}t+\\sigma u\\mathrm{d}W_{t}+\\dfrac{1}{2}\\sigma^{2}u\\mathrm{d}t\\\\\n& =\\left(\\mu+\\dfrac{1}{2}\\sigma^{2}\\right)u\\mathrm{dt+\\sigma u\\mathrm{d}}W_{t},\n\\end{align*}\\]\nasí, \\[\\begin{align*}\nY_{t} & =u\\left(t,Y_{t}\\right)\\\\\n& =Y\\left(0\\right)\\exp\\left(\\mu t+\\sigma W_{t}\\right)\n\\end{align*}\\]\nPor lo tanto, \\(Y_{t}\\) resuelve (Equation 12.1).\n\n\nUse el hecho de que la relación en la ecuación en (Equation 12.2) resulve la ecuación (Equation 12.1), para confirmar que \\[\nY\\left(t\\right)=Y\\left(0\\right)\\exp\\left(\\left(\\mu-\\dfrac{1}{2}\\sigma^{2}\\right)t+\\sigma W\\left(t\\right)\\right),\n\\]\nresuelve,\n\\[\n\\mathrm{d}Y\\left(t\\right)=\\mu Y\\left(t\\right)\\mathrm{d}t+\\sigma Y\\left(t\\right)\\mathrm{d}W\\left(t\\right)\n\\]\n\n\nSolution. Consideremos \\[\nu\\left(t,x\\right)=y_{0}\\exp\\left(q_{1}t+q_{2}x\\right)\n\\]\ncalculando las parciales, se tiene que, \\[\\begin{align*}\n\\partial_{t}u & =q_{1}u\\\\\n\\partial_{x}u & =q_{2}u\\\\\n\\partial_{xx} & =q_{2}^{2}u\n\\end{align*}\\]\nnuevamente, usando que,\n\\[\n\\mathrm{d}S_{t}=\\mathrm{d}W_{t}\n\\]\nEntonces \\[\\begin{align*}\n\\mathrm{d}u\\left(t,Y_{t}\\right) & =q_{1}u\\mathrm{d}t+q_{2}u\\mathrm{d}Y_{t}+\\dfrac{1}{2}q_{2}^{2}u\\left(\\mathrm{d}Y_{t}\\right)^{2}\\\\\n& =q_{1}u\\mathrm{d}t+q_{2}u\\mathrm{d}W_{t}+\\dfrac{1}{2}q_{2}^{2}u\\mathrm{d}t\\\\\n& =\\left(q_{1}+\\dfrac{1}{2}q_{2}^{2}\\right)u\\mathrm{d}t+q_{2}u\\mathrm{d}W_{t},\n\\end{align*}\\] así, \\[\\begin{align*}\nq_{2} & =\\sigma\\\\\nq_{1}+\\dfrac{1}{2}\\sigma^{2} & =\\mu\\Longrightarrow q_{1} =\\mu -\\dfrac{1}{2}\\sigma^{2} ,\n\\end{align*}\\]\nPor lo tanto, de lo anterior se sigue que, \\[\nY_{t}=Y\\left(0\\right)\\exp\\left(\\left(\\mu -\\dfrac{1}{2}\\sigma^{2}\\right)t+\\sigma W_{t}\\right)\n\\]\n\n\nConsidere la siguiente ecuación diferencial estocástica lineal. \\[\n\\mathrm{d}S\\left(t\\right)=\\left(a_{1}\\mathrm{S}\\left(t\\right)+a_{2}\\right)\\mathrm{d}t+g\\left(S\\left(t\\right)\\right)\\mathrm{d}W\\left(t\\right),\n\\]\ndonde \\(g:\\mathbb{R}\\to\\mathbb{R}\\) es cualquier función global de Lipschitz con crecimiento lineal, y \\(a_{1},a_{2}\\) son dos constantes diferentes de cero. Use la forma integral de la ecuación diferencial estocástica, la propiedad de martingala de la integral de Ito y la notación \\[\nm\\left(t\\right)=E\\left[X_{t}\\right],\n\\] para deducir que \\[\nm\\left(t\\right)-m\\left(0\\right)=a_{1}\\int_{0}^{t}m\\left(s\\right)\\mathrm{d}s+a_{2}t\n\\]\nUsando que \\(m\\left(t\\right)\\) es la solución \\[\n\\dfrac{\\mathrm{d}m\\left(t\\right)}{\\mathrm{d}t}=a_{1}m\\left(t\\right)+a_{2},m\\left(0\\right)=E\\left[X_{0}\\right]\n\\]\nFinalmente, muestre que \\[\nE\\left[X\\left(t\\right)\\right]=-\\dfrac{a_{2}}{a_{1}}+\\left(E\\left[X\\left(0\\right)\\right]+\\dfrac{a_{2}}{a_{1}}\\right)\\exp\\left(a_{1}t\\right)\n\\]\n\n\nSolution. De la formula Integral, se tiene que\n\\[\\begin{align*}\nS\\left(t\\right) & =S\\left(0\\right)+\\int_{0}^{t}\\left(a_{1}\\mathrm{S}\\left(t\\right)+a_{2}\\right)\\mathrm{d}t+\\int_{0}^{t}g\\left(S\\left(s\\right)\\right)\\mathrm{d}W\\left(s\\right)\\\\\n& =S\\left(0\\right)+a_{1}\\int_{0}^{t}\\mathrm{S}\\left(t\\right)\\mathrm{d}t+a_{2}t+\\int_{0}^{t}g\\left(S\\left(s\\right)\\right)\\mathrm{d}W\\left(s\\right),\n\\end{align*}\\]\ncalculando esperanza, resulta \\[\nm\\left(t\\right) =m\\left(0\\right)+a_{1}\\int_{0}^{t}m\\left(t\\right)\\mathrm{d}t+a_{2}t+\\int_{0}^{t}E\\left[g\\left(S\\left(s\\right)\\right)\\right]\\mathrm{d}B\\left(s\\right)\n\\] entonces \\[\nm\\left(t\\right)-m\\left(0\\right) =a_{1}\\int_{0}^{t}m\\left(t\\right)\\mathrm{d}t+a_{2}t+E\\left[\\int_{0}^{t}g\\left(S\\left(s\\right)\\right)\\mathrm{d}B\\left(s\\right)\\right]\n\\] Dado que, \\(g\\) es de lipschitz y de crecimiento lineal, y además \\(S\\in L_{\\text{ad}}^{2}\\), se sigue que, \\(g\\left(S\\right)\\in L_{\\text{ad}}^{2}\\left(\\Omega_{a}^{b}\\right),\\) por lo tanto existe una constante \\(c\\) tal que \\[\nE\\left[\\int_{0}^{t}g\\left(S\\left(s\\right)\\right)\\mathrm{d}W\\left(s\\right)\\right]=c,\\ \\forall t\n\\]\nPor otro lado, sabemos que \\[\n\\int_{0}^{t}g\\left(S\\left(s\\right)\\right)\\mathrm{d}W\\left(s\\right)=\\lim_{n\\to\\infty}\\sum_{i=1}^{n}g\\left(S\\left(t_{i-1}\\right)\\right)\\left(B_{i}-B_{i-1}\\right),\n\\] además, para cada \\(i\\)\n\\[\\begin{align*}\nE\\left[g\\left(S\\left(t_{i-1}\\right)\\right)\\left(B_{i}-B_{i-1}\\right)\\right] & =E\\left[E\\left[g\\left(S\\left(t_{i-1}\\right)\\right)\\left(B_{i}-B_{i-1}\\right)\\mid\\mathcal{F}_{i-1}\\right]\\right]\\\\\n& =E\\left[g\\left(S\\left(t_{i-1}\\right)\\right)E\\left[\\left(B_{i}-B_{i-1}\\right)\\mid\\mathcal{F}_{i-1}\\right]\\right]\\\\\n& =0,\n\\end{align*}\\]\npor lo tanto, de los hechos anteriores, se tiene que \\[\nm\\left(t\\right)-m\\left(0\\right)=a_{1}\\int_{0}^{t}m\\left(t\\right)\\mathrm{d}t+a_{2}t,\n\\]\nahora, considere su forma diferencial. \\[\n\\dfrac{\\mathrm{d}m\\left(t\\right)}{\\mathrm{d}t}=a_{1}m\\left(t\\right)+a_{2},\n\\]\nla cual es una ecuación diferencial lineal. Ahora resolveremos esta ecuación: \\[\n\\dfrac{\\mathrm{d}m\\left(t\\right)}{\\mathrm{d}t}-a_{1}m\\left(t\\right)=a_{2},\n\\] definamos\n\\[\\begin{align*}\nu & =\\exp\\left(\\int -a_{1}\\mathrm{d}t\\right)\\\\\n& =e^{-a_{1}t},\n\\end{align*}\\]\nentonces, \\[\\begin{align*}\n\\dfrac{\\mathrm{d}}{\\mathrm{d}t}\\left[um\\left(t\\right)\\right] & =a_{2}u\\\\\nu\\left(t\\right)m\\left(t\\right) & =a_{2}\\int u\\left(t\\right)\\mathrm{d}t\\\\\nm\\left(t\\right) & =\\dfrac{a_{2}}{u\\left(t\\right)}\\int u\\left(t\\right)\\mathrm{d}t\\\\\n& =a_{2}e^{a_{1}t}\\int e^{-a_{1}t}\\mathrm{d}t\\\\\n& =-a_{2}e^{a_{1}t}\\left[-\\dfrac{1}{a_{1}}e^{-a_{1}t}+C\\right]\\\\\nm\\left(t\\right) & =-\\dfrac{a_{2}}{a_{1}}-Ca_{2}e^{a_{1}t},\n\\end{align*}\\]\nde la condición inicial se tiene que,. \\[\\begin{align*}\nm\\left(0\\right) & =-\\dfrac{a_{2}}{a_{1}}-Ca_{2}\\\\\n\\Rightarrow C & =-\\dfrac{1}{a_{1}}-\\dfrac{1}{a_{2}}m\\left(0\\right).\n\\end{align*}\\]\nPor lo tanto, \\[\nE\\left[X_{t}\\right]=-\\dfrac{a_{2}}{a_{1}}-\\left[-\\dfrac{a_{2}}{a_{1}}-E\\left[X_{0}\\right]\\right]\\exp\\left(a_{1}t\\right)\n\\]\n\n\nConsidere la siguiente ecuación diferencial estocástica lineal. \\[\n\\mathrm{d}S\\left(t\\right)=\\left(\\alpha\\left(t\\right)\\mathrm{S}\\left(t\\right)\\right)\\mathrm{d}t+\\beta\\left(t\\right)S\\left(t\\right)\\mathrm{d}W\\left(t\\right),\\quad S\\left(0\\right)=s_{0},\n\\]\ncon \\(s_{0}\\) constante y funciones \\(\\alpha,\\beta\\) integrables. Use la formula de Ito con la fórmula \\[\nu\\left(t,x\\right)=\\ln\\left(\\dfrac{x}{S_{0}}\\right),\n\\] para deducir que \\[\nS\\left(t\\right)=S\\left(0\\right)\\exp\\left(\\int_{0}^{t}\\left[\\alpha\\left(s\\right)-\\dfrac{1}{2}\\beta^{2}\\left(s\\right)\\right]\\mathrm{d}s+\\int_{0}^{t}\\beta\\left(s\\right)\\mathrm{d}W\\left(s\\right)\\right)\n\\]\n\n\nSolution. Calculemos las parciales de \\(u\\). \\[\\begin{align*}\nu_{t} & =0\\\\\nu_{x} & =\\dfrac{1}{x}\\\\\nu_{xx} & =-\\dfrac{1}{x^{2}}\n\\end{align*}\\]\nEntonces, se tiene que, \\[\\begin{align*}\n\\mathrm{d}u\\left(t,S_{t}\\right) & =\\alpha\\left(t\\right)\\mathrm{d}t+\\beta\\left(t\\right)\\mathrm{d}W\\left(t\\right)-\\dfrac{1}{2S_{t}^{2}}\\left(dS_{t}\\right)^{2}\\\\\n& =\\alpha\\left(t\\right)\\mathrm{d}t+\\beta\\left(t\\right)\\mathrm{d}W\\left(t\\right)-\\dfrac{1}{2S_{t}^{2}}\\beta^{2}S_{t}^{2}\\mathrm{d}t\\\\\n& =\\left[\\alpha\\left(t\\right)-\\dfrac{\\beta^{2}\\left(t\\right)}{2}\\right]\\mathrm{d}t+\\beta\\left(t\\right)\\mathrm{d}W\\left(t\\right),\n\\end{align*}\\]\nasí, \\[\\begin{align*}\n\\ln\\left(\\dfrac{S_{t}}{S_{0}}\\right) & =\\int_{0}^{t}\\left[\\alpha\\left(t\\right)-\\dfrac{\\beta^{2}\\left(t\\right)}{2}\\right]\\mathrm{d}t+\\int_{0}^{t}\\beta\\left(t\\right)\\mathrm{d}W\\left(t\\right)\\\\\nS_{t} & =S_{0}\\exp\\left(\\int_{0}^{t}\\left[\\alpha\\left(t\\right)-\\dfrac{\\beta^{2}\\left(t\\right)}{2}\\right]\\mathrm{d}t+\\int_{0}^{t}\\beta\\left(t\\right)\\mathrm{d}W\\left(t\\right)\\right)\n\\end{align*}\\]\n\n\nConsidere la siguiente ecuación diferencial estocástica lineal. \\[\n\\mathrm{d}S\\left(t\\right)=\\left(\\alpha\\left(t\\right)\\mathrm{S}\\left(t\\right)\\right)\\mathrm{d}t+\\beta\\left(t\\right)S\\left(t\\right)\\mathrm{d}W\\left(t\\right),S\\left(0\\right)=s_{0},\n\\]\ncon constantes \\(s_{0}\\) y funciones \\(\\alpha,\\beta\\) integrables. Considere \\[\\begin{align*}\n\\alpha\\left(t\\right) & =\\sin\\left(t\\right)\\\\\n\\beta\\left(t\\right) & =\\dfrac{t}{1+t}\\\\\ns_{0} & =1,\n\\end{align*}\\] sobre el intervalo \\(\\left[0,5\\right]\\).\nUsando el acercamiento apropiado, la salida del código reproduce 200 realizaciones de la solución con el proceso de Euler-Maruyama.\nAdapta el código para obtener la media de la solución de 1000 realizaciones y comparalo con la media de la solución de la forma diferencial, usando los mismos parámetros. Ilustra la diferencia con un log-plot de\n\\[\n\\ln\\left|S\\left(t\\right)-\\tilde{S}\\left(t\\right)\\right|,\n\\]\ndonde \\(S\\) es la solución de Euler y \\(\\tilde{S}\\) es la solución de la diferencial.\n\n\nSolution. \n\n\ncomparacion de Euler-Maruyama con la version analitica.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef strong_brownian(t, n): \n    dt = t / n\n    dw = np.zeros(n)\n    w = np.zeros(n)\n    for i in np.arange(1, n):\n        dw[i] = np.sqrt(dt)*np.random.standard_normal()\n        w[i] = w[i - 1] + dw[i]\n    time = np.linspace(0, t, n)\n    return time, w\n\n\ndef alpha(t):\n    y = np.sin(t)\n    return y\ndef beta(t):\n    y = t / (1.0 + t)\n    return y\n\ndef drift(t, x):\n    a = alpha(t) * x\n    return a\n\ndef diffusion(t, x):\n    b = beta(t) * x\n    return b\n\nsamples = 200\nsigma = 2 ** (-2)\nn_p = 100\nT = 5.0\nx_0 = 1.0\n\ndef get_em_solution(x_0, T, N, sigma):\n    x_t = np.zeros(N)\n    x_t[0] = x_0\n    dt = T / N\n    t, W = strong_brownian(T, N)\n    for i in np.arange(N - 1):\n        w_inc = W[i + 1] - W[i]\n        f = drift(t[i], x_t[i])\n        g = diffusion(t[i], x_t[i])\n        x_t[i + 1] = x_t[i] + f * dt + sigma * g * w_inc  # Importante la sigma.\n    return t, x_t\n\nfig, ax = plt.subplots()\ndf = []\nfor k in np.arange(samples):\n    t, x_t = get_em_solution(x_0, T, n_p, sigma)\n    df.append(x_t)\n    ax.plot(t, x_t, color=\"C0\", alpha=0.1)\nplt.show()\n\ns_0 = x_0\n\ndef u(t, sigma):\n    y = alpha(t) - 0.5 * (sigma * beta(t)) ** 2\n    return y\ndef v(t,sigma):\n    y = sigma * beta(t)\n    return y\n\ndef ito_n(n_points: int, t: float, sigma: float):\n    time, w = strong_brownian(t, n_points)\n    integral = np.zeros(n_points)\n    for i in range(n_points - 1):\n        integral[i] = v(time[i], sigma) * (w[i + 1] - w[i])\n    ito = integral.sum()\n    return ito\n\n\ndef riemann_integral(a, b, n_points, sigma):\n    r = np.zeros(n_points)\n    time = np.linspace(a,b,n_points)\n    for i in range(n_points - 1):\n        r[i] = u(time[i], sigma) * (time[i + 1] - time[i])\n    riemann = r.sum()\n    return riemann\n\ndef St(a_0,t,n_p, sigma):\n    rmnn = riemann_integral(0, t, n_p, sigma)\n    ito = ito_n(n_p, t, sigma) \n    y = a_0 * np.exp(rmnn + ito)\n    return y\n\nS = []\nfor i in range(n_p):\n    S.append(St(x_0, t[i], n_p, sigma))\n\nplt.plot(t, S)\nplt.show()\n\nem_solutions = np.array(df)\n\nem_mean = em_solutions.mean(axis = 0)\n\ngeneral_estimation = np.zeros((samples, n_p))\n\nfor i in range(samples):\n    for j in range(n_p):\n        general_estimation[i, j] = St(x_0, t[j], n_p, sigma)\n\ngeneral_mean = general_estimation.mean(axis = 0)\n\n\nplt.plot(t, em_mean, 'r')\nplt.plot(t, general_mean, 'b', alpha = 0.5)\nplt.legend(['Euler-Maruyama','Integral de Ito'])\nplt.show()\n\n \nplt.loglog(t, np.abs(em_mean - general_mean))\nplt.title(\"Grafica Log-Plot de |EM(t) - G(t)|\")\nplt.show()\n\n\n\n   \nFigure 12.1: ?(caption)"
  },
  {
    "objectID": "ProyectoFinal.html#objetivo",
    "href": "ProyectoFinal.html#objetivo",
    "title": "13  Proyecto Final",
    "section": "13.1 objetivo:",
    "text": "13.1 objetivo:\nNuestra referencia principal sera el articulo “Analytic Solution of a Stochastic Richards Equation driven by Brownian motion” para encontrar la solución de la ecuación de Richard, deducir la forma y condiciones para obtener una distribución estacionaria y también contrastar el comportamiento de la realización de la solución respecto a su contraparte determinista."
  },
  {
    "objectID": "ProyectoFinal.html#introducción",
    "href": "ProyectoFinal.html#introducción",
    "title": "13  Proyecto Final",
    "section": "13.2 Introducción:",
    "text": "13.2 Introducción:\nUno de los modelos más famosos de la dinámica de poblaciones es la ecuación logística que se propuso por primera vez por P F Verhulst en 1838. La ecuación logística, también conocida como ecuación de Verhulst, viene dada por la ecuación diferencial ordinaria \\[\ndN(t)=rN(t)\\left(1-\\frac{N(t)}{K}\\right)dt\n\\] donde \\(N(t)\\) es el tamaño de la población en el momento \\(t\\), \\(r\\) es la tasa de crecimiento intrínseco y \\(K&gt; 0\\) es la capacidad de carga/nivel de saturación.\nEn 1959, FJ Richard en propuso la siguiente modificación de la ecuación logística para modelar el crecimiento de poblaciones biológicas: \\[\ndN(t)=rN(t)\\left(1-\\left(\\frac{N(t)}{K}\\right)^{\\alpha}\\right)dt,\n\\tag{13.1}\\]\ncon la condición inicial \\(N(0)=N_{0}\\), se asume que \\(0&lt;N_{0}&lt;K\\) y \\(\\alpha&gt;0\\) es el exponente de la desviación de la curva logística estándar.\nAhora construimos el modelo estocástico de Richard insertando el término de ruido multiplicativo en el modelo determinista (Equation 13.1) para obtener una ecuación aleatoria. Escribimos \\(N_{t}\\) en lugar de \\(N(t)\\) para enfatizar que \\(N_{t}\\), ya no es una función determinista sino una variable aleatoria, el ruido como el ruido blanco gaussiano \\(dB_{t}\\) y obtenemos la ecuación diferencial estocástica en el sentido de Itô \\[\ndN(t)=rN(t)\\left(1-\\left(\\frac{N(t)}{K}\\right)^{\\alpha}\\right)dt+\\sigma N_{t}dB_{t}\n\\tag{13.2}\\] donde \\(\\sigma\\) es el coeficiente de difusión que mide el tamaño de la fluctuación del ruido. Para resolver la ecuación diferencial estocástica (Equation 13.2) necesitamos algunos resultados del cálculo estocástico de Itô.\nPrimero, recordemos que un proceso estocástico \\(\\left(X_{t}\\right)_{t\\geq 0}\\) se llama adaptado si existe un espacio de probabilidad filtrado \\(\\left(\\Omega,\\mathcal{F},\\left(\\mathcal{F}_{t}\\right)_{t\\geq 0},\\mathbb{P}\\right)\\) tal que, para cada \\(t\\geq 0\\) la variable aleatoria \\(X_{t}\\) está definida en \\(\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)\\) así como \\(\\mathcal{F}_{t}\\)-medible. Un proceso estocástico adaptado \\(\\left(X_{t}\\right)_{t\\in [0,T]}\\) se llama proceso de Itô si se puede escribir de la forma \\[\nX_{t}=X_{0}+\\int_{0}^{t}b\\left(s\\right)\\mathrm{d}s+\\int_{0}^{t}\\sigma\\left(s\\right)\\mathrm{d}B_{s},\\,X_{0}=x_{0},\\,t\\in[0,T],\n\\] donde \\(b\\) y \\(\\sigma\\) son procesos estocásticos adaptados que satisfacen \\(\\int_{0}^{t}|b(s)|ds&lt;\\infty\\) y \\(\\int_{0}^{t}\\sigma(s)^{2}ds&lt;\\infty\\). El proceso \\(\\langle X\\rangle_{t}:=\\langle X,X\\rangle_{t}=\\int_{0}^{t}\\sigma(s)^{2}ds\\) es llamada la variación cuadrática de \\(X\\)."
  },
  {
    "objectID": "ProyectoFinal.html#desarrollo",
    "href": "ProyectoFinal.html#desarrollo",
    "title": "13  Proyecto Final",
    "section": "13.3 Desarrollo:",
    "text": "13.3 Desarrollo:\nPara encontrar la solucion de la ecuación diferencial estocástica de Richard (Equation 13.2) primero necesitamos los siguientes dos Teoremas.\n\n\nTheorem 13.1 Si \\((X_{t})_{t\\in[0,T]}\\) es un proceso de Itô y \\(F\\in C^{1,2}([0,T],\\mathbb{R})\\), entonces \\[\nF(T,X_{t})-F(0,X_{0})=\\int_{0}^{T}\\frac{\\partial F}{\\partial x}(t,X_{t})dX_{t}+\\int_{0}^{T}\\frac{\\partial F}{\\partial t}(t,X_{t})dt+\\frac{1}{2}\\int_{0}^{T}\\frac{\\partial^{2} F}{\\partial x^{2}}(t,X_{t})d\\langle X\\rangle_{t}.\n\\]\n\n\nProof. Sea \\(X_{t}\\) un proceso de Itô dado por \\[\nX_{t}=X_{0}+\\int_{0}^{t}b\\left(s\\right)\\mathrm{d}s+\\int_{0}^{t}\\sigma\\left(s\\right)\\mathrm{d}B_{s},\\,X_{0}=x_{0},\\,t\\in[0,T].\n\\tag{13.3}\\] Sea \\(F\\left(t,x\\right)\\), dado que \\(F\\in C^{1,2}\\) entonces \\(F\\left(t,X_{t}\\right)\\) es también un proceso de Itô.\nUsaremos la serie de Taylor y la tabla de Itô para encontrar la formula de Itô del proceso \\(F\\).\nPrimero recordemos que la serie de Taylor para dos variables, esta dada por, \\[\\begin{eqnarray*}\nF\\left(t,x\\right) & = & F\\left(t_{0},x_{0}\\right)+\\partial_{t}F(t_{0},x_{0})\\left(t-t_{0}\\right)+\\partial_{x}F(t_{0},x_{0})\\left(x-x_{0}\\right)+\\frac{1}{2}\\left(\\partial_{xx}^{2}F\\left(t_{0},x_{0}\\right)\\left(x-x_{0}\\right)^{2}\\right.\\\\\n&  & \\left.+\\partial_{xt}^{2}F\\left(t_{0},x_{0}\\right)\\left(t-t_{0}\\right)\\left(x-x_{0}\\right)+\\partial_{tx}^{2}F\\left(t_{0},x_{0}\\right)\\left(x-x_{0}\\right)\\left(t-t_{0}\\right)+\\partial_{tt}^{2}F\\left(t_{0},x_{0}\\right)\\left(t-t_{0}\\right)^{2}\\right).\n\\end{eqnarray*}\\] Sea \\(\\Delta_{n}=\\{t_{0}=0,t_{1},\\dots,t_{n}=T\\}\\) una partición del intervalo \\([0,T]\\), entonces \\[\\begin{eqnarray*}\nF\\left(T,X_{t}\\right)-F\\left(0,X_{0}\\right) & = & \\sum_{i=1}^{n}\\frac{\\partial F}{\\partial t}(t_{i-1},X_{t_{i-1}})\\left(t_{i}-t_{i-1}\\right)+\\sum_{i=1}^{n}\\frac{\\partial F}{\\partial x}(t_{i-1},X_{t_{i-1}})\\left(X_{t_{i}}-X_{t_{i-1}}\\right)\\\\\n&  & +\\sum_{i=1}^{n}\\frac{1}{2}\\frac{\\partial^{2}F}{\\partial x \\partial  x}(t_{i-1},X_{t_{i-1}})\\left(X_{t_{i}}-X_{t_{i-1}}\\right)^{2}+\\sum_{i=1}^{n}\\frac{1}{2}\\frac{\\partial ^{2}F}{\\partial x\\partial t}(t_{i-1},X_{t_{i-1}})\\left(t_{i}-t_{i-1}\\right)\\left(X_{t_{i}}-X_{t_{i-1}}\\right)\\\\\n&  & +\\sum_{i=1}^{n}\\frac{1}{2}\\frac{\\partial^{2}F}{\\partial t\\partial x}(t_{i-1},X_{t_{i-1}})\\left(X_{t_{i}}-X_{t_{i-1}}\\right)\\left(t_{i}-t_{i-1}\\right)+\\sum_{i=1}^{n}\\frac{1}{2}\\frac{\\partial^{2}F}{\\partial t \\partial t}(t_{i-1},X_{t_{i-1}})\\left(t_{i}-t_{i-1}\\right)^{2}\n\\end{eqnarray*}\\] Por otro lado, dado que, \\(\\left(t_{i}-t_{i-1}\\right)=dt_{i}\\), \\(\\left(t_{i}-t_{i-1}\\right)\\left(X_{t_{i}}-X_{t_{i-1}}\\right)=dt_{i}dX_{t_{i}},\\) y \\(\\left(X_{t_{i}}-X_{t_{i-1}}\\right)=(dX_{i})^{2}\\), de la tabla de Itô y de la ecuación (Equation 13.3), resulta para cada \\(i=1,\\dots,n\\), \\[\n\\left(X_{t_{i}}-X_{t_{i-1}}\\right)^{2}=\\sigma^{2}(t_{i})dt_{i},\n\\]\n\\[\n\\left(t_{i}-t_{i-1}\\right)\\left(X_{t_{i}}-X_{t_{i-1}}\\right)=0,\n\\]\n\\[\n\\left(X_{t_{i}}-X_{t_{i-1}}\\right)\\left(t_{i}-t_{i-1}\\right)=0,\n\\]\n\\[\n\\left(t_{i}-t_{i-1}\\right)^{2}=0.\n\\] Sustituyendo las relaciones anteriores se obtiene, \\[\\begin{eqnarray*}\nF\\left(T,X_{t}\\right)-F\\left(0,X_{0}\\right) & = & \\sum_{i=1}^{n}\\frac{\\partial F}{\\partial t}(t_{i-1},X_{t_{i-1}})dt_{i}+\\sum_{i=1}^{n}\\frac{\\partial F}{\\partial x}(t_{i-1},X_{t_{i-1}})dX_{t_{i}}\\\\\n&  & +\\sum_{i=1}^{n}\\frac{1}{2}\\frac{\\partial^{2}F}{\\partial x \\partial  x}(t_{i-1},X_{t_{i-1}})\\sigma^{2}(t_{i})dt_{i}.\n\\end{eqnarray*}\\] Haciendo \\(\\|\\Delta_{n}\\|\\to0\\), se tiene que, \\[\n\\sum_{i=1}^{n}\\frac{\\partial F}{\\partial t}(t_{i-1},X_{t_{i-1}})dt_{i}\\rightarrow \\int_{0}^{T} \\frac{\\partial F}{\\partial t}(t,X_{t})dt\n\\] \\[\n\\frac{1}{2}\\sum_{i=1}^{n}\\frac{\\partial^{2}F}{\\partial x^{2}}(t_{i-1},X_{t_{i-1}})\\sigma^{2}(t_{i})dt_{i}\\rightarrow \\frac{1}{2}\\int_{0}^{T} \\frac{\\partial^{2}F}{\\partial x^{2}}(t,X_{t})\\sigma^{2}(t)dt\n\\] y del Teorema 5.3.3 del Kuo, \\[\n\\sum_{i=1}^{n}\\frac{\\partial F}{\\partial x}(t_{i-1},X_{t_{i-1}})dX_{t_{i}}\\rightarrow \\int_{0}^{T} \\frac{\\partial F}{\\partial x}(t,X_{t})dX_{t}.\n\\] Entonces de todo lo anterior se sigue que, \\[F\\left(T,X_{t}\\right)-F\\left(0,X_{0}\\right)=\\int_{0}^{T} \\frac{\\partial F}{\\partial t}(t,X_{t})dt+\\int_{0}^{T} \\frac{\\partial F}{\\partial x}(t,X_{t})dX_{t}+\\frac{1}{2}\\int_{0}^{T} \\frac{\\partial^{2}F}{\\partial x^{2}}(t,X_{t})\\sigma^{2}(t)dt.\\] Además como \\(\\sigma^{2}dt=d\\langle X\\rangle_{t}\\), se concluye que \\[\nF\\left(T,X_{t}\\right)-F\\left(0,X_{0}\\right)=\\int_{0}^{T} \\frac{\\partial F}{\\partial t}(t,X_{t})dt+\\int_{0}^{T} \\frac{\\partial F}{\\partial x}(t,X_{t})dX_{t}+\\frac{1}{2}\\int_{0}^{T} \\frac{\\partial^{2}F}{\\partial x^{2}}(t,X_{t})d\\langle X\\rangle_{t}\n\\] \\(\\blacksquare\\)\n\nUna ecuación diferencial estocástica lineal es una ecuación de la forma \\[\ndX_{t}=\\left(a_{1}(t)X_{t}+a_{2}(t)\\right)dt+\\left(b_{1}(t)X_{t}+b_{2}(t)\\right)dB_{t}.\\quad X_{0}=x_{0},\n\\tag{13.4}\\] donde \\(a_{i}\\) y \\(b_{i}\\), \\(i=1,2\\) son funciones deterministas, acotadas en todo intervalo finito \\([0,T]\\).\n\nTheorem 13.2 El proceso estocástico \\[\nX_{t}=\\Phi_{t}^{-1}\\left(x_{0}+\\int_{0}^{t}\\left(a_{2}(s)-b_{1}(s)b_{2}(s)\\right)\\Phi_{s}ds+\\int_{0}^{t}b_{2}(s)\\Phi_{s}dB_{s}\\right),\\quad t\\geq 0,\n\\] donde \\(\\Phi_{t}:=e^{-\\left(\\int_{0}^{t}\\left(a_{1}(s)-\\frac{1}{2}b_{1}^{2}(s)\\right)ds+\\int_{0}^{t}b_{1}(s)dB_{s}\\right)}\\) es la solución de la ecuación lineal diferencial estocástica (Equation 13.4).\n\n\nProof. Primero, necesitamos encontrar \\(d(\\Phi_{t}X_{t})\\). Para esto, recordemos de la sección 7.5 del Kuo H-H, se tiene que para dos procesos de Itô \\(X_{t}\\), \\(Y_{t}\\) se satisface que \\[\nd(X_{t}Y_{t})=Y_{t}dX_{t}+X_{t}dY_{t}+dX_{t}dY_{t}.\n\\]\nLa igualdad anterior se llama la fórmula del producto de Itô.\nPor la fórmula del producto de Itô, aplicada para \\(\\Phi_{t}\\) y \\(X_{t}\\) tenemos que \\[\nd(\\Phi_{t}X_{t})=\\Phi_{t}dX_{t}+X_{t}d\\Phi_{t}+(d\\Phi_{t})(dX_{t}).\n\\tag{13.5}\\] Sea \\(Z_{t}=\\int_{0}^{t}\\left(a_{1}(s)-\\frac{1}{2}b_{1}^{2}(s)\\right)ds+\\int_{0}^{t}b_{1}(s)dB_{s}\\), entonces \\(\\Phi_{t}=e^{-Z_{t}}\\), usando la fórmula de Itô para encontrar \\(d\\Phi_{t}\\) se tiene que:\n\\[\\begin{eqnarray*}\nd\\Phi_{t} & = & d(e^{-Z_{t}})\\\\\n& = & -e^{-Z_{t}}dZ_{t}-\\frac{1}{2}\\left(de^{-Z_{t}}\\right)(dZ_{t})\\\\\n& = & - \\Phi_{t}dZ_{t}+\\frac{1}{2}e^{-Z_{t}}(dZ_{t})^{2}\\\\\n& = & \\Phi_{t}\\left(-a_{1}(t)dt+\\frac{1}{2}b_{1}^{2}(t)dt-b_{1}(t)dB_{t}\\right)+\\frac{1}{2}\\Phi_{t}b_{1}^{2}(t)dt\\\\\n& = & \\Phi_{t}\\left(-a_{1}(t)dt+b_{1}^{2}(t)dt-b_{1}(t)dB_{t}\\right)\n\\end{eqnarray*}\\]\nDe esta ultima igualdad, la ecuacion (Equation 13.4) y de la tabla de Itô, se sigue que \\[\\begin{eqnarray*}\n(d\\Phi_{t})(dX_{t}) & = & \\left(\\Phi_{t}\\left(-a_{1}(t)d+b_{1}^{2}(t)dt-b_{1}(t)dB_{t}\\right)\\right)\\big(\\left(a_{1}(t)X_{t}+a_{2}(t)\\right)dt+\\left(b_{1}(t)X_{t}+b_{2}(t)\\right)dB_{t}\\big)\\\\\n& = & -\\Phi_{t}b_{1}^{2}(t)X_{t}dt-\\Phi_{t}b_{1}(t)b_{2}(t)dt\\\\\n& = & -\\Phi_{t}b_{1}(t)\\{b_{1}(t)X_{t}+b_{2}(t)\\}dt\n\\end{eqnarray*}\\]\nsustituyendo el valor de \\(d\\Phi_{t}\\) y el de \\((d\\Phi_{t})(dX_{t})\\) en la ecuación (Equation 13.5) resulta \\[\\begin{eqnarray*}\nd(\\Phi_{t}X_{t}) & = & \\Phi_{t}dX_{t}+X_{t}\\Phi_{t}\\left(-a_{1}(t)dt+b_{1}^{2}(t)dt-b_{1}(t)dB_{t}\\right)-\\Phi_{t}b_{1}(t)\\{b_{1}(t)X_{t}+b_{2}(t)\\}dt\\\\\n& = & \\Phi_{t}\\{dX_{t}-a_{1}(t)X_{t}dt-b_{1}(t)X_{t}dB_{t}-b_{2}(t)b_{1}(t)d(t)\\},\n\\end{eqnarray*}\\] sustituyendo la ecuación (Equation 13.4) en la igualdad anterior resulta, \\[\nd(\\Phi_{t}X_{t})=\\Phi_{t}\\{b_{2}(t)dB(t)+a_{2}(t)dt-b_{2}(t)b_{1}(t)dt\\},\n\\] entonces se tiene que, \\[\n\\Phi_{t}X_{t}= x_{0}+\\int_{0}^{t}\\Phi_{s}b_{2}(s)dB(s)+\\int_{0}^{t}\\Phi_{s}(a_{2}(s)-b_{2}(s)b_{1}(s))ds\n\\] Al dividir ambos lados por \\(\\Phi_{t}\\) obtenemos la solución \\(X_{t}\\) de la Ecuación (Equation 13.4), \\[\nX_{t}= \\Phi_{t}^{-1}\\left( x_{0}+\\int_{0}^{t}\\Phi_{s}b_{2}(s)dB(s)+\\int_{0}^{t}\\Phi_{s}(a_{2}(s)-b_{2}(s)b_{1}(s))ds\\right)\n\\] \\(\\blacksquare\\)\n\nSolución exacta de la ecuación estocástica de Richards.\n\\[\ndN_{t}=rN_{t}\\left(1-\\left(\\frac{N_{t}}{K}\\right)^{\\alpha}\\right)dt +\\sigma N_{t}dB_{t}\n\\tag{13.6}\\]\n\nTheorem 13.3 La solución de la ecuación estocástica de Richards (Equation 13.6) viene dada por \\[\nN_{t}=N_{0}\\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(1+\\left(\\frac{N_{0}}{K}\\right)^{\\alpha}r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)}^{-\\frac{1}{\\alpha}}\n\\]\n\n\nProof. Sea \\(X_{t}:=\\dfrac{N_{t}}{K}\\). Entonces de la ecuacion (Equation 13.6), obtenemos \\(dX_{t}=rX_{t}\\left(1-X_{t}^{\\alpha}\\right)dt+\\sigma X_{t}dB_{t}\\) con condición inicial \\(X_{0}=\\dfrac{N_{0}}{K}\\). Sea \\(F(t,x):=x^{-\\alpha}\\) entonces, \\[\n\\frac{\\partial F}{\\partial x}=-\\frac{\\alpha}{x^{\\alpha+1}},\\,\\frac{\\partial^{2} F}{\\partial x^{2}}=\\frac{\\alpha(\\alpha+1)}{x^{\\alpha+2}}\\text{ y }\\frac{\\partial F}{\\partial t}=0.\n\\] La variación cuadrática de \\(X_{t}\\) esta dada por \\(\\langle X\\rangle_{t}=\\int_{0}^{t}\\sigma^{2}X_{s}^{2}ds\\) lo que significa \\(d\\langle X\\rangle_{t}=\\sigma^{2}X_{t}^{2}dt\\). Usando la notación \\(Y_{t}:=X_{t}^{-\\alpha}\\) y aplicando la formula de Itô Teorema Theorem 13.1 obtenemos\n\\[\\begin{eqnarray*}\ndY_{t} & = & -\\frac{\\alpha}{X_{t}^{\\alpha+1}}dX_{t}+\\frac{1}{2}\\frac{\\alpha(\\alpha+1)}{X_{t}^{\\alpha+2}}d\\langle X\\rangle_{t}\\\\\n& = & -\\frac{\\alpha}{X_{t}^{\\alpha+1}}dX_{t}+\\frac{1}{2}\\frac{\\alpha(\\alpha+1)}{X_{t}^{\\alpha+2}}\\sigma^{2}X_{t}^{2}dt\\\\\n& = & -\\frac{\\alpha}{X_{t}^{\\alpha+1}}\\left(rX_{t}\\left(1-X_{t}^{\\alpha}\\right)dt+\\sigma X_{t}dB_{t}\\right)+\\frac{1}{2}\\frac{\\alpha(\\alpha+1)}{X_{t}^{\\alpha}}\\sigma^{2}dt\\\\\n& = & -\\frac{\\alpha r\\left(1-X_{t}^{\\alpha}\\right)dt}{X_{t}^{\\alpha}}-\\frac{\\sigma\\alpha dB_{t}}{X_{t}^{\\alpha}}+\\frac{1}{2}\\frac{\\alpha(\\alpha+1)}{X_{t}^{\\alpha}}\\sigma^{2}dt\\\\\n& = & -\\alpha r Y_{t}dt+\\alpha rdt-\\sigma\\alpha Y_{t}dB_{t}+\\frac{1}{2}\\alpha(\\alpha+1)Y_{t}\\sigma^{2}dt\\\\\n& = & \\left(\\left(\\frac{1}{2}\\alpha(\\alpha+1)\\sigma^{2}-r\\alpha\\right)Y_{t}+r\\alpha\\right)dt-\\sigma\\alpha Y_{t}dB_{t}.\n\\end{eqnarray*}\\]\nEsta es una ecuación diferencial estocástica lineal con factor integrante, donde, \\[a_{1}(t)=\\frac{1}{2}\\alpha(\\alpha+1)\\sigma^{2}-r\\alpha\n   ,\\,a_{2}(t)=r\\alpha,\\, b_{1}(t)=-\\sigma\\alpha,\\,b_{2}(t)=0.\n\\] entonces es una ecuación del tipo de (Equation 13.4), con \\[\\begin{eqnarray*}\n\\Phi_{t} & = & \\exp{\\left(-\\left(\\int_{0}^{t}\\left(\\frac{1}{2}\\alpha(\\alpha+1)\\sigma^{2}-r\\alpha\\right)ds-\\frac{1}{2}\\int_{0}^{t}(-\\sigma\\alpha)^{2}ds-\\int_{0}^{t}\\sigma\\alpha dB_{s}\\right)\\right)}\\\\\n& = & \\exp{\\left(-\\int_{0}^{t}\\frac{1}{2}\\alpha(\\alpha+1)\\sigma^{2}ds+\\int_{0}^{t}r\\alpha ds+\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}\\alpha^{2}ds+\\int_{0}^{t}\\sigma\\alpha dB_{s}\\right)}\\\\\n& = & \\exp{\\left(-\\frac{1}{2}\\int_{0}^{t}\\alpha^{2}\\sigma^{2}ds-\\frac{1}{2}\\int_{0}^{t}\\alpha\\sigma^{2}ds+\\int_{0}^{t}r\\alpha ds+\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}\\alpha^{2}ds+\\int_{0}^{t}\\sigma\\alpha dB_{s}\\right)}\\\\\n& = & \\exp{\\left(-\\frac{1}{2}\\int_{0}^{t}\\alpha\\sigma^{2}ds+\\int_{0}^{t}r\\alpha ds+\\int_{0}^{t}\\sigma\\alpha dB_{s}\\right)}\\\\\n& = & \\exp{\\left(-\\frac{1}{2}\\alpha\\sigma^{2}t+r\\alpha t+\\sigma\\alpha B_{t}\\right)}.\n\\end{eqnarray*}\\]\nPor lo tanto, según el Teorema Theorem 13.2, la solución de la ecuación diferencial estocástica lineal en \\(Y_{t}\\) es\n\\[\\begin{eqnarray*}\nY_{t} & = &  \\Phi_{t}^{-1}\\left(Y_{0}+\\int_{0}^{t}r\\alpha\\Phi_{s}ds\\right)\\\\\n& = &  \\exp{\\left(\\frac{1}{2}\\alpha\\sigma^{2}t-r\\alpha t-\\sigma\\alpha B_{t}\\right)}\\left(Y_{0}+\\int_{0}^{t}r\\alpha\\exp{\\left(-\\frac{1}{2}\\alpha\\sigma^{2}s+r\\alpha s+\\sigma\\alpha B_{s}\\right)}ds\\right)\\\\\n& = &  \\exp{\\left(\\alpha\\left(\\left(\\frac{1}{2}\\sigma^{2}-r \\right)t-\\sigma B_{t}\\right)\\right)}\\left(Y_{0}+r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right).\n\\end{eqnarray*}\\] Reescribiendo la última expresión en términos de \\(X_{t}\\) se obtiene \\[\nX_{t}=\\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(\\left(\\frac{1}{X_{0}}\\right)^{\\alpha}+r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)^{-\\frac{1}{\\alpha}}}\n\\]\nFinalmente, la solución de (Equation 13.6) viene dada por\n\\[\\begin{eqnarray*}\nX_{t} & = & \\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(\\left(\\frac{1}{X_{0}}\\right)^{\\alpha}+\\left(\\frac{X_{0}}{X_{0}}\\right)^{\\alpha}r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)^{-\\frac{1}{\\alpha}}}\\\\\n& = & \\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(\\frac{1}{X_{0}}\\right)^{-1}\\left(1+(X_{0})^{\\alpha}r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)^{-\\frac{1}{\\alpha}}}\\\\\n& = & \\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(\\frac{N_{0}}{K}\\right)\\left(1+\\left(\\frac{N_{0}}{K}\\right)^{\\alpha}r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)^{-\\frac{1}{\\alpha}}}.\n\\end{eqnarray*}\\] Por lo tanto, \\[\nN_{t}=N_{0}\\exp{\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)t+\\sigma B_{t}\\right)\\left(1+\\left(\\frac{N_{0}}{K}\\right)^{\\alpha}r\\alpha\\int_{0}^{t}\\exp{\\left(\\alpha\\left(\\left(r-\\frac{1}{2}\\sigma^{2}\\right)s+\\sigma B_{s}\\right)\\right)}ds\\right)^{-\\frac{1}{\\alpha}}}.\n\\] \\(\\blacksquare\\)\n\nPor último, para encontrar la distribución estacionaria, utilizaremos la siguiente proposición.\n\nProposition 13.1 Sea \\(X\\) un proceso de difusión con generador \\(A\\) y densidad de transición \\(p=p(t,x,y)\\) que tiene una densidad estacionaria \\(p_{0}=p_{0}(y)\\) en el intervalo \\((a,b)\\). Supongamos que \\(p\\) y \\(p_{0}\\) son funciones continuas y derivadas parciales continuas \\(\\frac{\\partial p}{\\partial y}, \\frac{\\partial p}{\\partial t}, \\frac{\\partial^{2} p}{\\partial y^{2}}, \\frac{\\partial p_{0}}{\\partial y}\\) y \\(\\frac{\\partial^{2} p_{0}}{\\partial y^{2}}\\). Supongamos también que \\(\\sigma(x)&gt;0\\), \\(x\\in(a,b)\\). Entonces la densidad estacionaria \\(p_{0}\\) de \\(X\\) es de la forma \\[\np_{0}(y)=\\dfrac{N}{\\sigma^{2}(y)}\\exp{\\left(2\\int_{k}^{y}\\frac{b(u)}{\\sigma^{2}(u)}du\\right)},\\quad y\\in(a,b)\n\\] donde \\(k\\) es un punto arbitrario de \\((a,b)\\) y \\(N\\) es la constante de normalización tal que \\(\\int_{a}^{b}p_{0}(y)dy=1\\).\n\n\nProof. Usaremos la ecuación de Fokker-Planck (FPE), para la evolución de la densidad de probabilidad de transición \\(p(y,t|x)\\) para la formula de Itô, la cual esta dada por: \\[\n\\partial_{t}p(y,t|x)=-\\partial_{y}f(y)p(y,t|x)+\\frac{\\sigma^{2}}{2}\\partial_{yy}g^{2}(y)p(y,t|x),\n\\] con \\(p_{s}(x)\\) la solución estacionaria de FPE que se puede escribir de la siguiente forma: \\[\n\\partial_{t}p\\left(x,t|x_{0},0\\right)+\\partial_{x}J\\left(x,t|x_{0},0\\right)=0,\n\\] donde \\[\nJ\\left(x,t|x_{0},0\\right)=f(x)p\\left(x,t|x_{0},0\\right)-\\frac{\\sigma^{2}}{2}\\partial_{x}g^{2}(y)p(y,t|x_{0},0),\n\\] Por lo tanto, se tiene que, para la densidad de probabilidad estacionaria \\(p_{s}(x):\\) \\[\n-f(x)p_{s}(x)+\\frac{\\sigma^{2}}{2}\\partial_{x}g^{2}(x)p_{s}(x)=-J,\n\\tag{13.7}\\] para resolver la ecuación (Equation 13.7) definimos la función auxiliar \\(q(x)=g^{2}(x)p_{s}(x)\\). La FPE estacionaria queda dada por: \\[\n\\partial_{x}q(x)=\\frac{2}{\\sigma^{2}}\\frac{f(x)}{g^{2}(x)}q(x)-\\frac{2}{\\sigma^{2}}J,\n\\] y su solución queda determinada por: \\[\nq(x)=N\\exp{\\left(\\frac{2}{\\sigma^{2}}\\int^{x}\\frac{f(u)}{g^{2}(u)}du\\right)}-\\frac{2J}{\\sigma^{2}}\\int^{x}\\exp{\\left(\\frac{2}{\\sigma^{2}}\\int_{z}^{x}\\frac{f(u)}{g^{2}(u)}du\\right)dz}\n\\] por lo tanto la densidad estacionaria \\[\np_{s}(x)=\\frac{N}{g^{2}(x)}\\exp{\\left(\\frac{2}{\\sigma^{2}}\\int^{x}\\frac{f(u)}{g^{2}(u)}du\\right)}-\\frac{2J}{\\sigma^{2}g^{2}(x)}\\int^{x}\\exp{\\left(\\frac{2}{\\sigma^{2}}\\int_{z}^{x}\\frac{f(u)}{g^{2}(u)}du\\right)dz}\n\\] en este caso \\(J=0\\), así \\[\np_{s}(x)=\\frac{N}{g^{2}(x)}\\exp{\\left(\\frac{2}{\\sigma^{2}}\\int^{x}\\frac{f(u)}{g^{2}(u)}du\\right)}.\n\\]\nQue por todas las notaciones anteriores, resulta \\[\np_{0}(y)=\\dfrac{N}{\\sigma^{2}(y)}\\exp{\\left(2\\int_{k}^{y}\\frac{b(u)}{\\sigma^{2}(u)}du\\right)}.\n\\] \\(\\blacksquare\\)\n\nLa proposición Proposition 13.1 nos da una idea de como sería (en caso de existir) la función de densidad estacionaria de la ecuación de Richard (Equation 13.6), esta quedaría determinada por, \\[\\begin{eqnarray*}\np_{0}(x) & = & \\frac{N}{\\sigma^{2}x^{2}}\\exp{\\left(2\\int_{1}^{x}\\frac{ru\\left(1-\\left(\\frac{u}{K}\\right)^{\\alpha}\\right)}{\\sigma^{2}u^{2}}\\right)}\\\\\n& = & \\frac{N}{\\sigma^{2}x^{2}}\\exp{\\left(2\\int_{1}^{x}\\left(\\frac{r}{\\sigma^{2}u}-\\frac{r}{\\sigma^{2}K}u^{\\alpha-1}\\right)du\\right)}\\\\\n& = & \\frac{N}{\\sigma^{2}x^{2}}\\exp{\\left(\\frac{2r}{\\sigma^{2}}\\left(\\ln{x}+\\frac{1}{\\alpha K}(1-x^{\\alpha})\\right)\\right)}\\\\\n& = & \\frac{N}{\\sigma^{2}}\\exp{\\left(\\frac{2r}{\\alpha K\\sigma^{2}}\\right)x^{\\frac{2r}{\\sigma^{2}}-2}}\\exp{\\left(-\\frac{2r}{\\alpha K\\sigma^{2}}x^{\\alpha}\\right)}.\n\\end{eqnarray*}\\]\nDe aqui se puede observar que, la función \\(p_{0}\\) es integrable en \\((0,\\infty)\\) si y solo si \\(\\frac{2r}{\\sigma^{2}}-2&gt;-1\\), es decir, para \\(r&gt;\\frac{1}{2}\\sigma^{2}\\). Esto significa que el proceso de difusión descrito por la ecuación de Richard (Equation 13.6) no tiene densidad estacionaria para \\(r\\leq \\frac{1}{2}\\sigma^{2}\\) y tiene una densidad estacionaria para \\(r&gt;\\frac{1}{2}\\sigma^{2}\\).\n\nPor último, veremos la simulación de la comparacion de la solución exacta con la de Euler-Maruyana y tambien el histograma de como se vería la distribución estacionaria.\nPrimero definimos las siguientes funciones auxiliares.\n\\[\\begin{align*}\nu\\left(t,X_{t}\\right) & =\\alpha X_{t}\\left[1-\\left(\\dfrac{X_{t}}{K}\\right)^{m}\\right]\\\\\nv\\left(t,X_{t}\\right) & =\\sigma X_{t}\n\\end{align*}\\]\nReescribiendo el sistema, queda\n\\[\\begin{align*}\n\\mathrm{d}X_{t} & =u\\left(t,X_{t}\\right)\\mathrm{d}t+v\\left(t,X_{t}\\right)\\mathrm{d}B_{t},t&gt;t_{0}\\\\\nX_{t_{0}} & =x_{0},\n\\end{align*}\\]\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport matplotlib.pyplot as plt\n\n\ndef log_rich(pars,t,x):\n    alpha = pars[0]\n    m = pars[1]\n    k = pars[2]\n    sigma = pars[3]\n    u = alpha * x * (1 - (x / k) ** m)\n    v = sigma * x\n    return u, v\n\ndef logistic_step(pars,t,x, dt):\n    u, v = log_rich(pars,t,x)\n    y = x + u * dt + v * np.sqrt(dt)* np.random.standard_normal()\n    return y\n\nx_0 = 20\ntf = 32\nlog_pars = [0.5, 1, 400, 0.01]\nn = 2 ** 12\n\ntime = np.linspace(0, tf, n) \ndelta_t = time[1] - time[0]\n\nx_t = [x_0]\nfor i in range(n - 1):\n    x_t.append(logistic_step(log_pars, time[i], x_t[i], delta_t))\n\ndef exact_solution(pars, x_0, t):\n    alpha = pars[0]\n    m = pars[1]\n    k = pars[2]\n    a_1 = (k / x_0) ** m - 1\n    a_2 = np.exp( -1 * m * alpha * t)\n    a_3 = 1 + a_1 * a_2\n    y = k * a_3 ** (-1 * m ** (-1))\n    return y \n\nexact = [exact_solution(log_pars, x_0, t) for t in time]\nplt.plot(time, exact, 'r')\nplt.plot(time, x_t, 'b', alpha = 0.5)\nplt.legend(['Exacta','EM'])\nplt.title('Comparativa: EM vs Determinista')\nplt.show()\n\ndef solve_logistic(pars, x_0, tf, n):\n    delta_t = tf / (n - 1)\n    x_t = [x_0]\n    for i in range(n - 1):\n        x_t.append(logistic_step(pars, 1, x_t[i], delta_t))\n    return x_t\n\n\nsamples = 1000\n\ndata = []\nfor i in range(samples):\n    y_t = solve_logistic(log_pars, x_0, tf, n)\n    for j in range(n):\n        data.append([i + 1, j, j * delta_t, y_t[j]])\n\ndistr = pd.DataFrame(data, columns=['sample','step','time','x_t'])\ndistr.head()\ndistr.to_csv(\"muestra_em_logistic.csv\")\n\nfig, ax = plt.subplots()\nmarker_style_00 = dict(\n    color=\"blue\",\n    linestyle=\"-\",\n    # marker=\"\",\n    markersize=1,\n    markerfacecoloralt=\"gray\",\n    alpha=0.1,\n)\nx_T = []\nfor j in range(samples):\n    ax.plot(distr.iloc[j *n: (j+1) * n, 2], distr.iloc[j *n: (j+1) * n, 3], **marker_style_00, label=r\"$X(t_i)$\")\n    x_T.append(distr.iloc[j *n: (j+1) * n -1, 3])\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$X(t)$\")\n\nx_T = np.array(x_T)\nymax = np.max(np.abs(x_T))\nbinwidth = 0.025\nlim = (int(ymax / binwidth) + 1) * binwidth\nbins = 100  \ndivider = make_axes_locatable(ax)\nax_histy = divider.append_axes(\"right\", 1.2, pad=0.1, sharey=ax)\nn, bins, patches = ax_histy.hist(x_T, bins=bins, orientation=\"horizontal\", density=True)\nsigma = np.std(x_T)\nmu = np.mean(x_T)\ny = (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(\n    -0.5 * ((1.0 / sigma) * (bins - mu)) ** 2\n)\n\nplt.savefig(\"gen_log_sde_batch_sample_path.png\", dpi=300)\nplt.show()\ng = grid = sns.JointGrid(data=distr, x=\"time\", y=\"x_t\")\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.kdeplot)\ng.plot_marginals(sns.histplot)\ng.ax_marg_x.remove()\ng.savefig(\"marginal.png\")"
  },
  {
    "objectID": "ProyectoFinal.html#referencias",
    "href": "ProyectoFinal.html#referencias",
    "title": "13  Proyecto Final",
    "section": "13.4 Referencias:",
    "text": "13.4 Referencias:\n[1] Analytic Solution of a Stochastic Richards Equation driven by Brownian motion, H P Suryawan, 2018.\n[2] H.-H. Kuo, Introduction to stochastic integration, Springer, New York, 2006.\n[3] Oksendal B, 2003, Stochastic Differential Equations, sixth edition (Heidelberg: Springer)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]