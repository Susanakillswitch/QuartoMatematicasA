[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matematicas aplicadas",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "3  Tarea 1",
    "section": "",
    "text": "Exercise 3.1 Se generan variables aleatorias Bernoulli y el histograma de los valores que toma con paremetro \\(p=0.3\\).\n\n\n\nExploring functions to generate random variables with a Bernoulli distribution.py\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\nfig_01, ax_01 = plt.subplots(1, 1)\nfig_02, ax_02 = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(mean, var, skew,kurt)\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax_01.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax_01.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\nr = bernoulli.rvs(p, size=1000)\nax_02.hist(r, bins=200)\nplt.show()\n\n \n\nExercise 3.2 Se generan variables aleatorias normales y el histograma de los valores que toma.\n\n\n\nExploring functions to generate random variables with a Gaussian distribution.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1, 1)\nmean, var, skew, kurt = norm.stats(moments='mvsk')\n\nx = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\nax.plot(\n    x,\n    norm.pdf(x),\n    'r-',\n    lw=5,\n    alpha=0.6,\n    label='norm pdf'\n)\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nvals = norm.ppf([0.001, 0.5, 0.999])\n\nnp.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n\nr = norm.rvs(size=50000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\nax.set_xlim([x[0], x[-1]])\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\nFigura 3\n\n\n\nExercise 3.3 Modificando reproducir el gráfico de una distribución gaussiana bivariada con media vectorial \\(\\mu[0.1, 0.5]\\) y matriz de covarianza \\[\\Sigma=\n\\begin{bmatrix}\n3.0 & 0.3\\\\\n0.75 & 1.5\n\\end{bmatrix}\n\\]\n\n\n\nRevising multivariate Gaussian.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nfrom scipy.stats import multivariate_normal\n\nx = np.linspace(0, 5, 100, endpoint=False)\ny = multivariate_normal.pdf(x, mean=2.5, cov=0.5);\n\nfig1 = plt.figure()\nax = fig1.add_subplot(111)\nax.plot(x, y)\n# plt.show()\n\nx, y = np.mgrid[-5:5:.1, -5:5:.1]\npos = np.dstack((x, y))\nrv = multivariate_normal([0.1, 0.5], [[3.0, 0.3], [0.75, 1.5]])\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nax2.contourf(x, y, rv.pdf(pos))\n# plt.show()\n\nax = plt.figure().add_subplot(projection='3d')\nax.plot_surface(\n    x,\n    y,\n    rv.pdf(pos),\n    edgecolor='royalblue',\n    lw=0.5,\n    rstride=8,\n    cstride=8,\n    alpha=0.4\n)\nax.contour(x, y, rv.pdf(pos), zdir='z', offset=-.2, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='x', offset=-5, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='y', offset=5, cmap='coolwarm')\n\nax.set(\n    xlim=(-5, 5),\n    ylim=(-5, 5),\n    zlim=(-0.2, 0.2),\n    xlabel='X',\n    ylabel='Y',\n    zlabel='Z'\n)\nplt.show()"
  },
  {
    "objectID": "Tarea2.html#demostración",
    "href": "Tarea2.html#demostración",
    "title": "4  Tarea 2",
    "section": "Demostración:",
    "text": "Demostración:\nConsidere una caminata aleatoria que comienza en 0 con saltos \\(h\\) y \\(-h\\) igualmente probables en los momentos \\(\\delta\\), 2 \\(\\delta\\),\\(\\dots\\), donde \\(h\\) y \\(\\delta\\) son números positivos. Más precisamente, sea \\(\\{X_{n}\\}_{n=1}^{\\infty}\\) una sucesión de elementos aleatorios independientes e idénticamente distribuidos. variables con \\[\nP\\left[X_{i}=h\\right]=P\\left[X_{i}=-h\\right]=\\dfrac{1}{2},\\forall i,\n\\]\nSea \\(Y_{\\delta,h}(0)=0\\) y pongamos \\[\nY_{\\delta,h}(n\\delta)=X_{1}+X_{2}+\\cdots+X_{n}.\n\\]\nPara \\(t&gt;0\\), defina \\(Y_{\\delta,h}(t)\\) mediante linealización, es decir, para \\(n\\delta&lt;t&lt;(n + 1)\\delta\\), defina \\[\nY_{\\delta,h}(t)=\\frac{(n+1)\\delta-t}{\\delta}Y_{\\delta,h}(n\\delta)+\\frac{t-n\\delta}{\\delta}Y_{\\delta,h}((n+1)\\delta).\n\\]\nCalculemos la función característica de \\(Y_{\\delta,h}(t)\\), donde \\(\\lambda\\in\\mathbb{R}\\) fijo y sea \\(t=n\\delta\\) así, \\(n=t/\\delta\\). Entonces se tiene que\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right] & = & \\prod_{j=1}^{n}Ee^{i\\lambda X_{j}},\\text{ por ser variables independientes,}\\\\\n    & = & (Ee^{i\\lambda X_{j}})^{n},\\text{ por ser idénticamente distribuidas,}\\\\\n    & = & \\frac{1}{2}(e^{i\\lambda h}+e^{-i\\lambda h})^{n},\\\\\n    & = & (\\cos(\\lambda h))^{n},\\\\\n    & = & (\\cos(\\lambda h))^{t/\\delta},\n\\end{eqnarray}\\] \\[\n\\\n\\tag{4.1}\\]\nPor otro lado, sea \\(u=\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\Rightarrow\\ln\\left(u\\right)=\\dfrac{1}{\\delta}\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]\\).\nUsando la expansión de Taylor de \\(\\cos\\left(x\\right)\\) se tiene que \\[\n\\cos\\left(\\lambda h\\right)\\approx1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!},\n\\]\nentonces\n\\[\\begin{eqnarray}\n    \\ln\\left(\\cos\\left(\\lambda h\\right)\\right) & \\approx & \\ln\\left[1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right]\\\\\n& \\approx & -\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}-\\frac{1}{2}\\left(-\\frac{\\lambda^{2}h^{2}}{2!}+\\frac{\\lambda^{4}h^{4}}{4!}\\right)^{2}\\\\\n  & = & -\\dfrac{\\lambda^{2} h^{2}}{2!}+\\dfrac{\\lambda ^{4}h^{4}}{4!}-\\frac{1}{2}\\left(\\frac{\\lambda^{4}h^{4}}{4}-\\frac{\\lambda^{6}h^{6}}{24^{2}}+\\frac{\\lambda^{8}h^{8}}{24}\\right)\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}+\\dfrac{\\lambda^{4} h^{4}}{24}-\\frac{\\lambda^{4}h^{4}}{8}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\\\\n   & = & -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\n\\end{eqnarray}\\]\npara una \\(h\\) pequeña, se satisface que, \\[\n-\\frac{\\lambda^{6}h^{6}}{(2)24^{2}}+\\frac{\\lambda^{8}h^{8}}{48}\\approx 0\n\\]\nPor lo tanto, \\(\\ln\\left(\\cos\\left(\\lambda h\\right)\\right)\\approx -\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\).\\ Así, para \\(\\delta\\) y \\(h\\) pequeña, se tiene que \\(\\ln u\\approx \\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\).\\ Entonces\n\\[\\begin{equation}\n    u\\approx\\exp\\left[\\dfrac{1}{\\delta}\\left(-\\dfrac{\\lambda^{2} h^{2}}{2}-\\dfrac{\\lambda^{4} h^{4}}{12}\\right)\\right]\n\\end{equation}\\]\nEntonces por la ecuación (Equation 4.1),\n\\[\\begin{equation}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\approx\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\n\\end{equation}\\]\nCalculando el limite \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left[-t\\left(\\left[\\dfrac{h^{2}}{\\delta}\\right]\\left(\\dfrac{\\lambda^{2}}{2}-\\dfrac{\\lambda^{4}h^{2}}{24}\\right)\\right)\\right],\n\\]\nAsumamos que \\(\\delta\\to0\\), \\(h\\to0\\) pero \\(h^{2}/\\delta\\to\\infty\\). Entonces \\(\\lim_{\\delta\\to0} Y_{\\delta, h}(t)\\) no existe. Por otro lado, consideremos la siguiente renormalización,\n\\[\\begin{eqnarray}\n    E\\exp\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right] & = & E\\left[\\exp (i\\lambda Y_{n,\\delta}\\left(t\\right))\\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\right]\\\\\n    & = & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)E\\exp\\left[ i\\lambda Y_{n,\\delta}\\left(t\\right)\\right]\\\\\n    & \\approx & \\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\exp\\left[-\\dfrac{t\\lambda^{2} h^{2}}{2\\delta}-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right]\\\\\n    & = & \\exp\\left(-\\dfrac{t\\lambda^{4} h^{4}}{12\\delta}\\right)\n\\end{eqnarray}\\]\nAsí, si \\(\\delta,h\\to0\\) de tal manera que \\(h^{2}/\\delta\\to\\infty\\) y \\(h^{4}/\\delta\\to0\\), entonces \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2}\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left(\\dfrac{\\left(\\lambda h\\right)^{4}}{24\\delta}\\right)=1\n\\]"
  },
  {
    "objectID": "Tarea3.html",
    "href": "Tarea3.html",
    "title": "5  Tarea 3",
    "section": "",
    "text": "Exercise 5.1 Si \\(X\\thicksim N(\\mu,\\sigma^{2})\\) entonces \\(\\left(\\dfrac{X-\\mu}{\\sigma}\\right)\\thicksim N(0,1)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\dfrac{X-\\mu}{\\sigma}\\),\n\\[\\begin{eqnarray}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & E\\left[e^{it\\left(\\frac{X-\\mu}{\\sigma}\\right)}\\right]\\\\\n& = & E\\left[e^{\\left(\\frac{itX}{\\sigma}-\\frac{it\\mu}{\\sigma}\\right)}\\right]\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}E\\left[e^{\\left(\\frac{itX}{\\sigma}\\right)}\\right]\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}}}dx\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.1}\\]\nObservemos que,\n\\[\\begin{eqnarray}\\label{1.2}\n\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}} & = & \\frac{x^{2}-2x\\mu+\\mu^{2}-2itx\\sigma}{\\sigma^{2}}\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x\\mu}{\\sigma^{2}}+\\frac{\\mu^{2}}{\\sigma^{2}}-\\frac{2itx\\sigma}{\\sigma^{2}}\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x}{\\sigma}\\left(\\frac{\\mu+it\\sigma}{\\sigma^{2}}\\right)+\\frac{\\mu^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)^{2}+\\frac{\\mu^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\sigma\\mu}{\\sigma^{2}}-\\frac{(it\\sigma)^{2}}{\\sigma^{2}}\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.2}\\]\nSustituyendo (Equation 5.2) en (Equation 5.1), resulta\n\\[\\begin{eqnarray}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left[\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}\\right]}dx\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}e^{\\frac{it\\mu}{\\sigma}-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\\\\\n& = & e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.3}\\]\nSea \\(u=\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\Longrightarrow du=\\frac{1}{\\sigma}dx\\), sustituyendo esto en (Equation 5.3), resulta\n\\[\\begin{equation}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) = e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}du\n\\end{equation}\\] \\[\n\\\n\\tag{5.4}\\]\nde aquí se sigue que \\(u\\thicksim N(0,1)\\), entonces \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}dx=1.\n\\] sustituyendo esto ultimo en (Equation 5.4), se tiene,\n\\[\\begin{equation}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) =e^{-\\frac{t^{2}}{2}},\n\\end{equation}\\] \\[\n\\\n\\tag{5.5}\\] Por otro lado, consideremos \\(Z\\thicksim N(0,1)\\), entonces\n\\[\\begin{equation*}\n\\varphi_{Z}(t) =e^{-\\frac{t^{2}}{2}}.\n\\end{equation*}\\]\nEntonces \\(\\varphi_{Z}(t)=\\varphi_{\\frac{X-\\mu}{\\sigma}}(t)\\), como las funciones características coinciden se concluye que \\(\\frac{X-\\mu}{\\sigma}\\thicksim N(0,1)\\).\n\n\nExercise 5.2 Si \\(Y\\thicksim N(0,1)\\) entonces \\(\\sigma Y+\\mu \\thicksim N(\\mu,\\sigma)\\).\n\n\nProof. Calculemos la función característica de la variable \\(\\sigma Y+\\mu\\),\n\\[\\begin{eqnarray}\n\\varphi_{\\sigma Y+\\mu}(t) & = & E\\left[e^{it(\\sigma Y+\\mu)}\\right]\\\\\n& = & E\\left[e^{it\\sigma Y+it\\mu}\\right]\\\\\n& = & e^{it\\mu}E\\left[e^{it\\sigma Y}\\right]\\\\\n& = & e^{it\\mu}\\int_{-\\infty}^{\\infty}e^{it\\sigma y}\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-y^{2}}{2}}dy\\\\\n& = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y^{2}-2yit\\sigma) }dy.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.6}\\]\nObservemos que,\n\\[\\begin{eqnarray}\ny^{2}-2yit\\sigma & = & (y-it\\sigma)^{2}-(it\\sigma)^{2}\\\\\n& = & (y-it\\sigma)^{2}+t^{2}\\sigma^{2}.\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.7}\\]\nSustituyendo, (Equation 5.7) en (Equation 5.6) resulta\n\\[\\begin{eqnarray}\n\\varphi_{\\sigma Y+\\mu}(t) & = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}((y-it\\sigma)^{2}+t^{2}\\sigma^{2}) }dy\\\\\n& = & e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.8}\\]\nTomando \\(u=y-it\\sigma\\Longrightarrow du=dy\\), se tiene que \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2} }{2}}du,\n\\] entonces \\(U\\thicksim N(0,1)\\), por lo tanto, \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=1\n\\]\nsustituyendo esto ultimo en (Equation 5.8), resulta, \\[\n\\varphi_{\\sigma Y+\\mu}(t)=e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] Sea \\(Z\\) una variable aleatoria tal que \\(Z\\thicksim N(\\mu,\\sigma)\\) sabemos que, \\[\n\\varphi_{Z}(t)=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\] De estas dos ultimas igualdades se sigue que, \\[\n\\varphi_{Z}(t)=\\varphi_{\\sigma Y+\\mu}(t).\n\\] Dado que tienen iguales funciones características se concluye que, \\[\n\\sigma Y+\\mu\\thicksim N(\\mu,\\sigma)\n\\]\n\n\nExercise 5.3 Si \\(X\\thicksim N(\\mu_{1},\\sigma_{1}^{2})\\), \\(Y\\thicksim N(\\mu_{2},\\sigma_{2}^{2})\\) además \\(X\\) y \\(Y\\) son independientes entonces \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nProof. Por definición, se tiene que,\n\\[\\begin{eqnarray}\n\\varphi_{X+Y}(t) & = & E[e^{it(X+Y)}]\\\\\n& = & E[e^{itX}e^{itY}]\\text{ por ser independientes, del ejercicio 4}\\\\\n& = & E[e^{itX}]E[e^{itY}]\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t).\n\\end{eqnarray}\\] \\[\n\\\n\\tag{5.9}\\]\nPor otro lado, sea \\(Z\\) una variables aleatoria tal que, \\(Z\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\), sabemos que la función característica de \\(Z\\), esta dada por,\n\\[\\begin{eqnarray*}\n\\varphi_{Z}(t) & = & e^{it(\\mu_{1}+\\mu_{2})-\\frac{t^{2}}{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}+it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}}e^{it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t),\n\\end{eqnarray*}\\]\nentonces, de esta ultima igualdad y de (Equation 5.9) se sigue que, \\[\n\\varphi_{Z}(t)= \\varphi_{X+Y}(t).\n\\]\nComo las funciones características coinciden se sigue que, \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\nExercise 5.4 (Ejercicio 4:) Si \\(X\\), \\(Y\\) son variables aleatorias normales entonces \\(X\\), \\(Y\\) son independientes si y solo si \\(E(XY)=E(X)E(Y)\\).\n\n\nProof. Primero recordemos que \\[\nE\\left(XY\\right)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\n\\]\nComo \\(X,Y\\) son independientes, sabemos que \\[\nf_{XY}\\left(x,y\\right)=f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\n\\]\nEntonces\n\\[\\begin{eqnarray}\nE\\left(XY\\right) & = & \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& = & \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{X}\\left(x\\right)f_{y}\\left(y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& = & \\left(\\int_{-\\infty}^{\\infty}xf_{X}\\left(x\\right)\\mathrm{d}x\\right)\\left(\\int_{-\\infty}^{\\infty}yf_{y}\\left(y\\right)\\mathrm{d}y\\right)\\\\\n& = & E\\left(X\\right)E\\left(Y\\right)\n\\end{eqnarray}\\]\n\n\nTheorem 5.1 (Desigualdad de Chebyshev) Sea \\(X\\) una variable aleatoria con esperanza \\(\\mu=E(X)\\) y sea \\(\\varepsilon&gt;0\\). Entonces \\[\nP(|X-\\mu|\\geq\\varepsilon)\\leq\\frac{Var(X)}{\\varepsilon^{2}}\n\\]\n\n\nProof. Sea \\(Y=\\left|X-\\mu\\right|\\), observemos que \\(Y\\) es positiva, así por la desigualdad de Markov y dado que \\(\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] =\\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\), se cumple que\n\\[\\begin{eqnarray}\n\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] & = & \\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\\\\n& \\leq & \\dfrac{E\\left[\\left(X-\\mu\\right)^{2}\\right]}{\\epsilon^{2}}=\\dfrac{\\text{Var}\\left[X\\right]}{\\epsilon^{2}}\n\\end{eqnarray}\\]\n\n\nTheorem 5.2 (Ley de los grandes números) Sean \\(X_{1},X_{2},\\dots, X_{n}\\) procesos de ensayos independientes, con esperanza finita \\(\\mu=E(X_{j})\\) y varianza finita \\(\\sigma^{2}=Var(X_{j})\\). Sean \\(S_{n}=X_{1}+X_{2}+\\ldots+X_{n}\\). Entonces para cada \\(\\epsilon&gt;0\\).\n\\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nProof. Observemos que\n\\[\\begin{eqnarray}\n\\text{Var}\\left[\\dfrac{S_{n}}{n}-\\mu\\right] & = & \\dfrac{1}{n^{2}}\\text{Var}\\left(S_{n}\\right)\\\\\n& = & \\dfrac{1}{n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(X_{i}\\right),\\text{ por ser iid}\\\\\n& = & \\dfrac{\\sigma^{2}}{n}\n\\end{eqnarray}\\]\nEntonces, por el Teorema 5.1, \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\leq\\dfrac{\\sigma^{2}}{n\\epsilon},\n\\] así, tomando el limite cuando \\(n\\to\\infty\\) \\[\n\\dfrac{\\sigma^{2}}{n\\epsilon}\\to0.\n\\]\nEntonces \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\nTheorem 5.3 (Teorema del Limite Central) Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una secuencia de v.a.i.id con media \\(a\\) y varianza \\(b^{2}\\). Entonces para doo \\(\\alpha,\\beta\\in\\mathbb{R}\\), con \\(\\alpha&lt;\\beta\\), entonces \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right)=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}e^{\\left(-\\dfrac{1}{2}x^{2}\\right)}\\mathrm{d}x\n\\]\n\n\nProof. Definamos a \\[\nS_{M}={\\displaystyle \\sum_{i=1}^{M}}\\left[X_{i}-a\\right],\n\\] y \\[\nY_{M}=\\dfrac{S_{M}}{\\sqrt{M}b}.\n\\] Sea \\(\\varphi_{Y_{M}}\\) la función generadora de momentos de \\(Y_{M}\\) y \\(\\varphi\\) la función generadora de momentos de la distribución normal estándar, demostraremos que \\(\\varphi_{Y_{M}}\\to\\varphi\\).\nPor definición,\n\\[\\begin{eqnarray}\n\\varphi_{Y_{M}}\\left(t\\right) & = & E\\left[\\exp\\left(t\\dfrac{S_{M}}{\\sqrt{Mb}}\\right)\\right]\\\\\n& = & \\varphi_{S_M}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\\\\n& = & \\left[\\varphi_{\\left(X_{1}-a\\right)}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\right]^{M} \\text{ ya que, las }X_{i}\\text{ son i.i.d}\\\\\n& = & \\left[E\\left[\\exp\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)\\right]\\right]^{M}\n\\end{eqnarray}\\]\nRecordando la serie de Taylor\n\\[\\begin{eqnarray}\n\\varphi_{Y_M}\\left(t\\right) & = & \\left[\\sum_{i=0}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!}\\right]^{M}\\\\\n& = & \\left[1+\\dfrac{1}{2}\\left(\\dfrac{t}{b\\sqrt{M}}\\right)^{2}E\\left[\\left(X_{1}-a\\right)^{2}\\right]+\\epsilon\\left(3\\right)\\right]^{M}\\\\\n& = & \\left[1+\\dfrac{1}{M}\\dfrac{t^{2}}{2}+\\epsilon\\left(3\\right)\\right]^{M},\n\\end{eqnarray}\\]\ndonde\n\\[\\begin{eqnarray}\n\\epsilon\\left(3\\right) & = &\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!},\n\\end{eqnarray}\\]\nAhora sea \\(s=\\dfrac{t}{b\\sqrt{M}},\\) así,\n\\[\n\\epsilon\\left(3\\right)=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i}}{i!}\n\\] Además observemos que, cuando \\(t\\to0\\), \\(s\\to0\\).\nAsí, de lo anterior, si \\(\\varphi_{1}\\) existe, se cumple que, \\[\n\\dfrac{\\epsilon\\left(3\\right)}{s^{2}}=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i-2}}{i!}\\to0,\\text{ cuando, }s\\to0.\n\\]\nPor otro lado,\n\\[\n\\varphi_{Y_M}\\left(t\\right)=\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M},\n\\] y \\(s\\to0\\) cuando \\(M\\to\\infty\\).\nEntonces \\(\\epsilon\\left(3\\right)s^{-2}=M\\epsilon\\left(3\\right)b^{2}t^{-2}\\to0\\). Dado que \\(b,t\\) estan fijas, se cumple que \\[\nM\\epsilon\\left(3\\right)\\to0,\\text{ cuando, }M\\to\\infty,\n\\]\npor lo tanto \\[\n\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right) \\to\\dfrac{t^{2}}{2},\\text{ cuando, }M\\to\\infty\n\\] esto implica que,\n\\[\n\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M} \\to\\exp\\left(t^{2}/2\\right),M\\to\\infty\n\\]\nDe aqui se concluye que, \\[\n\\lim_{M\\to\\infty}\\varphi_{M}\\left(t\\right)  =\\exp\\left(t^{2}/2\\right)=\\varphi\\left(t\\right)\n\\]\nla cual es la función generadora de momentos de la distribución normal estándar. Por lo tanto \\[\nF_{M}\\left(x\\right)\\to F_{N\\left(0,1\\right)}\\left(x\\right)\n\\] que es equivalente a,\n\\[\nF_{M}\\left(b\\right)-F_{M}\\left(a\\right)  \\to F_{N}\\left(b\\right)-F_{N}\\left(a\\right)\n\\] \\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right) =\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}\\exp\\left(-\\dfrac{1}{2}x^{2}\\right)\\mathrm{d}x\n\\]\n\n\nTheorem 5.4 Sea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una sucesión de v.a.i.i.d con media \\(a\\). Entonces \\[\n\\mathcal{P}\\left[\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}=a\\right]=1.\n\\]\n\n\nProof. Esto es similar a decir que \\[\n\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\stackrel{\\text{c.s}}{=}a\n\\]\nSin perdida de generalidad, diremos que \\(X_{i}\\geq0,\\forall i\\). Definamos \\[\nY_{n}=X_{n}I_{\\left[\\left|X_{n}\\right|\\leq n\\right]},Q_{n}=\\sum_{i=1}^{n}Y_{i}\n\\]\nPor la desigualdad de Chebyshev\n\\[\\begin{eqnarray}\n\\sum_{n=1}^{\\infty}\\mathcal{P}\\left[\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right] & \\leq & \\sum_{n=1}^{\\infty}\\dfrac{\\text{Var}\\left(Q_{n}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(Y_{i}\\right)\\\\\n& \\leq & \\sum_{n=1}^{\\infty}\\dfrac{E\\left(Y_{n}^{2}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\int_{0}^{n}x^{2}\\mathrm{d}F\\\\\n& \\leq & \\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}}\\int_{0}^{n}x\\mathrm{d}F&lt;\\infty,\n\\end{eqnarray}\\]\ndonde \\(F\\) es la función de distribución de \\(X_{i}\\). Luego \\[\nE\\left[X_{1}\\right]=\\lim_{n\\to\\infty}\\int_{0}^{n}x\\mathrm{d}F=\\lim_{n\\to\\infty}E\\left[Y_{n}\\right]=\\lim_{n\\to\\infty}\\dfrac{E\\left[Q_{n}\\right]}{n}.\n\\]\nEntonces, por el Lema de Borel Canteli. \\(\\mathcal{\\mathcal{P}}\\left[\\limsup\\left(\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right)\\right]=0\\)\n\\[\n\\lim_{n\\to\\infty}\\dfrac{Q_{n}}{n}=E\\left[X_{1}\\right],\\text{c.s}\n\\]\nAhora, calcularemos la siguiente probabilidad \\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}\\neq Y_{i}\\right]=\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\n\\]\ncomo \\(E\\left[X_{i}\\right]&lt;\\infty\\) y \\(X_{i}\\) son v.a.i.i.d.\n\\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\\leq E\\left[X_{1}\\right]&lt;\\infty\n\\]\nDe nuevo, por el Lema de Borel Cantelli. \\[\n\\mathcal{P}\\left[\\limsup\\left[X_{i}\\neq Y_{i}\\right]\\right]=0,\\forall i\n\\]\nEntonces\n\\[\\begin{eqnarray}\nX_{i} & = & Y_{i},\\text{c.s}\\\\\n& \\Rightarrow & \\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\to E\\left[X_{1}\\right]=\\mu.\\text{ c.s}\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "Tarea4.html",
    "href": "Tarea4.html",
    "title": "6  Tarea 4",
    "section": "",
    "text": "Exercise 6.1 Sea \\(W(t)\\) un movimiento Browniano estándar en \\([0,T]\\). Pruebe que para cualquier \\(c&gt;0\\) fijo, \\[\nV(t) = \\dfrac{1}{c} W(c^2 t)\n\\]\nes un movimiento Browniano sobre \\([0,T]\\).\n\n\nProof. \nVeamos que \\(V\\) cumple las propiedades del movimiento Browniano. \nPropiedad C1 (Que comience en 0).\nSe tiene que, \\(V(0) = \\dfrac{1}{c} W (c^2\\cdot0)=0\\).\n\nPropiedad C2 (Incrementos Independientes).\nSean \\(s&lt;t&lt;u&lt;v\\), por definición de \\(V\\), se tiene que, \\[\nE[\\left(V(t)-V(s)\\right)\\left(V(v)-V(u)\\right)]=\\dfrac{1}{c^2}E[\\left(W(c^2 t)-W(c^2 s)\\right)\\left(W(c^2 v)-W(c^2 u)\\right)]\n\\]\nDado que \\(W\\) tiene incrementos independientes, se cumple que,\n\\[\\begin{eqnarray}\n\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right] & = &\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\right]E\\left[\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right]\n\\end{eqnarray}\\]\nEntonces \\(V\\) tiene incrementos independientes.\n\nPropiedad C3 (Incrementos estacionarios).\nSea \\(s&lt;t\\). \\[\nV(t)-V(s)=\\dfrac{1}{c}\\left[W(c^2 t) - W(c^2 s)\\right]\n\\]\nPor las propiedades de la definicion del movimiento Browniano.\n\\[\\begin{eqnarray}\nE\\left[V(t)-V(s)\\right] & = & \\dfrac{1}{c}E\\left[W(c^{2}t)-W(c^{2}s)\\right]=0\\\\\n\\text{Var}\\left[V(t)-V(s)\\right] & = & \\dfrac{1}{c^{2}}\\text{Var}\\left[W(c^{2}t)-W(c^{2}s)\\right]=\\dfrac{1}{c^{2}}\\left(c^{2}\\left(t-s\\right)\\right)=t-s\n\\end{eqnarray}\\]\nEntonces \\(V\\) tiene incrementos estacionarios.\nCon todo lo anterior se concluye que, \\(V\\) es un movimiento browniano.\n\n\nExercise 6.2 Hacer un script para ilustrar la propiedad de escalado del movimiento Browniano para el caso de \\(c = \\dfrac{1}{5}\\). Estar seguro que usa el mismo camino browniano discretizado en cada subplot.\n\n\n\nBrowniano escalado, con c=1/5.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\nT = 1  \nn= 100  \ndt = 1 / (n - 1)\ndw = np.sqrt(dt) * prng.standard_normal(n - 1) \nw = np.concatenate(([0],dw.cumsum()))\n\ntime = np.linspace(0,T, n)\nc = 0.2  # 1/5\nc_time = c**2 * time  \nc_w = c**(-1) * w  \n\nfig, browniano_escalado = plt.subplots(2)\nbrowniano_escalado[0].plot(time, w)\nbrowniano_escalado[1].plot(c_time, c_w)\nbrowniano_escalado[0].set_title('Movimiento browniano')\nbrowniano_escalado[1].set_title('Moviemiento browniano escalado')\nplt.show()\n\n\n\n\nFigura 1\n\n\n\nExercise 6.3 Modifique el script half_brownian_refinement.py encapsulando el código en una función. Esta función deberá recibir el extremo derecho del intervalo \\([0, T]\\) y el número de incrementos \\(N\\) de un camino browniano base. El propósito es calcular los incrementos de relleno de una refinamiento con \\(2N\\) incrementos.\n\n\n\nBrowniano refinado, con refinamiento 2N.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(123456789)\n\ndef refined_brownian_2n(T,L):\n    dt = T / L\n    W = np.zeros(L + 1)\n    W_refined = np.zeros(2 * L + 1)\n    xi = np.sqrt(dt) * prng.normal(size=L)\n    xi_half = np.sqrt(0.5 * dt) * prng.normal(size=L)\n    W[1:] = xi.cumsum()\n    W_ = np.roll(W, -1)\n    W_half = 0.5 * (W + W_)\n    W_half = np.delete(W_half, -1) + xi_half\n    W_refined[1::2] = W_half\n    W_refined[2::2] = W[1:]\n    t = np.arange(0, T + dt, dt)\n    t_half = np.arange(0, T + 0.5 * dt, 0.5 * dt)\n    return t,t_half,W, W_refined\n\n\nExercise 6.4 En un script separado, incluya la función de arriba y grafique una figura con la trayectoria del browniano con 100 incrementos y muestre su refinamiento correspondiente.\n\n\n\nBrowniano refinado, con refinamiento 2N y 100 incrementos.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h_b_r as hbr\n\na, b, c, d = hbr.refined_brownian_2n(1, 100)\n\nplt.plot(a, c, 'r-+')\nplt.plot(\n    b,\n    d,\n    'g*--',\n    # alpha = transparecia\n    )\nplt.show()\n\n\n\n\nFigura 2"
  },
  {
    "objectID": "Tarea5.html",
    "href": "Tarea5.html",
    "title": "7  Tarea 5",
    "section": "",
    "text": "Exercise 7.1 Demuestre que el movimiento browniano satisface \\[\nE[|W(t)-W(s)|^{2}]=|t-s|.\n\\]\n\n\nProof. Consideremos dos casos:\nSi \\(t&gt;s\\).\n\\[\\begin{align*}\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right] & =E\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right]\\\\\n& =t-s,\n\\end{align*}\\]\nya que, \\(W(t)-W(s)\\thicksim N(0,t-s)\\).\nMientras que si \\(t\\leq s\\).\n\\[\\begin{align*}\nE\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right] & =E\\left[\\left(W\\left(s\\right)-W\\left(t\\right)\\right)^{2}\\right]\\\\\n& =s-t,\n\\end{align*}\\]\npor lo tanto \\[\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right]=\\left|t-s\\right|\n\\]\n\n\nExercise 7.2 Dados \\(W(t_{i})\\) y \\(W(t_{i+1})\\), demuestre que la variable aleatoria \\[\nW(t_{i+\\frac{1}{2}}):=\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+\\frac{1}{2}\\sqrt{\\delta t\\xi},\\quad\\xi\\thicksim N(0,1)\n\\] satisface las tres condiciones C1, C2, C3 de la definicion de movimiento Browniano.\n\n\nProof. (C1) Veamos que \\(W\\left(0\\right)=0\\), cuando \\(t=0\\).\nSe tiene por definicion del proceso que, \\[\nW\\left(0\\right)=\\frac{1}{2}(W(0)+W(0))+\\frac{1}{2}\\sqrt{\\delta(0)\\xi}=0.\n\\] Por la propiedad C1 se satisface.\n\n(C2) Que tenga incrementos estacionarios.\nNotemos que\n\\[\\begin{align*}\nW(t_{i+\\frac{1}{2}})-W(t_{i}) & = \\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)+W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi-\\frac{1}{2}(W(t_{i})+W(t_{i}))\\\\\n& = \\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi,\n\\end{align*}\\]\nEntonces\n\\[\\begin{eqnarray*}\nE\\left[W(t_{i+\\frac{1}{2}})-W(t_{i})\\right] & = & E\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]\\right]+E\\left[\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & \\dfrac{1}{2}E\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & 0\\quad\\text{ya que, }E\\left[\\xi\\right]=0\\text{ y }E\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]=0.\n\\end{eqnarray*}\\]\ny\n\\[\\begin{eqnarray}\nVar\\left[W(t_{i+\\frac{1}{2}})-W(t_{i})\\right]& = & Var\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & Var\\left[\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]\\right]+Var\\left[\\dfrac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\\\\n& = & \\dfrac{1}{4}Var\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{4}\\delta t Var\\left[\\xi\\right]\\\\\n& = & \\dfrac{1}{4}\\delta t+\\dfrac{1}{4}\\delta t\\quad\\text{ya que, }Var\\left[\\xi\\right]=1\\text{ y }Var\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]=\\delta t\\\\\n& = & \\dfrac{1}{2}\\delta t.\n\\end{eqnarray}\\]\nAdemás, sabemos que la combinación lineal de normales es una nornal.\nPor lo tanto \\(W(t_{i+\\frac{i}{2}})-W(t_{i})\\sim N\\left(0,\\dfrac{\\delta t}{2}\\right)\\), con esto C2 se cumple.\n\n(C3) Que tenga incrementos independientes.\nPara esta parte usaremos que dos variables aleatorias \\(X\\) y \\(Y\\) son independientes si y solo si \\[\nE(XY)=E(X)E(Y)\n\\]\ncalculemos \\(E\\left[\\left(W\\left(t_{i+1}\\right)-W\\left(t_{i+\\frac{1}{2}}\\right)\\right)\\left(W(t_{j+1})-W\\left(t_{j+\\frac{1}{2}}\\right)\\right)\\right]\\) y definamos a \\(\\Delta W(t_{i}):=W(t_{i+1})-W(t_{i})\\).\nPor lo anterior se tiene que:\n\\[\\begin{eqnarray*}\n    E\\left[\\left(\\Delta W\\left(t_{i+\\frac{1}{2}}\\right)\\right)\\left(\\Delta W\\left(t_{j+\\frac{1}{2}}\\right)\\right)\\right]& = & E\\left[\\left(\\dfrac{1}{2}\\Delta W\\left(t_{i}\\right)+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)\\left(\\dfrac{1}{2}\\Delta W\\left(t_{j}\\right)+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)\\right],\n\\end{eqnarray*}\\]\ndonde \\(\\Delta W\\left(t_{i+\\frac{1}{2}}\\right)=W(t_{i+1})-W\\left(t_{i+\\frac{1}{2}}\\right)\\) y \\(\\Delta W\\left(t_{j+\\frac{1}{2}}\\right)=W(t_{j+1})-W\\left(t_{j+\\frac{1}{2}}\\right)\\).\\ Desarrollando la parte derecha de la igualdad anterior, resulta\n\\[\\begin{eqnarray*}\nE\\left[\\left(\\Delta W(t_{i+\\frac{1}{2}})\\right)\\left(\\Delta W(t_{j+\\frac{1}{2}})\\right)\\right]& = & E\\left[\\dfrac{1}{4}\\Delta W(t_{i})\\Delta W(t_{j})+\\dfrac{1}{4}\\Delta W(t_{i})\\sqrt{\\delta t}\\xi\\right.\\\\\n& & \\left.+\\dfrac{1}{4}\\Delta W(t_{j})\\sqrt{\\delta t}\\xi+\\left(\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right)^{2}\\right]\\\\\n\\text{ya que, }\\Delta W(t_{i}),\\Delta W(t_{j})\\text{ son independientes} & = & \\dfrac{1}{4}E\\left[\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j})\\right]+\\dfrac{1}{4}E\\left[\\Delta W(t_{i})\\right]\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{1}{4}E\\left[\\Delta W(t_{j})\\right]\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n  & = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\dfrac{1}{2}\\Delta W(t_{j})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]+E\\left[\\dfrac{1}{2}\\Delta W(t_{j})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n  & = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]+E\\left[\\dfrac{1}{2}\\Delta W(t_{j})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]+\\dfrac{\\delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right] +E\\left[\\dfrac{1}{2}\\Delta W(t_{j})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]+E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\frac{1}{2}\\sqrt{\\delta t}E\\left[\\xi\\right]\\\\\n& = & E\\left[\\dfrac{1}{2}\\Delta W(t_{i})+\\frac{1}{2}\\sqrt{\\delta t}\\xi\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\\\\n& = & E\\left[\\Delta W(t_{i+\\frac{1}{2}})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right].\n\\end{eqnarray*}\\]\nPor lo tanto \\(E\\left[\\left(\\Delta W(t_{i+\\frac{1}{2}})\\right)\\left(\\Delta W(t_{j+\\frac{1}{2}})\\right)\\right]= E\\left[\\Delta W(t_{i+\\frac{1}{2}})\\right]E\\left[\\Delta W(t_{j+\\frac{1}{2}})\\right]\\), con lo que se concluye que se satisface la propiedad C3. Con todo lo anterior se concluye que \\(W(t_{i+\\frac{1}{2}})\\) define un Movimiento Browniano.\n\n\nExercise 7.3 Generalice la formula en el {Exercise 10.2} para el caso, dado \\(W(t_{i}), W(t_{i+1})\\), y \\(\\alpha\\in(0,1)\\) el valor \\[\nW(t_{i}+\\alpha dt)\n\\] satisface las tres condiciones que define un movimiento Browniano.\n\n\nProof. Observemos que \\[\nt_{i+\\alpha}=\\alpha t_{i+1}+(1-\\alpha)t_{i},\n\\] y \\[\nW(t_{i+\\alpha})-W(t_{i}) \\sim \\alpha\\sqrt{ dt}N(0,1)\n\\] Definamos a \\[\nW(t_{i+\\alpha})=W\\left(t_{i}+\\alpha\\Delta t\\right) :=\\left(1-\\alpha\\right)W(t_{i})+\\alpha W(t_{i+1})+Y.\n\\] donde \\(Y\\) será una v.a independiente de \\(W\\left(t\\right)\\).\nDado que,\n\\[\\begin{align*}\nW(t_{i+\\alpha})-W(t_{i}) & =\\left(1-\\alpha\\right)W(t_{i})+\\alpha W(t_{i+1})+Y-W_{i}\\\\\n& =\\alpha\\left(W_{i+1}-W(t_{i})\\right)+Y.\n\\end{align*}\\]\nEntonces, \\[\nE\\left[W(t_{i+\\alpha})-W(t_{i})\\right]-E[\\alpha\\left(W(t_{i+1})-W(t_{i})\\right)]=E\\left[Y\\right]\\Longrightarrow E[Y]=0,\n\\] y \\[\nVar\\left[W(t_{i+\\alpha})-W(t_{i})\\right]=\\alpha^{2}d t+Var\\left[Y\\right],\n\\] Así, \\[\nVar\\left[Y\\right]=d t\\left(\\alpha-\\alpha^{2}\\right),\n\\] entonces \\(Y=\\sqrt{\\alpha\\left(1-\\alpha\\right)dt}\\xi,\\xi\\sim N\\left(0,1\\right)\\).\nCon esto se cumple \\(C1\\). \\[\nW\\left(0\\right)=0.\n\\] y por construcción análogamente que el ejercicio anterior se satisfacen las propiedades C2 y C3.\n\n\nExercise 7.4 Suponga que \\(X\\thicksim N(0,1)\\), sabemos que \\(E[X]=0\\) y \\(E(X^{2})=1\\).\nAdemás de la definción, el pésimo-momento satisface \\[\nE[X^{p}]=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}x^{p}\\exp(-x^{2}/2)dx.\n\\] Usando esta relación, demuestre que \\(E[X^{3}]=0\\) y \\(E[X^{4}]=3\\). Entonces deduce que un incremento Browniano \\(\\delta W_{i}:=W(t_{i+1})-W(t_{i})\\) satisface que \\(E[\\delta Wt_{i}^{3}]=0\\) y \\(E[\\delta Wt_{i}^{4}]=3\\delta t^{2}\\). Entonces encuentre una expresion para \\(E[X^{p}]\\) para un entero positivo \\(p\\)\n\n\nProof. De la definición del \\(p\\)-esimo momento se tiene para \\(p=4\\), que \\[\nE\\left[X^{4}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{4}\\exp\\left(-\\frac{x^{2}}{2}\\right)d x.\n\\] resolviendo esta integral por el método integración por partes, se tiene que \\(E\\left[X^{4}\\right]=uv-\\int vdu\\) , donde \\(u=x^{3}\\Longrightarrow du=3x^{2}dx\\) y \\(dv=x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\), calculemos primero \\(v\\),\n\\[\\begin{eqnarray*}\nv & = & \\int x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n& = & \\dfrac{1}{\\sqrt{2\\pi}}\\int x\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\quad\\text{ sea } y=-\\dfrac{x^{2}}{2}\\Longrightarrow dy=-xdx\\\\\n& = & -\\dfrac{1}{\\sqrt{2\\pi}}\\int \\exp{\\left(y\\right)}dy\\\\\n& = & -\\dfrac{1}{\\sqrt{2\\pi}} \\exp{\\left(y\\right)}\\\\\n& = &  -\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}.\n\\end{eqnarray*}\\]\nSustituyendo todo lo anterior se tiene que,\n\\[\\begin{eqnarray*}\n    E\\left[X^{4}\\right] & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}3x^{2}dx\\\\\n     & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+3\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} x^{2}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & \\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+3E[X^{2}],\n\\end{eqnarray*}\\]\npor otro lado, \\[\n\\left.-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}=\\lim_{x\\to\\infty}-x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}+\\lim_{x\\to-\\infty}x^{3}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}=0.\n\\] Por lo tanto, dado que \\(E[X^{2}]=1\\), se concluye que \\[\nE\\left[X^{4}\\right]=3.\n\\] Procediendo de igual manera que el caso anterior, se tiene que: \\[\nE\\left[X^{3}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{3}\\exp\\left(-\\frac{x^{2}}{2}\\right)d x.\n\\] tomando \\(u=x^{2}\\Longrightarrow du=2xdx\\) y \\(dv\\), \\(v\\) igual al caso anterior, se tiene que\n\\[\\begin{eqnarray*}\n    E\\left[X^{3}\\right] & = & \\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -2x\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & \\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}+2E[X]\\\\\n     & = & 0,\n\\end{eqnarray*}\\]\nusando el hecho que \\(E[X]=0\\) y \\[\n\\left.-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}=\\lim_{x\\to\\infty}-x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}+\\lim_{x\\to-\\infty}x^{2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}=0.\n\\] Por lo tanto, \\[\nE\\left[X^{3}\\right]=0.\n\\] De manera general se tiene que,\n\\[\\begin{eqnarray*}\n    E\\left[X^{p}\\right] & = & \\left.-x^{p-1}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}\\right|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty} -(p-1)x^{p-2}\\dfrac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\dfrac{x^{2}}{2}\\right)}dx\\\\\n     & = & 0+(p-1)E[X^{p-2}]\\\\\n     & = & (p-1)E[X^{p-2}].\n\\end{eqnarray*}\\]\nPor otro lado, observemos que \\(\\delta W_{i}\\sim N\\left(0,\\delta t\\right)\\), donde \\(\\delta t=t_{i+1}-t_{i}\\), entonces \\[\nZ=\\dfrac{\\delta W_{i}}{\\sqrt{\\delta t}}\\sim N\\left(0,1\\right),\n\\]\nque por lo visto anteriormente, para \\(p=4\\). \\[\nE\\left[Z^{4}\\right]=3\\Longrightarrow E[(\\delta W_{i})^{4}]=E[Z^{4}](\\delta t )^{2}=3(\\delta t )^{2}\n\\]\ny para \\(p=3\\), resulta \\[\nE[Z^{3}]=0\\Longrightarrow E[(\\delta W_{i})^{3}]=E[Z^{3}](\\delta t )^{3/2}=0.\n\\]\n\n\nExercise 7.5 Suponga que \\(X\\thicksim N(0,1)\\). Demuestre que para \\(a, b\\in\\mathbb{R}\\), \\[\nE[\\exp(a+bX)]=\\exp\\left(a+\\frac{1}{2}b^{2}\\right).\n\\] Por lo tanto deduzca que \\[\nE[\\exp(t+\\frac{1}{4}W_{t})]=\\exp\\left(\\frac{32}{33}t\\right).\n\\]\n\n\nProof. Se tiene que \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=\\exp{(a)}E\\left[\\exp\\left(bX\\right)\\right],\n\\]\nobservemos que \\(bX\\sim N\\left(0,b^{2}\\right)\\) además, \\(E\\left[\\exp\\left(bX\\right)\\right]\\) es la función generadora de momentos cuando \\(t=1\\) \\[\nM_{bX}\\left(1\\right)=E\\left[\\exp\\left(bX\\right)\\right]=\\exp\\left(\\dfrac{b^{2}}{2}\\right),\n\\] sustituyendo, resulta \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=\\exp{(a)}\\exp\\left(\\dfrac{b^{2}}{2}\\right)=\\exp\\left(a+\\dfrac{1}{2}b^{2}\\right).\n\\] Ahora calculemos \\(E\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]\\), se tiene que, \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=E\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right],\n\\] entonces consideremos a \\(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\), observemos que, \\(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\sim N\\left(0,1\\right),\\) por lo tanto, podemos usar la fórmula anterior con \\(a=t\\) y \\(b=\\dfrac{1}{4}\\sqrt{t}\\),\n\\[\\begin{align*}\nE\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right] & = E\\left[\\exp\\left(t+\\dfrac{1}{4}\\sqrt{t}\\left(\\dfrac{W_{t}-W_{0}}{\\sqrt{t}}\\right)\\right)\\right]\\\\\n& =\\exp\\left(t+\\dfrac{1}{2}\\left(\\dfrac{1}{16}t\\right)\\right)\\\\\n& =\\exp\\left(t+\\dfrac{1}{32}t\\right)\\\\\n& =\\exp\\left(\\dfrac{33}{32}t\\right).\n\\end{align*}\\]\nPor lo tanto se concluye que, \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=\\exp\\left(\\dfrac{33}{32}t\\right).\n\\]"
  },
  {
    "objectID": "Tarea6.html",
    "href": "Tarea6.html",
    "title": "Matematicas aplicadas",
    "section": "",
    "text": "Exercise 1 Cree un scrip para muestrear 10000 rutas del proceso \\(u(t,W_{t})\\) definido en el ejercicio 5.5. Graficar 10 rutas de muestra y la media de 10000 rutas de muestra de este proceso \\(u(t,W_{t})\\).\n\n\nSolution. \n\n\nExploring functions to generate random variables with a Bernoulli distribution.py\n\nimport numpy as np\nimport aux_functions as aux\nimport matplotlib.pyplot as plt\n\ndef b_function(t, a, w): \n    y = np.exp(t+ a * w)\n    return y\n\n\nn_samples = 10000\nn = 100\nt_initial = 0\nt_final = 1\n\nmean = np.zeros(n) \nfor i in range(n_samples):\n    time, b_w = aux.strong_brownian(t_final,n) \n    y = b_function(time,0.25, b_w) \n    if i &lt; 10:\n        plt.plot(time,b_w,'g-',alpha = 0.5) \n    mean += y \n\nmean = (n_samples)**(-1) * mean \ntime = np.linspace(0,t_final, n_)\n\ny = [np.exp(33 / 32 * t) for t in time] \nplt.plot(time, mean, 'r-*', alpha = 0.5)\nplt.plot(time, y,'b-',alpha = 0.8)\nplt.show()\n\n\n\n\nFigura 1\n\n\n\n\nExercise 2 Siguiendo las ideas para llenar un camino browniano en puntos \\(t_{i+\\frac{1}{2}}:=t_{i}+\\frac{1}{2}\\delta t\\). Haga una función de Python para llenar un camino browniano dada una fracción \\(\\alpha\\in(0,1)\\) para llenar en los puntos \\(t_{i+\\alpha}:=t_{i}+\\alpha\\delta t\\)\n\n\nSolution. \n\n\nExploring functions to generate random variables with a Bernoulli distribution.py"
  },
  {
    "objectID": "Tarea7.html",
    "href": "Tarea7.html",
    "title": "9  Tarea 7",
    "section": "",
    "text": "Exercise 9.1 Sea \\(W(t)\\) un Movimiento Browniano y \\(Z_{i}\\) una colección de variables aleatorias i.i.d, con distribución \\(N\\left(0,\\frac{\\delta t}{4}\\right)\\).\nPruebe que la suma \\[\n\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right),\n\\] tiene valor esperado igual a cero y una varianza de \\(O(\\delta t)\\).\n\n\nProof. Sin perdida de generalidad dado como estan definidas \\(Z_{i}\\) y \\(W(t_{i+1})-W(t_{i})\\) podemos suponer que son variables aleatorias independientes para cada \\(i=1,\\dots ,L\\). Entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\mathbb{E}\\left[Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\\\\n& = &  \\sum_{i=0}^{L}\\mathbb{E}(Z_{i})\\mathbb{E}\\left(W(t_{i+1})-W(t_{i})\\right)\\\\\n& = & 0 \\quad\\text{ ya que, por hipótesis, }\\mathbb{E}(Z_{i})=0\\text{ y }\\mathbb{E}\\left(W(t_{i+1})-W(t_{i})\\right)=0\n\\end{eqnarray*}\\] \\[\n\\therefore \\mathbb{E}\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]=0.\n\\] Ahora calculemos la varianza; sabemos que \\(Var(X)=\\mathbb{E}(X^{2})-(\\mathbb{E}(X))^{2}\\), sustituyendo resulta\n\\[\\begin{eqnarray*}\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &   \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]-\\left(\\mathbb{E}\\left[\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right]\\right)^{2}\\\\\n& = &  \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]\\quad\\text{ usando el hecho que tiene valor esperado igual a 0}.\n\\end{eqnarray*}\\] Por el Teorema multinomial, se tiene que\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] & = &  \\mathbb{E}\\left[\\sum_{i=0}^{L} \\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \\sum_{i=0}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right].\n\\end{eqnarray*}\\] Dado que \\(i\\neq j\\), sin perdida de generalidad podemos suponer que \\(i&lt;j\\), entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right] & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\n\\}\\\\\n& = & \\mathbb{E}\\{[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j} ]\\mathbb{E}\\left[(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\\}\\\\\n& = & 0\n\\end{eqnarray*}\\] Por otro lado,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2} & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{i}\\right]\n\\}\\\\\n& = & \\mathbb{E}\\{Z_{i}^{2}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{i}\\right]\\}\\\\\n& = & \\mathbb{E}[Z_{i}^{2}](t_{i+1}-t_{i})\\\\\n& = & \\dfrac{\\delta t}{4}(t_{i+1}-t_{i}),\n\\end{eqnarray*}\\] sustituyendo todo lo anterior, resulta \\[\\begin{eqnarray*}\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\dfrac{\\delta t}{4}(t_{i+1}-t_{i})\\\\\n& = &  \\dfrac{\\delta t}{4}(t_{L+1}-t_{0}).\n\\end{eqnarray*}\\] Para un \\(L\\) suficientemente grande podemos considerar que, dado un \\(\\varepsilon&gt;0\\), tal que, \\((t_{L+1}-t_{0})\\leq\\dfrac{\\varepsilon}{4}\\), así \\[\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\leq \\varepsilon\\delta t.\n\\] Por lo tanto, con esto se concluye que, la varianza es de orden \\(\\delta t\\).\n\n\nExercise 9.2 La regla del punto medio de la integral de Riemann de una función \\(h\\in C^{2}([a,b])\\) sobre una partición de \\(L\\) puntos del intervalo \\([a,b]\\) está dada por, \\[\n\\int_{a}^{b}h(t)dt=\\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}h\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)\\delta t.\n\\] Use la relación \\[\nW\\left(\\frac{t_{i}+t_{i+1}}{2}\\right)=\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ \\underbrace{Z_{i}}_{i.i.d.\\sim N(0,\\delta t/4)},\n\\] y el ejercicio anterior para demostrar que la regla del punto medio de la integral de Riemann implica que \\[\n\\int_{0}^{T}W(t)dW(t)=\\frac{1}{2}W(T)^{2}.\n\\]\n\n\nProof. Sea \\(\\Delta_{L}=\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la regla del punto medio, aplicada para \\(h(t)=W(t)\\), se tiene que\n\\[\\begin{eqnarray*}\n\\int_{0}^{T}W(t)dW(t) & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}W\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\left[\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ Z_{i}\\right](W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\frac{1}{2}\\left(W(t_{i+1})^{2}-W(t_{i})^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\frac{1}{2}\\left(W(T)^{2}-W(0)^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\frac{1}{2}W(T)^{2}+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i})).\n\\end{eqnarray*}\\] De la igualdad anterior solo nos faltaria demostrar que, \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2}\n\\] es decir, \\[\n\\lim_{\\|\\Delta_{L}\\|\\to0}E\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]=0\n\\] Del ejercicio anterior, sabemos que \\[\nE\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] = O(\\delta t)\\leq \\varepsilon\\|\\Delta_{L}\\|,\n\\] así, tomando el limite cuando \\(\\|\\Delta_{L}\\|\\to\\) se tiene que, \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2},\n\\] por lo tanto, sustituyendo este último resultado, se concluye que \\[\n\\int_{0}^{T}W(t)dW(t)=\\frac{1}{2}W(T)^{2}\n\\]\n\n\nExercise 9.3 Usando la aproximación de la suma de Riemann \\[\n\\int_{0}^{T}h(t)dW(t)\\sim\\sum_{i=0}^{L}h(t_{i})(W(t_{i+1})-W(t_{i})),\n\\tag{9.1}\\] argumente que, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T^{3}}{3}.\n\\] Por tanto, enuncie la isometría de Itô y deduzca que esta isometría es válida para el caso \\(h(t)=t\\).\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la aproximación de la suma de Riemann, tenemos que \\[\\begin{eqnarray*}\n\\int_{0}^{T}tdW(t) & \\sim & \\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\\\\n\\Longrightarrow \\left(\\int_{0}^{T}tdW(t)\\right)^{2} & \\sim & \\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2},\n\\end{eqnarray*}\\] del Teorema Multinomial, se sigue que \\[\n\\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2} = \\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\n\\] de las relaciones anteriores se tiene que, \\[\\begin{eqnarray*}\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & \\sim &  \\mathbb{E}\n\\left[\\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}\\mathbb{E}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i}),\n\\end{eqnarray*}\\] además, observemos que, \\[\n\\lim_{L\\to 0}\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i})=\\int_{0}^{T}t^{2}dt=\\frac{T}{3}\n\\] entonces, de esto último se concluye que, \\[\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T}{3}.\n\\] Por otro lado, de la isometría de Itô, se cumple que \\[\\begin{eqnarray*}\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & = & \\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)\\left(\\int_{0}^{T}tdW(t)\\right)\\right]\\\\\n& = &\n\\int_{0}^{T}\\mathbb{E}(t^{2})dt\\\\\n& = &\n\\int_{0}^{T}t^{2}dt\\\\\n& = & \\frac{T}{3}.\n\\end{eqnarray*}\\] Por lo tanto, la isometría de Itô se cumple para \\(h(t)=t\\)\n\n\nExercise 9.4 Escriba una función de Python para calcular la integral de Itô del movimiento Browniano \\(W(t)\\) sobre \\([0,T]\\). La función tendría la siguiente firma."
  },
  {
    "objectID": "Tarea8.html",
    "href": "Tarea8.html",
    "title": "10  Tarea 8",
    "section": "",
    "text": "Exercise 10.1 Use la aproximación de la suma de Riemann la ecuación (Equation 9.1). Muestra la propiedad de linealidad de la integral estocástica. Es decir, \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}=\\alpha\\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\), de la aproximación de la suma de Riemann ecuación (Equation 9.1), se satisface que, \\[\\begin{eqnarray*}\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t} & \\sim & \\sum_{i=0}^{L}(\\alpha f(t_{i})+\\beta g(t_{i}))(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\sum_{i=0}^{L}\\alpha f(t_{i})(W(t_{i+1})-W(t_{i}))+ \\sum_{i=0}^{L}\\beta g(t_{i})(W(t_{i+1})-W(t_{i}))\\\\\n& = & \\alpha\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))+\\beta\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))\n\\end{eqnarray*}\\] observemos que, \\[\n\\alpha\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))\\text{ es la aproximacion de suma de Riemann de; } \\alpha\\int_{0}^{T} f(t) dW_{t},\n\\] análogamente se tiene para \\(\\beta\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))\\), por lo tanto de aquí se sigue que, tomando el limite cuando \\(L\\to\\infty\\), resulta \\[\n\\alpha\\lim_{L\\to\\infty}\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))=\\alpha \\int_{0}^{T}f(t)dW_{t}\n\\] y \\[\n\\beta\\lim_{L\\to\\infty}\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))=\\beta\\int_{0}^{T}g(t)dW_{t}\n\\] entonces de estas dos últimas relaciones, se concluye que: \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}= \\alpha \\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\n\nExercise 10.2 Escriba con detalle la demostración del siguiente Teorema, también incluya la demostración del Lema 5.18 del Mao.\n\n\nTheorem 10.1 (6.1 del Mao) Sea \\(f\\in\\mathcal{M}^{2}([0,T];\\mathbb{R})\\), sea \\(\\rho,\\tau\\) dos tiempos de paro tales que \\(0\\leq\\rho\\leq\\tau\\leq T\\). Entonces \\[\\begin{eqnarray}\n\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dW_{s}\\mid\\mathcal{F}_{\\rho}\\right) & = & 0,\\\\\n\\mathbb{E}\\left(\\left|\\int_{\\rho}^{\\tau}f(s)dW_{s}\\right|^{2}\\mid\\mathcal{F}_{\\rho}\\right) & = & \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}\\left|f(s)\\right|^{2}ds\\mid\\mathcal{F}_{\\rho}\\right).\n\\end{eqnarray}\\]\n\nAntes de la demostración del teorema Theorem 10.1, veamos el siguiente Lema.\n\n\nLemma 10.1 (5.18 del Mao) Sea \\(f\\in\\mathcal{M}^{2}([0,T],\\mathbb{R})\\) y sea \\(\\tau\\) un tiempo de paro tal que \\(0\\leq \\tau\\leq T\\). Entonces \\[\n\\int_{0}^{\\tau}f(s)dW(s)=I(\\tau),\n\\] donde \\(\\{I(t)\\}_{0\\leq t \\leq t}\\) es la integral indefinida de \\(f\\) dada por la Definición 5.11.\n\n\nProof. La definición 5.11 del Mao, nos dice que, \\[\nI(t)=\\int_{0}^{t}f(s)dW(s),\\quad 0\\leq t\\leq T\n\\] Por otro lado, de la definición 5.15 del Mao, también se tiene que \\[\n\\int_{0}^{\\tau}f(s)dW(s)=\\int_{0}^{T}\\mathbb{I}_{[0,\\tau]}f(s)dW(s)\n\\] así, por las dos definiciones anteriores se cumple que, \\[\n\\int_{0}^{\\tau}f(s)dW(s)=I(\\tau)\n\\]\n\n\nProof (Del Teorema 6.1). El Teorema de paro de la martingala de Doob, nos dice que: \\[\\begin{equation}\nE(I(\\tau)|\\mathcal{F}_{\\rho})=I(\\rho)\n\\end{equation}\\] Además la definición 5.15, nos dice que para \\(\\rho\\) otro tiempo de paro, tal que \\(0\\leq \\rho\\leq \\tau\\), se cumple que \\[\n\\int_{\\rho}^{\\tau}f(s)dW(s)=\\int_{0}^{\\tau}f(s)dW(s)-\\int_{0}^{\\rho}f(s)dW(s).\n\\] Entonces, aplicando la igualdad anterior, el Lema Lemma 10.1 y el Teorema de paro de la martingala de Doob, resulta \\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dW_{s}\\Big|\\mathcal{F}_{\\rho}\\right) & = & \\mathbb{E}\\left(\\left(\\int_{0}^{\\tau}f(s)dW(s)-\\int_{0}^{\\rho}f(s)dW(s)\\right)\\Big|\\mathcal{F}_{\\rho}\\right)\\\\\n& = & \\mathbb{E}(I(\\tau)-I(\\rho)|\\mathcal{F}_\\rho)\\\\\n& = & \\mathbb{E}(I(\\tau)|\\mathcal{F}_\\rho)-\\mathbb{E}(I(\\rho)|\\mathcal{F}_\\rho)\\\\\n& = & I(\\rho)-I(\\rho)\\\\\n& = & 0\n\\end{eqnarray*}\\] De aquí, se concluye que la primera relación del teorema Theorem 10.1 se satisface.\nPor otro lado, nuevamente por el Teorema de paro de Doob, se tiene que \\[\\begin{equation}\nE(I^{2}(\\tau)-\\langle I,I\\rangle_{\\tau}|\\mathcal{F}_{\\rho})=I^{2}(\\rho)-\\langle I,I\\rangle_{\\rho},\n\\end{equation}\\] y además, del Teorema 5.14 del Mao, se tiene que \\[\n\\langle I,I\\rangle_{t}=\\int_{0}^{t}|f(s)|^{2}ds.\n\\] De estos dos hechos anteriores, resulta\n\\[\\begin{eqnarray*}\n\\mathbb{E}(|I(\\tau)-I(\\rho)|^{2}|\\mathcal{F}_{\\rho}) & = & \\mathbb{E}(I^{2}(\\tau)-2I(\\rho)I(\\tau)+I^{2}(\\rho)|\\mathcal{F}_{\\rho}) \\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-2I(\\rho)\\mathbb{E}(I(\\tau)|\\mathcal{F}_{\\rho})+I^{2}(\\rho) \\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-2I(\\rho)^{2}+I^{2}(\\rho)\\\\\n& = & \\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-I^{2}(\\rho)\\\\\n& = & \\mathbb{E}(\\langle I,I\\rangle_{\\tau}-\\langle I, I\\rangle_{\\rho}|\\mathcal{F}_{\\rho})\\\\\n& = & \\mathbb{E}\\left(\\int_{0}^{\\tau}|f(s)|^{2}ds-\\int_{0}^{\\rho}|f(s)|^{2}ds|\\mathcal{F}_{\\rho}\\right)\\\\\n& = & \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}|f(s)|^{2}ds|\\mathcal{F}_{\\rho}\\right).\n\\end{eqnarray*}\\] Ya que, \\[\n\\mathbb{E}\\left(\\left|\\int_{\\rho}^{\\tau}f(s)dW_{s}\\right|^{2}\\mid\\mathcal{F}_{\\rho}\\right)=\\mathbb{E}(|I(\\tau)-I(\\rho)|^{2}|\\mathcal{F}_{\\rho}),\n\\] se sigue que la segunda relación del teorema Theorem 10.1 se satisface.\n\n\nExercise 10.3 Usando la aproximación de la suma de Riemann ecuación (Equation 9.1), la isometría de Itô y la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\) pruebe que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right]=\\int_{0}^{T}\\mathbb{E}[f(t)g(t)]dt.\n\\]\n\n\nProof. Sea \\(a=\\int_{0}^{T}g(t)dW_{t}\\) y \\(b=\\int_{0}^{T}f(t)dW_{t}\\), entonces de la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\), se tiene que \\[\\begin{eqnarray*}\n4\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right) & = & \\left(\\int_{0}^{T}g(t)dW_{t}+\\int_{0}^{T}f(t)dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}g(t)dW_{t}-\\int_{0}^{T}f(t)dW_{t}\\right)^{2}\\\\\n& = & \\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2},\n\\end{eqnarray*}\\] Entonces\n\\[\\begin{eqnarray*}\n4\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right] & = & \\mathbb{E}\\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\mathbb{E}\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2}\\\\\n& = &\\left(\\int_{0}^{T} \\mathbb{E}(g(t)+f(t))^{2}dt\\right)-\\left(\\int_{0}^{T}\\mathbb{E}(g(t)-f(t))^{2}dt\\right)\\quad\\text{esto se sigue por la isometria de Itô}\\\\\n& = & \\left(\\int_{0}^{T}(\\mathbb{E}[(g(t)+f(t))^{2}]-\\mathbb{E}[(g(t)-f(t))^{2}])dt\\right)\\\\\n& = & \\left(\\int_{0}^{T}\\mathbb{E}[(g(t)+f(t))^{2}-(g(t)-f(t))^{2}]dt\\right)\\quad\\text{usando nuevamente que, } 4ab=(a+b)^{2}-(a-b)^{2}\\\\\n& = & 4\\left(\\int_{0}^{T}\\mathbb{E}[g(t)f(t)]dt\\right)\n\\end{eqnarray*}\\] De aquí, se concluye que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right]=\\left(\\int_{0}^{T}\\mathbb{E}[g(t)f(t)]dt\\right)\n\\]\n\n\nExercise 10.4 Usando la suma de Riemann ecuación (Equation 9.1), deduzca que, \\[\n\\int_{0}^{T}W(t)^{2}dW(t)=\\dfrac{1}{3}W(T)^{3}-\\int_{0}^{T}W(t)dt.\n\\]\n\n\nProof. Sea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). Primero, observemos que, \\[\n3W(t_{i})^{2}(W(t_{i+1})-W(t_{i}))=W(t_{i+1})^{3}-\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-3\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})-W(t_{i-1})^{3},\n\\] entonces de la ecuación (Equation 9.1) y la relación anterior, tenemos que,\n\\[\\begin{align*}\n\\int_{0}^{T}W(t)^{2}dW(t) & \\sim \\sum_{i=0}^{L}W(t_{i})^{2}(W(t_{i+1})-W(t_{i}))\\\\\n& = \\frac{1}{3} \\sum_{i=0}^{L}\\left[W(t_{i+1})^{3}-W(t_{i-1})^{3}\\right]-\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& =\\frac{1}{3}( W(T)^{3}-W(t_{0})^{3})- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& =\\frac{1}{3}W(T)^{3}- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i}).\n\\end{align*}\\] Afirmamos que \\(\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\to 0\\) en \\(L^{2}\\).\nEn efecto, calculemos la media de la variación cuadrática. Del Teorema Multinomial, resulta \\[\n\\frac{1}{9}\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right)^{2}\\right]=\\frac{1}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right]+\\frac{2}{9}\\sum_{i\\neq j}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right],\n\\] notemos que, como \\(i\\neq j\\), sin perdida de generalidad supongamos que \\(i&lt;j\\), entonces\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right] & = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]\\right|\\mathcal{F}_{j}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\mathbb{E}\\left.\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right|\\mathcal{F}_{j}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right]\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]\n\\end{eqnarray*}\\]\ndado que de la tarea 5 se demostro que, \\(\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right]=0\\), de la última igualdad ses concluye que, \\[\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]=0.\n\\] Por lo tanto, \\[\n\\frac{2}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]=0.\n\\] Por otro lado, también de la tarea 5, sabemos que \\[\n\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] = 15\\left(t_{i+1}-t_{i}\\right)^{3},\n\\] entonces\n\\[\\begin{align*}\n\\frac{1}{9}\\sum_{i=0}^{L}E\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] & =\\frac{5}{3}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)^{3}\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}L\\to0,\\quad\\text{cuando,  }\\|\\Delta_{L}\\|\\to0.\n\\end{align*}\\] Con todo lo anterior se concluye que, \\[\n\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\to 0.\n\\] Ahora veamos que, \\[\n\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\to \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right)\\text{ en }L^{2}\n\\] Observemos que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})- \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right) \\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}W(t_{i})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right] \\right)^{2}\\right]\\\\\n& = & \\mathbb{E}\\left[\\sum_{i=0}^{L}W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2} +\\sum_{i\\neq j}W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] \\right]\\\\\n& = & \\sum_{i=0}^{L}\\mathbb{E}\\left[W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2}\\right] +\\sum_{i\\neq j}\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] \\right],\n\\end{eqnarray*}\\] para \\(i&lt;j\\), se tiene que\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\left(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right)\\right] & = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\left(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right)\\right]\\right|\\mathcal{F}_{j}\\right\\}\\\\\n& = &  \\mathbb{E}\\left\\{W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\mathbb{E}\\left[\\left.\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right|\\mathcal{F}_{j}\\right]\\right\\}\\\\\n& = &  \\mathbb{E}\\left\\{W(t_{i})W(t_{j})\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right]\\right\\}\\\\\n& = & 0.\n\\end{eqnarray*}\\] Por lo tanto, \\[\n\\sum_{i=0}^{L}W(t_{i})W(t_{j})\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)\\right] =0.\n\\] Además, \\[\\begin{eqnarray*}\n\\mathbb{E}\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)^{2}\\right] & = & \\mathbb{E}\\left\\{\\mathbb{E}\\left.\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right)^{2}\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left.\\left\\{\\mathbb{E}\\left[W(t_{i})^{2}\\left(\\left(W(t_{i+1})-W(t_{i})\\right)^{4}-2\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)+\\left(t_{i+1}-t_{i}\\right)^{2}\\right)\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\mathbb{E}\\left.\\left[W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{4}-2W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)+W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right]\\right|\\mathcal{F}_{i}\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{4}\\right|\\mathcal{F}_{i}\\right]-2\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)\\right|\\mathcal{F}_{i}\\right]+\\mathbb{E}\\left[\\left.W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right|\\mathcal{F}_{i}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{W(t_{i})^{2}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{4}|\\mathcal{F}_{i}\\right]-2W(t_{i})^{2}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}\\left(t_{i+1}-t_{i}\\right)|\\mathcal{F}_{i}\\right]+W(t_{i})^{2}\\mathbb{E}\\left[\\left(t_{i+1}-t_{i}\\right)^{2}|\\mathcal{F}_{i}\\right]\\right\\}\\\\\n& = & \\mathbb{E}\\left\\{W(t_{i})^{2}3(t_{i+1}-t_{i})^{2}-2W(t_{i})^{2}(t_{i+1}-t_{i})^{2}+W(t_{i})^{2}\\left(t_{i+1}-t_{i}\\right)^{2}\\right\\}\\\\\n& = & 3t_{i}(t_{i+1}-t_{i})^{2}-2t_{i}(t_{i+1}-t_{i})^{2}+t_{i}(t_{i+1}-t_{i})^{2}\\\\\n& = & 2t_{i}(t_{i+1}-t_{i})^{2},\n\\end{eqnarray*}\\] de esta última igualdad se sigue que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\sum_{i=0}^{L}W(t_{i})^{2}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)\\right]^{2}\\right] & = & \\sum_{i=0}^{L}2t_{i}(t_{i+1}-t_{i})^{2}\\\\\n& \\leq & 2L\\|\\Delta_{L}\\|\\sum_{i=0}^{L}t_{i+1}-t_{i}\\\\\n& = & 2\\|\\Delta_{L}\\|L^{2}\\to0,\\quad\\text{ cuando }\\|\\Delta_{L}\\|\\to0.\n\\end{eqnarray*}\\] Por lo tanto, \\[\n\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\to \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right).\n\\] Así, sustituyendo todo lo anterior, resulta\n\\[\\begin{eqnarray*}\n\\int_{0}^{T}W(t)^{2}dW(t) & = & \\lim_{L\\to\\infty}\\frac{1}{3}W(T)^{3}-\\lim_{L\\to\\infty}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& = & \\frac{1}{3}W(T)^{3}-\\int_{0}^{T}W(t)dt\n\\end{eqnarray*}\\]\n\n\nExercise 10.5 Verifique que la isometría de Itô, dada por \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right],\n\\] se tiene cuando \\(h(t):= 1\\).\n\n\nProof. Del ejercicio Exercise 11.1 con \\(h(t)=1\\), se tiene que,\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW(t)\\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW_{t}\\right)\\left(\\int_{0}^{T}1dW_{t}\\right)\\right]\\\\\n& = & \\int_{0}^{T}\\mathbb{E}[1]dt\\\\\n& = & \\int_{0}^{T}dt\\\\\n& = & T.\n\\end{eqnarray*}\\] Por otro lado, \\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right] & = & \\mathbb{E}\\left[\\int_{0}^{T}1^{2}dt\\right]\\\\\n& = & \\mathbb{E}\\left[\\int_{0}^{T}dt\\right]\\\\\n& = & \\mathbb{E}\\left[T\\right]\\\\\n& = & T\n\\end{eqnarray*}\\] Así, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}dW(t)\\right)^{2}\\right]= \\mathbb{E}\\left[\\int_{0}^{T}dt\\right],\n\\] y con esto se concluye que, para \\(h(t)=1\\) se satisface que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]= \\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right].\n\\]"
  },
  {
    "objectID": "Tarea9.html",
    "href": "Tarea9.html",
    "title": "11  Tarea 9",
    "section": "",
    "text": "Exercise 11.1 Sea \\(\\tau\\) un tiempo de paro. Prueba que \\(W\\left(t+\\tau\\right)-W\\left(\\tau\\right)\\) es un movimiento browniano.\n\n\nProof. Definamos a, \\[\nW_{\\tau}\\left(t\\right)=W\\left(t+\\tau\\right)-W\\left(\\tau\\right),\n\\]\nClaramente \\(W_{\\tau}\\left(0\\right)=0\\), ya que: \\[\nW_{\\tau}\\left(0\\right)=W\\left(\\tau\\right)-W\\left(\\tau\\right)=0.\n\\]\nSea \\(s\\leq t\\), observemos que\n\\[\\begin{align*}\nW_{\\tau}\\left(t\\right)-W_{\\tau}\\left(s\\right) & =W\\left(t+\\tau\\right)-W\\left(\\tau\\right)-\\left[W\\left(s+\\tau\\right)-W\\left(\\tau\\right)\\right]\\\\\n& =W\\left(t+\\tau\\right)-W\\left(s+\\tau\\right),\n\\end{align*}\\]\ndado que \\(W\\left(t+\\tau\\right)-W\\left(s+\\tau\\right)\\sim N\\left(0,t-s\\right)\\) entonces de la última igualdad se sigue que \\[\nW_{\\tau}\\left(t\\right)-W_{\\tau}\\left(s\\right)\\sim N\\left(0,t-s\\right).\n\\] De aquí se concluye que \\(W_{\\tau}(t)\\) tiene incrementos independientes y estacionarios.\nPor lo tanto, con todo lo anterior se concluye que \\(W_{\\tau}(t)\\) es un Movimiento Browniano.\n\n\nExercise 11.2 Sea \\(W_{1}\\left(t\\right),W_{2}\\left(t\\right)\\) movimientos brownianos independientes con punto inicial \\(\\left(W_{1}\\left(0\\right),W_{2}\\left(0\\right)\\right)\\neq\\left(0,0\\right)\\).\nDefina \\(X_{t}:=\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right).\\)\n\n\n\nDemuestre que \\(X_{t}\\) es una martingala local.\nDemuestre que \\(E\\left|X_{t}\\right|&lt;\\infty\\) para cada \\(t&gt;0\\).\nDemuestre que \\(X_{t}\\) no es una martingala.\n\n\n\nProof. \\((a)\\) Consideremos a, \\[\n\\tau_{n}=\\inf_{t}\\left\\{ X_{t}=n\\right\\}\n\\]\nDado que \\(X_{t}\\) no es acotada, se tiene que, \\[\n\\tau_{n}\\to\\infty,\\text{ cuando }n\\to\\infty,\\forall n.\n\\]\nAhora probaremos que \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es una martingala.\nPrimero veamos que es adaptado a la filtración:\nSi \\(\\tau_{n}\\geq t\\) lo tenemos por construcción, ya que, en este caso \\[\nX_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=X_{t},\n\\] y \\(X_{t}\\) si es adaptado con respecto a la filtración. Ahora si \\(\\tau_{n}&lt;t\\), tenemos que, \\[\nX_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=X_{\\tau_{n}}=n.\n\\] Además, observemos que, \\[\n\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }=n\\right]\\subset\\left[\\tau_{n}&lt;t\\right],\n\\] y dado que \\(\\tau_{n}\\) es tiempo de paro, se cumple que \\(\\left[\\tau_{n}&lt;t\\right]\\in\\mathcal{F}_{t}\\). Por lo tanto, de la última relación se sigue que, \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es adaptado a la filtración.\nAhora solo nos queda probar que es una martingala. Sea \\(s&lt;t\\), se tiene que,\n\\[\\begin{align*}\nE\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\mid\\mathcal{F}_{s}\\right] & =E\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }1_{[t&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }1_{\\left[\\tau_{n}\\leq t\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{t}1_{[t&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq t\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{s}1_{[s&lt;\\tau_{n}]}\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq s\\right]}\\mid\\mathcal{F}_{s}\\right]\\\\\n& =X_{s}1_{[s&lt;\\tau_{n}]}+X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq s\\right]}\\\\\n& =X_{\\min\\left\\{ s,\\tau_{n}\\right\\} },\n\\end{align*}\\]\ny con esto se concluye que \\(X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\) es una martingala.\n\n\\((b)\\) Observemos que, como \\(X_{t} =\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\), entonces \\[\n\\exp\\left(X_{t}\\right) =W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right).\n\\] Asi,\n\\[\\begin{align*}\nE\\left[\\exp\\left(X_{t}\\right)\\right] & =E\\left[W_{1}^{2}\\left(t\\right)\\right]+E\\left[W_{2}^{2}\\left(t\\right)\\right]\\\\\n& =2t,\n\\end{align*}\\]\ndado que, \\(X_{t}\\geq0,\\forall t\\) y \\(X_{t} \\le\\exp\\left(X_{t}\\right)\\), entonces \\[\nE\\left[X_{t}\\right]\\leq2t&lt;\\infty,\\forall t\n\\] \\((c)\\) Primero recordemos lo siguiente:\nSi \\(X_{t}\\) es martingala entonces \\(E\\left[X_{t}\\right]\\) es constante, entonces este resultado nos diría que si \\(E\\left[X_{t}\\right]\\) no es constante, \\(X_{t}\\) no es martingala.\nDado lo anterior, supongamos que existe \\(c\\in\\mathbb{R}\\) tal que \\[\nE\\left[X_{t}\\right]=c,\\forall t\\Longrightarrow E\\left[\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\right] =c.\n\\] Así, \\[\n\\int_{0}^{\\infty}\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\mathrm{d}\\mathcal{P} =c,\n\\] de aquí se tendría que la integral es finita. Entonces \\[\nX_{t}\\to0,t\\to\\infty,\\text{ c.s}.\n\\] De esto último y de la continuidad de la exponencial, se concluye que \\[\nW_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\to1,t\\to\\infty,\\text{c.s}\n\\] Pero de lo demostrado del inciso anterior, sabemos que, \\[\nE\\left[W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right]=2t,\n\\] la cual converge a infinito, cuando \\(t\\to\\infty\\). Así, llegamos a una contradicción. Por lo tanto, \\(E\\left[X_{t}\\right]\\) no es constante, y se siguiría que \\(X_{t}\\) no puede ser martingala."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]